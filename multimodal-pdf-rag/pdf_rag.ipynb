{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e64821",
   "metadata": {},
   "source": [
    "# Multimodal PDF RAG\n",
    "\n",
    "## Problem Statement\n",
    "RAG Application that can answer any query related to an uploaded PDF(containing Text, Images and Tables).\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. Upload a PDF file containing Text, Images and Tables. \n",
    "2. Load and separate text, images and tables data from the PDF.\n",
    "3. Summarize the text and images using LLM.\n",
    "4. Embed and Store the text, summarized images & tables in a inmemory vectorDB.\n",
    "5. Create a Retriever for this indexed vectorDB.\n",
    "6. Create a text chain to answer related to text using LLM.\n",
    "7. Create a full chain with a multimodal LLM and text chain that can answer related to both text and images both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6431f3c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce7b8712",
   "metadata": {},
   "source": [
    "### Load Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54e84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cebe46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d15090",
   "metadata": {},
   "source": [
    "### Load PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a204a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/argha-ds/datascience/ai-assistant/RAG Use Cases/rag-use-cases-lc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a465129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PDF path\n",
    "pdf_path = \"./pdf-docs/rag_llm.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6835be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "# Use unstructured to read the PDF\n",
    "elements = partition_pdf(\n",
    "    filename=pdf_path,\n",
    "    strategy=\"hi_res\",                             # High-resolution layout parser (best for complex PDFs)\n",
    "    extract_images_in_pdf=True,                    # Extract images from the PDF\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],  # Extract image blocks that are images or tables\n",
    "    extract_image_block_to_payload=False,          # Don't embed image bytes directly into `el.metadata[\"image\"]`\n",
    "    extract_image_block_output_dir=\"./extracted_images\"  # Store extracted images here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287999ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: A Retrieval-Augmented Generation Based Large Language Model Benchmarked on a Novel Dataset...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Kieran Pichai...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Menlo School...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: ABSTRACT...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The evolution of natural language processing has seen marked advancements, particularly with the adv...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Introduction...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The evolution of natural language processing models has seen signiÔ¨Åcant strides from rule-based appr...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Curiously, little attention has been devoted to dissecting the individual components of RAG and thei...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 1...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice t...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Proposed Experiment...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Background and Importance...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally ri...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Our dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled ...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: By integrating this unique dataset into the RAG framework, we anticipate not only the preservation o...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The central objective of our experiment is twofold: to analyze the performance implications of modul...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The ultimate goal is to showcase the potential of a modular RAG system in processing culturally sign...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: 2...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Source and Composition...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Our proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of ver...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The data collection was an extensive process, where linguists and researchers engaged in deep conver...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Cultural and Environmental SigniÔ¨Åcance...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a ...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: for millennia....\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The environmental signiÔ¨Åcance of the Amazon Rainforest cannot be overstated‚Äîit is a keystone of glob...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Through our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Retrieval-Augmented Generation Framework...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophistica...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In mathematical terms, given an input query , the retriever model searches a knowledge base and retr...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: To examine the eÔ¨Äects of component interchangeability, we adopt various base language models and sim...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùêØùêØùë†ùë† = ùê∏ùê∏(ùë†ùë†)...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùêØùêØùë°ùë° = ùê∏ùê∏(ùë°ùë°)...\n",
      "================================================================================\n",
      "Type: Formula\n",
      "Preview: eye Vs * Ve similarity(v,, v,) = ‚Äî‚Äî~‚Äî‚Äî IIvsll [Ivel]...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùêØùêØùë†ùë† ‚ãÖ ùêØùêØùë°ùë°...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: similarity(ùêØùêØùë†ùë†,ùêØùêØùë°ùë°) = denotes the dot product between the two vectors, and Here, denotes the Eucli...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The experiment tests diÔ¨Äerent conÔ¨Ågurations by substituting with embedding functions from various mo...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùê∏ùê∏...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: Experiment Setup...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The experiment commences with the training of the language models using our unique dataset. For the ...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: : The base language model, which can be either GPT or Palm....\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: : The training dataset, consisting of pairs ‚Ñí answer. where is a query from the dataset and is the c...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: (ùëûùëûùëñùëñ,ùëéùëéùëñùëñ)...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùëéùëéùëñùëñ...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùëûùëûùëñùëñ...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùíüùíü...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The language model is Ô¨Åne-tuned on , optimizing the weights to minimize the loss function, typically...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ‚Ñí...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùíüùíü...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Following training, the question-answering process involves feeding a new query to the trained model...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùëéùëé‚Ä≤...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Benchmarking and Evaluation...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The evaluation metric for our experiment is based on the similarity scores between the generated res...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùê¥ùê¥ = {ùëéùëé1...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ,ùëéùëé2...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ,‚Ä¶,ùëéùëéùëõùëõ...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ,ùëéùëé2...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùê¥ùê¥ref = {ùëéùëé1...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: }...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: n...\n",
      "================================================================================\n",
      "Type: Formula\n",
      "Preview: ùëõùëõ ‚àó...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 1...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ‚Ä≤...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: Scoreavg =...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ,ùêØùêØùëéùëéùëñùëñ...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ÔøΩsimilarity(ùêØùêØùëéùëéùëñùëñ...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ) This average score acts as the primary benchmark for comparing diÔ¨Äerent model conÔ¨Ågurations. We sy...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ,‚Ä¶,ùëéùëéùëõùëõ...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Additionally, we account for the presence or absence of context in the model's training and response...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Through this meticulous experimental setup, we aim to illuminate the intricate dynamics between diÔ¨Äe...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Pre-Experiment Performance Expectations and Discussion...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In the landscape of varying conÔ¨Ågurations, we hypothesize that certain setups will yield higher aver...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ùêØùêØcontext...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: When aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em- b...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: }...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 4...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: We denote the expected performance increase due to context as , and the alignment of native embeddin...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ‚àÜcontext...\n",
      "================================================================================\n",
      "Type: Formula\n",
      "Preview: Scoreavg context > Scoreavgno context + Acontext Scoreaygnative > Scoreavg non-native + Anative...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ‚àÜnative...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: Scoreavg,context > Scoreavg,no context + ‚àÜcontext...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: These hypotheses will be tested through a series of experiments, allowing us to determine the optima...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Potential Implications for LLMs...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The results of this experiment are expected to have signiÔ¨Åcant implications for the development and ...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Moreover, the experiment is poised to challenge the prevailing approach to LLM training and Ô¨Åne-tuni...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The potential success of the RAG framework in this context may also pave the way for more granular i...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Implications for Indigenous Knowledge Preservation...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The signiÔ¨Åcance of our experiment extends beyond the technical accomplishments within the Ô¨Åeld of na...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: This experiment, should it succeed, will demonstrate a practical application of LLMs in the service ...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Advancements in RAG Framework...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: From a methodological standpoint, our experiment is poised to contribute to the advancement of the R...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Furthermore, the experiment's focus on modularity could inspire a new wave of research into componen...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 5...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION e Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In conclusion, the proposed experiment holds the potential to make signiÔ¨Åcant contributions to both ...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: Outputted Answer Primary data gathered from inhabitants of the amazon...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Figure 1. Venn Diagram of Data Sources for RAG. This Ô¨Ågure represents a venn diagram of 3 sources of...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: putted answer.‚Äù...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: User Question ChatGPT Different API Calls SerpAPI (Google Search Results) Scrape through a premade Q...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Figure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures ...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 6...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Experimental Results and Discussion...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The purpose of this section is to lay down the diÔ¨Äerent steps and customizations used within our exp...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Table 1. Experimental Results. This table represents the various diÔ¨Äerent combinations of LLM compon...\n",
      "================================================================================\n",
      "Type: Table\n",
      "Preview: Context LLM Embed for Similarity Score Yes No GPT Palm OpenAI Embedding Palm Embedding 1 x x x 2 x x...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: This paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to co...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Another major component of our RAG methodology is the ability to switch out which embedding layer yo...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: When using no context, Palm‚Äôs embedding layer seems to perform much better across the board, allowin...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Additionally, another beneÔ¨Åcial feature of the RAG optimization and breakdown style is the ability t...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The Ô¨Årst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In ter...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 7...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: to reach very high accuracy levels when paired with a similarity score calculated from the same prog...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: To recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In terms of expected results, two things were noticed, Ô¨Årst that Palm had a slightly lower performan...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Some unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, ...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Most notably from this experiment was two realizations. First, that Palm Embedding layer tends to wo...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Conclusion...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Building upon the foundation laid by our initial Ô¨Åndings, it is paramount to recognize the exception...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: This pivot in performance based on context underscores the signiÔ¨Åcance of tailored datasets and embe...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In light of these advancements, our research signiÔ¨Åes a pivotal moment for LLMs. The evidence sugges...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 8...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Limitations...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The results of the ‚Äúoutputted answer‚Äù of this algorithm largely reÔ¨Çect the quality of the data. If t...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In the future there is a lot of potential to expand on this research by breaking down the RAG algori...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: References...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Bahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE in...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Devlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understandi...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: arXiv:1810.04805....\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Lewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in N...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Radford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. Siriward...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 9...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print the content types and preview\n",
    "for el in elements:\n",
    "    print(f\"Type: {el.category}\")\n",
    "    print(f\"Preview: {el.text[:100]}...\")\n",
    "    print(\"=\"* 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5bf6547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'image_path': './extracted_images/figure-1-1.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'image_path': './extracted_images/figure-2-2.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'image_path': './extracted_images/figure-3-3.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'image_path': './extracted_images/figure-4-4.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'image_path': './extracted_images/figure-5-5.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'image_path': './extracted_images/figure-6-6.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION e Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(570.8333333333333), np.float64(389.16812472222216)), (np.float64(570.8333333333333), np.float64(947.3625458333333)), (np.float64(1129.0277544444443), np.float64(947.3625458333333)), (np.float64(1129.0277544444443), np.float64(389.16812472222216))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'image_path': './extracted_images/figure-6-7.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: Outputted Answer Primary data gathered from inhabitants of the amazon\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(350.0), np.float64(1136.6699216666666)), (np.float64(350.0), np.float64(1873.9728108333331)), (np.float64(1349.9998805555556), np.float64(1873.9728108333331)), (np.float64(1349.9998805555556), np.float64(1136.6699216666666))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'image_path': './extracted_images/figure-6-8.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: User Question ChatGPT Different API Calls SerpAPI (Google Search Results) Scrape through a premade QA list to find which most closely fits the user question Palm Embed Outputted Answer of User Question\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'image_path': './extracted_images/figure-7-9.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'image_path': './extracted_images/figure-8-10.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'image_path': './extracted_images/figure-9-11.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print the content of images\n",
    "for el in elements:\n",
    "    if el.category == 'Image':\n",
    "        print(f\"Type: {el.category}\")\n",
    "        print(f\"Metadata: {el.metadata.to_dict()}\")\n",
    "        print(f\"Preview: {el.text}\")\n",
    "        print(\"=\"* 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ef9cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Table\n",
      "Preview: Context LLM Embed for Similarity Score Yes No GPT Palm OpenAI Embedding Palm Embedding 1 x x x 2 x x x 3 x x x 4 x x x 5 x x x 6 x x x 7 x x x 8 x x x Score 0.75 0.92 0.93 0.88 0.997 0.996 0.91 0.897\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print the content of tables\n",
    "for el in elements:\n",
    "    if el.category == 'Table':\n",
    "        print(f\"Type: {el.category}\")\n",
    "        print(f\"Preview: {el.text}\")\n",
    "        print(\"=\"* 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8885febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group each element type\n",
    "text_elements = [el for el in elements if el.category in [\"NarrativeText\", \"Title\", \"List\"] and el.text]\n",
    "table_elements = [el for el in elements if el.category == \"Table\"]\n",
    "image_elements = [el for el in elements if el.category == \"Image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe38fa",
   "metadata": {},
   "source": [
    "#### Remove Duplicate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f47f7d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicate image: ./extracted_images/figure-6-6.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-4-4.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-2-2.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-9-11.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-8-10.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-1-1.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-5-5.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-7-9.jpg\n"
     ]
    }
   ],
   "source": [
    "# Hash and Filter Unique Image Files\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "def hash_image(file_path):\n",
    "    \"\"\"Generate a hash for an image file\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "# Directory where images are saved\n",
    "image_dir = \"./extracted_images\"\n",
    "\n",
    "# Track hashes and remove duplicates\n",
    "hash_set = set()\n",
    "unique_files = []\n",
    "\n",
    "for file_name in os.listdir(image_dir):\n",
    "    file_path = os.path.join(image_dir, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        img_hash = hash_image(file_path)\n",
    "        if img_hash not in hash_set:\n",
    "            hash_set.add(img_hash)\n",
    "            unique_files.append(file_path)\n",
    "        else:\n",
    "            os.remove(file_path)  # Delete duplicate file\n",
    "            print(f\"Removed duplicate image: {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10cedea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Duplicates in image_elements\n",
    "# Filter image elements to only include unique image files\n",
    "unique_image_elements = []\n",
    "\n",
    "valid_image_paths = set(os.path.basename(path) for path in unique_files)\n",
    "\n",
    "for el in elements:\n",
    "    if el.category == \"Image\":\n",
    "        image_metadata = el.metadata.to_dict().get(\"image_path\", \"\")\n",
    "        if image_metadata and os.path.basename(image_metadata) in valid_image_paths:\n",
    "            unique_image_elements.append(el)\n",
    "    else:\n",
    "        continue  # keep non-image elements untouched\n",
    "\n",
    "# Rebuild elements with deduplicated images\n",
    "image_elements = [\n",
    "    el for el in elements if el.category != \"Image\"\n",
    "] + unique_image_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5813f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'image_path': './extracted_images/figure-3-3.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @¬Æ Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(570.8333333333333), np.float64(389.16812472222216)), (np.float64(570.8333333333333), np.float64(947.3625458333333)), (np.float64(1129.0277544444443), np.float64(947.3625458333333)), (np.float64(1129.0277544444443), np.float64(389.16812472222216))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'image_path': './extracted_images/figure-6-7.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: Outputted Answer Primary data gathered from inhabitants of the amazon\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(350.0), np.float64(1136.6699216666666)), (np.float64(350.0), np.float64(1873.9728108333331)), (np.float64(1349.9998805555556), np.float64(1873.9728108333331)), (np.float64(1349.9998805555556), np.float64(1136.6699216666666))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'image_path': './extracted_images/figure-6-8.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: User Question ChatGPT Different API Calls SerpAPI (Google Search Results) Scrape through a premade QA list to find which most closely fits the user question Palm Embed Outputted Answer of User Question\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print the content of images\n",
    "for el in image_elements:\n",
    "    if el.category == 'Image':\n",
    "        print(f\"Type: {el.category}\")\n",
    "        print(f\"Metadata: {el.metadata.to_dict()}\")\n",
    "        print(f\"Preview: {el.text}\")\n",
    "        print(\"=\"* 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb121eb6",
   "metadata": {},
   "source": [
    "### Summarize the tables and Images with Vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0eebafaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001 ['embedText', 'countTextTokens']\n",
      "models/gemini-1.5-pro-latest ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-002 ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-latest ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-002 ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-8b ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-001 ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-latest ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-2.5-pro-preview-03-25 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-05-20 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-06-17 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-05-06 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-06-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp-image-generation ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-preview-image-generation ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite-preview-02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp-02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-exp-1206 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-1219 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-tts ['countTokens', 'generateContent']\n",
      "models/gemini-2.5-pro-preview-tts ['countTokens', 'generateContent']\n",
      "models/learnlm-2.0-flash-experimental ['generateContent', 'countTokens']\n",
      "models/gemma-3-1b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e2b-it ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/embedding-001 ['embedContent']\n",
      "models/text-embedding-004 ['embedContent']\n",
      "models/gemini-embedding-exp-03-07 ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-exp ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-001 ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/aqa ['generateAnswer']\n",
      "models/imagen-3.0-generate-002 ['predict']\n",
      "models/imagen-4.0-generate-preview-06-06 ['predict']\n",
      "models/imagen-4.0-ultra-generate-preview-06-06 ['predict']\n",
      "models/veo-2.0-generate-001 ['predictLongRunning']\n",
      "models/veo-3.0-generate-preview ['predictLongRunning']\n",
      "models/veo-3.0-fast-generate-preview ['predictLongRunning']\n",
      "models/gemini-2.5-flash-preview-native-audio-dialog ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.5-flash-exp-native-audio-thinking-dialog ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-live-001 ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-live-2.5-flash-preview ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-live-preview ['bidiGenerateContent', 'countTokens']\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    print(model.name, model.supported_generation_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "316e6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Image\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9a9da25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-pro-latest',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Vision Model\n",
    "# vision_model = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
    "vision_model =  genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n",
    "\n",
    "vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c56baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prompt\n",
    "prompt_text = \"\"\"You are an assitant tasked with summarizing tables and images for retrieval.\n",
    "        These summaries will be embeded and used to retrieve the raw image or raw table elements.\n",
    "        Give a concise summary of the image or table that will be optimized for retrieval.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize Image or Table from Image\n",
    "\n",
    "\n",
    "def summarize_image(image_path):\n",
    "    \"\"\"Summarize an image (including tables) using Gemini Vision.\"\"\"\n",
    "    try:\n",
    "\n",
    "        img = Image.open(image_path)\n",
    "        response = vision_model.generate_content(\n",
    "            [prompt_text, img]\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd8cec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing: ./extracted_images/figure-3-3.jpg\n",
      ">>> Summary: Journal of Student Research, High School Edition...\n",
      "\n",
      "Summarizing: ./extracted_images/figure-6-8.jpg\n",
      ">>> Summary: Flowchart of a system processing user questions.  The system uses several API calls (SerpAPI for Google Search Results, Palm, and ChatGPT) and embedding methods (Palm Embed and OpenAI Embed) to find a...\n",
      "\n",
      "Summarizing: ./extracted_images/figure-6-7.jpg\n",
      ">>> Summary: Outputted answers combine: Google search results (SerpApi), existing Palm/OpenAI data, and primary data gathered from Amazon inhabitants....\n",
      "\n",
      "Summarizing: ./extracted_images/table-7-1.jpg\n",
      ">>> Summary: This table shows similarity scores for 8 different comparisons.  The comparisons vary by whether context was used (yes/no), which LLM was used (GPT/Palm), and which embedding model was used (OpenAI/Pa...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply to All Unique Image Files\n",
    "\n",
    "# Store results\n",
    "image_summaries = {}\n",
    "\n",
    "for img_path in unique_files:\n",
    "    print(f\"Summarizing: {img_path}\")\n",
    "    summary = summarize_image(img_path)\n",
    "    if summary:\n",
    "        image_summaries[img_path] = summary\n",
    "        print(f\">>> Summary: {summary[:200]}...\\n\")\n",
    "    else:\n",
    "        print(f\">>> Failed to summarize {img_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac03238",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "812e7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e826e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the image summaries into docs\n",
    "image_docs = [\n",
    "    Document(page_content=summary, metadata={\"source\": path, \"type\": \"image_or_table\"})\n",
    "    for path, summary in image_summaries.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a339dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './extracted_images/figure-3-3.jpg', 'type': 'image_or_table'}, page_content='Journal of Student Research, High School Edition'),\n",
       " Document(metadata={'source': './extracted_images/figure-6-8.jpg', 'type': 'image_or_table'}, page_content=\"Flowchart of a system processing user questions.  The system uses several API calls (SerpAPI for Google Search Results, Palm, and ChatGPT) and embedding methods (Palm Embed and OpenAI Embed) to find and process information relevant to the user's query.  It also scrapes a premade Q&A list.  All of this information is fed into a large language model to produce an outputted answer.\\n\"),\n",
       " Document(metadata={'source': './extracted_images/figure-6-7.jpg', 'type': 'image_or_table'}, page_content='Outputted answers combine: Google search results (SerpApi), existing Palm/OpenAI data, and primary data gathered from Amazon inhabitants.'),\n",
       " Document(metadata={'source': './extracted_images/table-7-1.jpg', 'type': 'image_or_table'}, page_content='This table shows similarity scores for 8 different comparisons.  The comparisons vary by whether context was used (yes/no), which LLM was used (GPT/Palm), and which embedding model was used (OpenAI/Palm). Scores range from 0.75 to 0.997. Notably, using Palm embeddings consistently yields higher scores than OpenAI embeddings.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14b23405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.5880249738693237, 'coordinates': {'points': ((np.float64(200.0), np.float64(207.7227020263672)), (np.float64(200.0), np.float64(339.69999999999976)), (np.float64(1411.8000000000002), np.float64(339.69999999999976)), (np.float64(1411.8000000000002), np.float64(207.7227020263672))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '8727570aa228bd375e82f4dd71a6eb10'}\n",
      "A Retrieval-Augmented Generation Based Large Language Model Benchmarked on a Novel Dataset\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.763323962688446, 'coordinates': {'points': ((np.float64(200.0), np.float64(387.167236328125)), (np.float64(200.0), np.float64(423.79999999999995)), (np.float64(404.93333333333334), np.float64(423.79999999999995)), (np.float64(404.93333333333334), np.float64(387.167236328125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '57383d799582a8e4de79ca8099c779cf'}\n",
      "Kieran Pichai\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.8045752644538879, 'coordinates': {'points': ((np.float64(199.34686279296875), np.float64(471.9896545410156)), (np.float64(199.34686279296875), np.float64(501.65550000000013)), (np.float64(376.0736666666666), np.float64(501.65550000000013)), (np.float64(376.0736666666666), np.float64(471.9896545410156))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '57383d799582a8e4de79ca8099c779cf'}\n",
      "Menlo School\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.7151427865028381, 'coordinates': {'points': ((np.float64(200.0), np.float64(548.317138671875)), (np.float64(200.0), np.float64(585.8000000000001)), (np.float64(382.9333333333334), np.float64(585.8000000000001)), (np.float64(382.9333333333334), np.float64(548.317138671875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '8727570aa228bd375e82f4dd71a6eb10'}\n",
      "ABSTRACT\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9544898271560669, 'coordinates': {'points': ((np.float64(200.0), np.float64(632.935791015625)), (np.float64(200.0), np.float64(1039.9338333333333)), (np.float64(1506.9865333333335), np.float64(1039.9338333333333)), (np.float64(1506.9865333333335), np.float64(632.935791015625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '7777393319ddadddb0e2c044c0332443'}\n",
      "The evolution of natural language processing has seen marked advancements, particularly with the advent of models like BERT, Transformers, and GPT variants, with recent additions like GPT and Bard. This paper investigates the Retrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its constituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our research demonstrates the signiÔ¨Åcance of preserving indigenous cultures and biodiversity. The experiment employs a customizable RAG methodology, allowing for the interchangeability of various components, such as the base language model and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm exhibits superior performance without context. The results also suggest that models tend to perform optimally when paired with similarity scores from their native platforms. Conclusively, our approach showcases the potential of a modular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to tra- ditional Ô¨Åne-tuning of large language models.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8799284100532532, 'coordinates': {'points': ((np.float64(200.0), np.float64(1092.8036666666667)), (np.float64(200.0), np.float64(1131.6370000000002)), (np.float64(411.05916666666667), np.float64(1131.6370000000002)), (np.float64(411.05916666666667), np.float64(1092.8036666666667))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '8727570aa228bd375e82f4dd71a6eb10'}\n",
      "Introduction\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9549375176429749, 'coordinates': {'points': ((np.float64(200.0), np.float64(1178.203857421875)), (np.float64(200.0), np.float64(1509.1005000000002)), (np.float64(1506.9948833333333), np.float64(1509.1005000000002)), (np.float64(1506.9948833333333), np.float64(1178.203857421875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87d99a6d77158f90f024b012d545c164'}\n",
      "The evolution of natural language processing models has seen signiÔ¨Åcant strides from rule-based approaches in the early stages of language understanding, eventually leading to the advent of neural networks. However, the full potential of these neural networks awaited the computational infrastructure to catch up. The pivotal moment arrived with the emergence of neural machine translation (NMT), exempliÔ¨Åed by Google Translate, which marked a turning point in machine language comprehension (Bahdanau, 2016). Subsequently, a plethora of advanced models, including BERT, Transformers, GPT-2, and GPT-3, have emerged, driving the Ô¨Åeld forward. Recent notable additions to this landscape are models like GPT and Bard (Devlin, 2018) (Vaswani, 2017) (Radford, 2018). While Ô¨Åne-tuning such models has proven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG) oÔ¨Äers a prom- ising alternative (Lewis, 2020) (Siriwardhana, 2023) (Yu, 2022).\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9535836577415466, 'coordinates': {'points': ((np.float64(199.32568359375), np.float64(1519.1004999999998)), (np.float64(199.32568359375), np.float64(1963.1005)), (np.float64(1506.9197333333332), np.float64(1963.1005)), (np.float64(1506.9197333333332), np.float64(1519.1004999999998))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87d99a6d77158f90f024b012d545c164'}\n",
      "Curiously, little attention has been devoted to dissecting the individual components of RAG and their respec- tive impacts on overall performance. In response to this gap, our paper undertakes a comprehensive investigation of the RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity and replaceability of its constituent modules. This research seeks to contribute to a deeper understanding of the mech- anisms underlying RAG and its potential for enhancing natural language understanding and generation. These Large Language Models (LLMs) exhibit a remarkable proÔ¨Åciency in replicating human language styles, achieving a level of linguistic verisimilitude that borders on the impeccable. In light of these capabilities, it is prudent to delve into the profound signiÔ¨Åcance of the Amazon rainforest, which equates to the importance of any ethnically or racially diverse nation across the globe. Within the vast expanse of the Amazon, an intricate tapestry of life unfolds, where millions of distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the indigenous populations who have made this ecosystem their home. The Amazon rainforest is not only a cradle of biological diversity but also a sanctuary for an array of religions and cultures, many of which teeter on the brink of\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7187889814376831, 'coordinates': {'points': ((np.float64(198.84555053710938), np.float64(2041.5390625)), (np.float64(198.84555053710938), np.float64(2064.449462890625)), (np.float64(394.1135559082031), np.float64(2064.449462890625)), (np.float64(394.1135559082031), np.float64(2041.5390625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87d99a6d77158f90f024b012d545c164'}\n",
      "ISSN: 2167-1907\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9530729651451111, 'coordinates': {'points': ((np.float64(199.2046356201172), np.float64(210.1004999999999)), (np.float64(199.2046356201172), np.float64(654.1005)), (np.float64(1506.9058166666666), np.float64(654.1005)), (np.float64(1506.9058166666666), np.float64(210.1004999999999))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'fba5bd666aa06416d7fb23c773927a1d'}\n",
      "oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice to the indigenous communities whose ancestral lands are enshrined within its boundaries. It is a call to safeguard the memories of the land, the traditions that have evolved within its embrace, and the very essence of their cultures. However, certain regions of the Amazon remain shrouded in obscurity, their Ô¨Çora and fauna so rare that reliable and readily available information is conspicuously lacking in the vast repository of knowledge available on the internet. In this context, advanced LLMs play an instrumental role in addressing this deÔ¨Åcit by facilitating the dissemination of indigenous narratives and thereby amplifying awareness and appreciation of the rich tapestry of beliefs, practices, and traditional knowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures with the global community, emphasizing the paramount importance of preserving the cultural diversity interwoven within this vast rainforest. In sum, the overarching mission of this endeavor is twofold: to document and educate the Western world about hitherto unknown cultures while concurrently ensuring the enduring preservation of these inval- uable facets of human heritage and biodiversity.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8712294101715088, 'coordinates': {'points': ((np.float64(200.0), np.float64(706.9703333333333)), (np.float64(200.0), np.float64(745.8036666666667)), (np.float64(560.7228333333333), np.float64(745.8036666666667)), (np.float64(560.7228333333333), np.float64(706.9703333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'fba5bd666aa06416d7fb23c773927a1d'}\n",
      "Proposed Experiment\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.7549414038658142, 'coordinates': {'points': ((np.float64(200.0), np.float64(794.6333333333332)), (np.float64(200.0), np.float64(827.9666666666667)), (np.float64(587.5000000000001), np.float64(827.9666666666667)), (np.float64(587.5000000000001), np.float64(794.6333333333332))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'fba5bd666aa06416d7fb23c773927a1d'}\n",
      "Background and Importance\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9518823027610779, 'coordinates': {'points': ((np.float64(200.0), np.float64(871.5422973632812)), (np.float64(200.0), np.float64(1089.9338333333333)), (np.float64(1506.980966666667), np.float64(1089.9338333333333)), (np.float64(1506.980966666667), np.float64(871.5422973632812))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '164f05502c751520de7c2ad97e3d16f4'}\n",
      "The intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally rich as the Amazon Rainforest, cannot be overstated. This knowledge, passed down through generations, encompasses not only cultural and religious beliefs but also practical insights into the local Ô¨Çora and fauna. As the modern world encroaches on these lands, this wisdom is in peril of being lost forever. Recognizing this, our proposed experiment aims to employ a state- of-the-art Retrieval-Augmented Generation (RAG) framework to capture and leverage this vast, yet vulnerable, knowledge base.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9425076842308044, 'coordinates': {'points': ((np.float64(200.0), np.float64(1099.9338333333333)), (np.float64(200.0), np.float64(1241.2671666666665)), (np.float64(1506.7749999999992), np.float64(1241.2671666666665)), (np.float64(1506.7749999999992), np.float64(1099.9338333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '164f05502c751520de7c2ad97e3d16f4'}\n",
      "Our dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled in its depth and breadth. It includes detailed discussions on religious practices, cultural nuances, and the integral role of the surrounding ecosystem in the daily lives of these communities. This data is not just a scientiÔ¨Åc or anthropological resource; it is a repository of living history and an urgent call to action for preservation eÔ¨Äorts.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9473974108695984, 'coordinates': {'points': ((np.float64(200.0), np.float64(1251.2671666666668)), (np.float64(200.0), np.float64(1430.4338333333333)), (np.float64(1506.9113833333333), np.float64(1430.4338333333333)), (np.float64(1506.9113833333333), np.float64(1251.2671666666668))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '164f05502c751520de7c2ad97e3d16f4'}\n",
      "By integrating this unique dataset into the RAG framework, we anticipate not only the preservation of knowledge but also the generation of responses that reÔ¨Çect the rich tapestry of Amazonian life. The experiment is designed to evaluate how diÔ¨Äerent components within the RAG setup‚Äîsuch as base language models and similarity scoring algorithms‚Äîcan be optimized to reÔ¨Çect the nuances captured within our dataset. In doing so, we aim to bridge the gap between advanced language models and the profound human insights found within the Amazon.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9563410878181458, 'coordinates': {'points': ((np.float64(199.37210083007812), np.float64(1439.8302001953125)), (np.float64(199.37210083007812), np.float64(1733.1004999999998)), (np.float64(1507.0060166666665), np.float64(1733.1004999999998)), (np.float64(1507.0060166666665), np.float64(1439.8302001953125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '164f05502c751520de7c2ad97e3d16f4'}\n",
      "The central objective of our experiment is twofold: to analyze the performance implications of modular design within the RAG framework and to demonstrate the profound capability of such a system to preserve and communicate the wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper understanding of the data but also allow us to Ô¨Åne-tune the system for optimal performance across diÔ¨Äerent conÔ¨Ågu- rations. To achieve this, we will systematically explore the interchangeability of various RAG components. We will assess diÔ¨Äerent base language models such as GPT and Palm and compare the eÔ¨Écacy of similarity scoring tools from diverse platforms. The experiment will rigorously test these combinations, identifying which synergies most eÔ¨Äectively capture the essence of the dataset.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9368348717689514, 'coordinates': {'points': ((np.float64(200.0), np.float64(1743.1004999999998)), (np.float64(200.0), np.float64(1884.433833333333)), (np.float64(1506.8946833333334), np.float64(1884.433833333333)), (np.float64(1506.8946833333334), np.float64(1743.1004999999998))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '164f05502c751520de7c2ad97e3d16f4'}\n",
      "The ultimate goal is to showcase the potential of a modular RAG system in processing culturally signiÔ¨Åcant information, paving the way for future applications that can beneÔ¨Åt from such tailored language models. We anticipate that our Ô¨Åndings will contribute signiÔ¨Åcantly to the Ô¨Åelds of computational linguistics and cultural preservation, demon- strating a novel approach to the application of large language models.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8222460150718689, 'coordinates': {'points': ((np.float64(200.0), np.float64(211.96666666666664)), (np.float64(200.0), np.float64(245.29999999999998)), (np.float64(536.5000000000001), np.float64(245.29999999999998)), (np.float64(536.5000000000001), np.float64(211.96666666666664))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'd53b1a52f43c2d233621cd4813a3e3b8'}\n",
      "Source and Composition\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.945088267326355, 'coordinates': {'points': ((np.float64(200.0), np.float64(290.08062744140625)), (np.float64(200.0), np.float64(469.43383333333327)), (np.float64(1506.7638666666655), np.float64(469.43383333333327)), (np.float64(1506.7638666666655), np.float64(290.08062744140625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '6f62cd8ffcc721c0fbad4e4ef7866cce'}\n",
      "Our proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of verbal histories, inter- views, and ecological insights gathered from the indigenous peoples of the Amazon Rainforest, as well as from biolo- gists and ecologists dedicated to studying this unique biome. The dataset is characterized by its diversity, comprising narratives that elucidate the intricate relationship between the natives and their environment, including the religious and cultural signiÔ¨Åcance of plant and animal life.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9435852766036987, 'coordinates': {'points': ((np.float64(200.0), np.float64(479.4338333333333)), (np.float64(200.0), np.float64(658.6005)), (np.float64(1506.911783333333), np.float64(658.6005)), (np.float64(1506.911783333333), np.float64(479.4338333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '6f62cd8ffcc721c0fbad4e4ef7866cce'}\n",
      "The data collection was an extensive process, where linguists and researchers engaged in deep conversations with the natives, recording their dialects, translating their stories, and documenting their knowledge of the ecological system. Similarly, biologists contributed their decades of research on the Ô¨Çora and fauna, providing a scientiÔ¨Åc per- spective to the indigenous narratives. The data thus forms a conÔ¨Çuence of traditional wisdom and modern scientiÔ¨Åc understanding, oÔ¨Äering a 360-degree view of the Amazon Rainforest's ecosystem.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8764854073524475, 'coordinates': {'points': ((np.float64(200.0), np.float64(708.2999999999998)), (np.float64(200.0), np.float64(741.6333333333331)), (np.float64(750.3333333333335), np.float64(741.6333333333331)), (np.float64(750.3333333333335), np.float64(708.2999999999998))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'd53b1a52f43c2d233621cd4813a3e3b8'}\n",
      "Cultural and Environmental SigniÔ¨Åcance\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9359689354896545, 'coordinates': {'points': ((np.float64(200.0), np.float64(789.6005)), (np.float64(200.0), np.float64(923.6652221679688)), (np.float64(1507.0088), np.float64(923.6652221679688)), (np.float64(1507.0088), np.float64(789.6005))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '723212a0d98d13268e9c610c2d321db8'}\n",
      "The urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a race against time, as globalization and environmental degradation threaten to erase unique cultures and the wisdom they hold. Our dataset serves as a digital ark, a means to preserve and perpetuate the knowledge that has sustained the Amazon's communities\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9390075206756592, 'coordinates': {'points': ((np.float64(196.1522979736328), np.float64(940.9338333333334)), (np.float64(196.1522979736328), np.float64(1082.2671666666665)), (np.float64(1506.8195333333326), np.float64(1082.2671666666665)), (np.float64(1506.8195333333326), np.float64(940.9338333333334))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '723212a0d98d13268e9c610c2d321db8'}\n",
      "The environmental signiÔ¨Åcance of the Amazon Rainforest cannot be overstated‚Äîit is a keystone of global biodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling the ecological interdependencies that are vital for the rainforest's survival. This dataset, therefore, is not just an aca- demic or technological asset; it is a critical record for environmental conservationists and policymakers.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.937074601650238, 'coordinates': {'points': ((np.float64(200.0), np.float64(1092.2671666666668)), (np.float64(200.0), np.float64(1233.6005)), (np.float64(1506.8612833333332), np.float64(1233.6005)), (np.float64(1506.8612833333332), np.float64(1092.2671666666668))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '723212a0d98d13268e9c610c2d321db8'}\n",
      "Through our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under- standing of their habitat is unmatched. By integrating their knowledge into the RAG framework, we hope to create a model that not only responds with information but also with wisdom that respects the interconnectedness of life and culture.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8728882670402527, 'coordinates': {'points': ((np.float64(199.95449829101562), np.float64(1282.9666666666667)), (np.float64(199.95449829101562), np.float64(1316.6333333333332)), (np.float64(807.3333333333336), np.float64(1316.6333333333332)), (np.float64(807.3333333333336), np.float64(1282.9666666666667))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'd53b1a52f43c2d233621cd4813a3e3b8'}\n",
      "Retrieval-Augmented Generation Framework\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9367923140525818, 'coordinates': {'points': ((np.float64(198.63092041015625), np.float64(1364.7671666666668)), (np.float64(198.63092041015625), np.float64(1506.1005)), (np.float64(1506.9419999999996), np.float64(1506.1005)), (np.float64(1506.9419999999996), np.float64(1364.7671666666668))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '62b9e6d1f361a969f5f7e3a1d0bf537b'}\n",
      "The heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophisticated algorithm that enables the deconstruction of the language model into discrete, interchangeable components. This framework integrates a retriever model that sources relevant context and a generator model that synthesizes the retrieved infor- mation into coherent responses.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9385144114494324, 'coordinates': {'points': ((np.float64(200.0), np.float64(1516.1005)), (np.float64(200.0), np.float64(1657.4338333333335)), (np.float64(1506.9638833333331), np.float64(1657.4338333333335)), (np.float64(1506.9638833333331), np.float64(1516.1005))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '62b9e6d1f361a969f5f7e3a1d0bf537b'}\n",
      "In mathematical terms, given an input query , the retriever model searches a knowledge base and retrieves a set of relevant documents . Each document is represented as a vector in a high-dimensional ùí¶ùí¶ ùëûùëû space, obtained from an embedding layer. This process transforms the raw text data into a structured form amenable ùê∑ùê∑ = {ùëëùëë1,ùëëùëë2,‚Ä¶,ùëëùëëùëòùëò} ùêØùêØùëëùëë ùëëùëë to computational manipulation.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9405785202980042, 'coordinates': {'points': ((np.float64(199.1611328125), np.float64(1667.113037109375)), (np.float64(199.1611328125), np.float64(1808.7671666666665)), (np.float64(1506.8959499999999), np.float64(1808.7671666666665)), (np.float64(1506.8959499999999), np.float64(1667.113037109375))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '62b9e6d1f361a969f5f7e3a1d0bf537b'}\n",
      "To examine the eÔ¨Äects of component interchangeability, we adopt various base language models and similar- ity scoring mechanisms. For instance, if denotes the embedding function, and and represent the input and target text sequences, respectively, their vector representations would be ùê∏ùê∏ larity as the basis for our similarity score, deÔ¨Åned by the formula: and ùë†ùë† ùë°ùë° . We employ cosine simi-\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9368948340415955, 'coordinates': {'points': ((np.float64(200.0), np.float64(1886.381333333333)), (np.float64(200.0), np.float64(1996.1005)), (np.float64(1506.9308666666668), np.float64(1996.1005)), (np.float64(1506.9308666666668), np.float64(1886.381333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "similarity(ùêØùêØùë†ùë†,ùêØùêØùë°ùë°) = denotes the dot product between the two vectors, and Here, denotes the Euclidean norm. This score quantiÔ¨Åes ÔøΩ|vùë†ùë†|ÔøΩ ÔøΩ|vùë°ùë°|ÔøΩ the closeness of the semantic meaning represented by the vectors, with a value of 1 indicating identical directionality ÔøΩ|‚ãÖ|ÔøΩ ‚ãÖ and thus, maximal similarity.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7907972931861877, 'coordinates': {'points': ((np.float64(199.06227111816406), np.float64(2041.833740234375)), (np.float64(199.06227111816406), np.float64(2064.45751953125)), (np.float64(393.99896240234375), np.float64(2064.45751953125)), (np.float64(393.99896240234375), np.float64(2041.833740234375))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "ISSN: 2167-1907\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9403257966041565, 'coordinates': {'points': ((np.float64(199.77186584472656), np.float64(210.1004999999999)), (np.float64(199.77186584472656), np.float64(351.4338333333332)), (np.float64(1506.7661666666654), np.float64(351.4338333333332)), (np.float64(1506.7661666666654), np.float64(210.1004999999999))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      "The experiment tests diÔ¨Äerent conÔ¨Ågurations by substituting with embedding functions from various mod- els (e.g., GPT, Palm), allowing us to discern the impact of the embedding layer on the Ô¨Ånal similarity score. By com- ùê∏ùê∏ paring the performance of diÔ¨Äerent choices, we can identify which embeddings yield the most semantically rich representations for our unique dataset.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9085817933082581, 'coordinates': {'points': ((np.float64(197.5093536376953), np.float64(482.43383333333344)), (np.float64(197.5093536376953), np.float64(548.1005)), (np.float64(1506.8863333333325), np.float64(548.1005)), (np.float64(1506.8863333333325), np.float64(482.43383333333344))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      "The experiment commences with the training of the language models using our unique dataset. For the training phase, we deÔ¨Åne the following:\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7520370483398438, 'coordinates': {'points': ((np.float64(205.61119079589844), np.float64(558.1005000000001)), (np.float64(205.61119079589844), np.float64(585.9338333333334)), (np.float64(910.4736666666665), np.float64(585.9338333333334)), (np.float64(910.4736666666665), np.float64(558.1005000000001))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      ": The base language model, which can be either GPT or Palm.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7297148704528809, 'coordinates': {'points': ((np.float64(200.0), np.float64(595.9338333333335)), (np.float64(200.0), np.float64(661.6005000000001)), (np.float64(1506.9224333333332), np.float64(661.6005000000001)), (np.float64(1506.9224333333332), np.float64(595.9338333333335))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      ": The training dataset, consisting of pairs ‚Ñí answer. where is a query from the dataset and is the corresponding\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.857230544090271, 'coordinates': {'points': ((np.float64(200.0), np.float64(671.6005000000001)), (np.float64(200.0), np.float64(737.2671666666668)), (np.float64(1506.7946499999996), np.float64(737.2671666666668)), (np.float64(1506.7946499999996), np.float64(671.6005000000001))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      "The language model is Ô¨Åne-tuned on , optimizing the weights to minimize the loss function, typically a cross-entropy loss between the predicted and actual answers.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9097127318382263, 'coordinates': {'points': ((np.float64(196.37156677246094), np.float64(746.5118408203125)), (np.float64(196.37156677246094), np.float64(850.7671666666666)), (np.float64(1506.9382166666664), np.float64(850.7671666666666)), (np.float64(1506.9382166666664), np.float64(746.5118408203125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      "Following training, the question-answering process involves feeding a new query to the trained model and retrieving the answer . This answer is then compared to a predeÔ¨Åned list of correct answers using the similarity score, which is ùëûùëû‚Ä≤ fundamental to evaluating the model's performance.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8631711006164551, 'coordinates': {'points': ((np.float64(200.0), np.float64(900.4666666666667)), (np.float64(200.0), np.float64(933.8)), (np.float64(610.1666666666667), np.float64(933.8)), (np.float64(610.1666666666667), np.float64(900.4666666666667))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      "Benchmarking and Evaluation\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9365326166152954, 'coordinates': {'points': ((np.float64(200.0), np.float64(978.1532592773438)), (np.float64(200.0), np.float64(1082.2671666666665)), (np.float64(1506.9197333333334), np.float64(1082.2671666666665)), (np.float64(1506.9197333333334), np.float64(978.1532592773438))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '92b2b0c686c9253bc2fe5fcddd2254ff'}\n",
      "The evaluation metric for our experiment is based on the similarity scores between the generated responses and a set of reference answers. Let be the set of answers generated by the model, and be the set of reference answers. We deÔ¨Åne the average similarity score as follows: ‚Ä≤ ‚Ä≤ ‚Ä≤ ‚àó ‚àó ‚àó\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9335139393806458, 'coordinates': {'points': ((np.float64(194.30596923828125), np.float64(1178.0813333333333)), (np.float64(194.30596923828125), np.float64(1286.6005)), (np.float64(1506.9252999999999), np.float64(1286.6005)), (np.float64(1506.9252999999999), np.float64(1178.0813333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      ") This average score acts as the primary benchmark for comparing diÔ¨Äerent model conÔ¨Ågurations. We systematically n ùëñùëñ=1 record the scores across various combinations of language models and similarity scoring mechanisms to assess which conÔ¨Ågurations yield the highest average similarity, indicating the most eÔ¨Äective model setup for our dataset.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9331151843070984, 'coordinates': {'points': ((np.float64(198.49351501464844), np.float64(1296.436279296875)), (np.float64(198.49351501464844), np.float64(1437.933833333333)), (np.float64(1506.9531333333332), np.float64(1437.933833333333)), (np.float64(1506.9531333333332), np.float64(1296.436279296875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Additionally, we account for the presence or absence of context in the model's training and response genera- tion. This is critical, as the presence of context has been shown to signiÔ¨Åcantly inÔ¨Çuence model performance, particu- larly in the domain of indigenous knowledge and biodiversity, where context provides essential background information that can drastically aÔ¨Äect the meaning and relevance of a response.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9340912103652954, 'coordinates': {'points': ((np.float64(200.0), np.float64(1447.9338333333335)), (np.float64(200.0), np.float64(1551.4338333333333)), (np.float64(1507.0310666666662), np.float64(1551.4338333333333)), (np.float64(1507.0310666666662), np.float64(1447.9338333333335))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Through this meticulous experimental setup, we aim to illuminate the intricate dynamics between diÔ¨Äerent components of the RAG framework and their collective impact on the model's ability to accurately replicate and convey the richness of the Amazon Rainforest's cultural and ecological knowledge.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.859574556350708, 'coordinates': {'points': ((np.float64(200.0), np.float64(1601.1333333333332)), (np.float64(200.0), np.float64(1634.4666666666667)), (np.float64(986.625), np.float64(1634.4666666666667)), (np.float64(986.625), np.float64(1601.1333333333332))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Pre-Experiment Performance Expectations and Discussion\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9320123195648193, 'coordinates': {'points': ((np.float64(200.0), np.float64(1681.2344970703125)), (np.float64(200.0), np.float64(1823.7671666666668)), (np.float64(1506.87365), np.float64(1823.7671666666668)), (np.float64(1506.87365), np.float64(1681.2344970703125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '27d1690292d036e8523fca1658e455d7'}\n",
      "In the landscape of varying conÔ¨Ågurations, we hypothesize that certain setups will yield higher average similarity scores than others, indicative of more nuanced and accurate language generation. Particularly, we expect that: The similarity scores for models trained with contextual data will surpass those trained without, due to the enriched understanding and background the model has of the subject matter.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9324306845664978, 'coordinates': {'points': ((np.float64(200.0), np.float64(1833.7671666666665)), (np.float64(200.0), np.float64(1976.4338333333333)), (np.float64(1506.6747999999989), np.float64(1976.4338333333333)), (np.float64(1506.6747999999989), np.float64(1833.7671666666665))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '27d1690292d036e8523fca1658e455d7'}\n",
      "When aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em- bed), the semantic vector representations should align more closely, thus producing higher similarity scores. The modular nature of the RAG setup will reveal that certain combinations of base language models and similarity (ùêØùêØùë†ùë†,ùêØùêØùë°ùë°) scoring mechanisms are more eÔ¨Äective than others, depending on whether context is included or not.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.6920047998428345, 'coordinates': {'points': ((np.float64(198.96026611328125), np.float64(2041.7052001953125)), (np.float64(198.96026611328125), np.float64(2064.4912109375)), (np.float64(393.9510803222656), np.float64(2064.4912109375)), (np.float64(393.9510803222656), np.float64(2041.7052001953125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '27d1690292d036e8523fca1658e455d7'}\n",
      "ISSN: 2167-1907\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9217211008071899, 'coordinates': {'points': ((np.float64(199.63858032226562), np.float64(210.1004999999999)), (np.float64(199.63858032226562), np.float64(275.7671666666665)), (np.float64(1506.8817666666666), np.float64(275.7671666666665)), (np.float64(1506.8817666666666), np.float64(210.1004999999999))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '693e507e0a18eba8508f6769e97b0318'}\n",
      "We denote the expected performance increase due to context as , and the alignment of native embeddings as . Mathematically, we can represent our hypothesis as:\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9192385077476501, 'coordinates': {'points': ((np.float64(200.0), np.float64(358.4338333333335)), (np.float64(200.0), np.float64(424.1005000000001)), (np.float64(1499.7415166666665), np.float64(424.1005000000001)), (np.float64(1499.7415166666665), np.float64(358.4338333333335))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "These hypotheses will be tested through a series of experiments, allowing us to determine the optimal model conÔ¨Ågu- Scoreavg,native > Scoreavg,non‚àínative + ‚àÜnative ration for processing and generating responses reÔ¨Çective of the Amazon Rainforest dataset.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8624512553215027, 'coordinates': {'points': ((np.float64(200.0), np.float64(473.79999999999995)), (np.float64(200.0), np.float64(507.13333333333327)), (np.float64(636.4583333333333), np.float64(507.13333333333327)), (np.float64(636.4583333333333), np.float64(473.79999999999995))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Potential Implications for LLMs\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9328924417495728, 'coordinates': {'points': ((np.float64(200.0), np.float64(554.8651123046875)), (np.float64(200.0), np.float64(696.4338333333334)), (np.float64(1506.9419999999998), np.float64(696.4338333333334)), (np.float64(1506.9419999999998), np.float64(554.8651123046875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '7c0b6861e44723d5687f177358333262'}\n",
      "The results of this experiment are expected to have signiÔ¨Åcant implications for the development and Ô¨Åne-tuning of Large Language Models (LLMs). By identifying the most eÔ¨Äective conÔ¨Ågurations, we can oÔ¨Äer insights into the adapt- ability of these models to specialized datasets, which is crucial for applications that require a high degree of cultural and contextual sensitivity.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9336577653884888, 'coordinates': {'points': ((np.float64(200.0), np.float64(706.4338333333334)), (np.float64(200.0), np.float64(809.9338333333334)), (np.float64(1506.9396166666666), np.float64(809.9338333333334)), (np.float64(1506.9396166666666), np.float64(706.4338333333334))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '7c0b6861e44723d5687f177358333262'}\n",
      "Moreover, the experiment is poised to challenge the prevailing approach to LLM training and Ô¨Åne-tuning, which often relies on static, one-size-Ô¨Åts-all models. Our Ô¨Åndings could suggest a shift towards a more dynamic, com- ponent-based approach, allowing for greater Ô¨Çexibility and precision in model performance across diverse domains.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9370346069335938, 'coordinates': {'points': ((np.float64(200.0), np.float64(819.9338333333334)), (np.float64(200.0), np.float64(961.2671666666666)), (np.float64(1506.8334499999987), np.float64(961.2671666666666)), (np.float64(1506.8334499999987), np.float64(819.9338333333334))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '7c0b6861e44723d5687f177358333262'}\n",
      "The potential success of the RAG framework in this context may also pave the way for more granular im- provements in LLMs, beyond the standard metrics of accuracy and Ô¨Çuency. It may, for instance, enhance the models' ability to engage with and preserve less-represented languages and dialects, fostering greater inclusivity and diversity in the realm of natural language processing.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8610591888427734, 'coordinates': {'points': ((np.float64(200.0), np.float64(1010.9666666666666)), (np.float64(200.0), np.float64(1044.3)), (np.float64(905.4583333333333), np.float64(1044.3)), (np.float64(905.4583333333333), np.float64(1010.9666666666666))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Implications for Indigenous Knowledge Preservation\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9484277963638306, 'coordinates': {'points': ((np.float64(198.09974670410156), np.float64(1091.2169189453125)), (np.float64(198.09974670410156), np.float64(1271.4338333333333)), (np.float64(1506.9921), np.float64(1271.4338333333333)), (np.float64(1506.9921), np.float64(1091.2169189453125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87631654c36409c301800ae12174efa3'}\n",
      "The signiÔ¨Åcance of our experiment extends beyond the technical accomplishments within the Ô¨Åeld of natural language processing. It serves as a testament to the power of advanced computational techniques in preserving the rich tapestry of human culture, particularly the imperiled knowledge of the Amazon Rainforest's indigenous peoples. By success- fully training a language model to accurately reÔ¨Çect and communicate this knowledge, we not only preserve it for future generations but also validate the importance of linguistic and cultural diversity in our global ecosystem.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9468027949333191, 'coordinates': {'points': ((np.float64(200.0), np.float64(1281.4338333333333)), (np.float64(200.0), np.float64(1460.893310546875)), (np.float64(1506.911383333333), np.float64(1460.893310546875)), (np.float64(1506.911383333333), np.float64(1281.4338333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87631654c36409c301800ae12174efa3'}\n",
      "This experiment, should it succeed, will demonstrate a practical application of LLMs in the service of cultural preservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns with the broader objectives of UNESCO's Intangible Cultural Heritage initiatives. It serves as a model for how com- munities around the world can leverage technology to protect and share their unique cultural identities and knowledge systems.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8661386370658875, 'coordinates': {'points': ((np.float64(200.0), np.float64(1510.3)), (np.float64(200.0), np.float64(1543.6333333333332)), (np.float64(673.125), np.float64(1543.6333333333332)), (np.float64(673.125), np.float64(1510.3))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Advancements in RAG Framework\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9382249116897583, 'coordinates': {'points': ((np.float64(200.0), np.float64(1591.669921875)), (np.float64(200.0), np.float64(1733.1004999999998)), (np.float64(1506.9614833333328), np.float64(1733.1004999999998)), (np.float64(1506.9614833333328), np.float64(1591.669921875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '76e2ebfd0a7eb2a1bb6e88696ed6bfe6'}\n",
      "From a methodological standpoint, our experiment is poised to contribute to the advancement of the RAG framework within the realm of AI language models. By dissecting the RAG components and examining their interplay, we will gain insights into the mechanics of modular design in language models, oÔ¨Äering a blueprint for future research and development.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9353484511375427, 'coordinates': {'points': ((np.float64(200.0), np.float64(1743.1004999999998)), (np.float64(200.0), np.float64(1884.433833333333)), (np.float64(1506.8422000000005), np.float64(1884.433833333333)), (np.float64(1506.8422000000005), np.float64(1743.1004999999998))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '76e2ebfd0a7eb2a1bb6e88696ed6bfe6'}\n",
      "The outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable framework, one that can be customized for specialized datasets and applications. This adaptability is critical as the demand for LLMs expands into increasingly varied and complex domains, from legal and medical to historical and anthropological.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9395981431007385, 'coordinates': {'points': ((np.float64(198.3177490234375), np.float64(1894.4136962890625)), (np.float64(198.3177490234375), np.float64(1997.9338333333333)), (np.float64(1501.3900146484375), np.float64(1997.9338333333333)), (np.float64(1501.3900146484375), np.float64(1894.4136962890625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '76e2ebfd0a7eb2a1bb6e88696ed6bfe6'}\n",
      "Furthermore, the experiment's focus on modularity could inspire a new wave of research into component- based architectures for LLMs. Such architectures may provide a more sustainable and eÔ¨Écient pathway to model im- provement, as opposed to the computationally intensive process of training large models from scratch.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7013646364212036, 'coordinates': {'points': ((np.float64(199.24911499023438), np.float64(2041.9556884765625)), (np.float64(199.24911499023438), np.float64(2064.46435546875)), (np.float64(394.8359375), np.float64(2064.46435546875)), (np.float64(394.8359375), np.float64(2041.9556884765625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '76e2ebfd0a7eb2a1bb6e88696ed6bfe6'}\n",
      "ISSN: 2167-1907\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9353991746902466, 'coordinates': {'points': ((np.float64(200.0), np.float64(210.1004999999999)), (np.float64(200.0), np.float64(313.6004999999999)), (np.float64(1507.0199333333328), np.float64(313.6004999999999)), (np.float64(1507.0199333333328), np.float64(210.1004999999999))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'a6cb6cb932678cc1906b79982556992b'}\n",
      "In conclusion, the proposed experiment holds the potential to make signiÔ¨Åcant contributions to both the Ô¨Åeld of AI and the preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative future for LLMs, where the voices of all communities can be heard and understood.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9215583205223083, 'coordinates': {'points': ((np.float64(199.0110321044922), np.float64(995.4058333333329)), (np.float64(199.0110321044922), np.float64(1088.6031494140625)), (np.float64(1507.1117833333333), np.float64(1088.6031494140625)), (np.float64(1507.1117833333333), np.float64(995.4058333333329))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Figure 1. Venn Diagram of Data Sources for RAG. This Ô¨Ågure represents a venn diagram of 3 sources of information (google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the ‚Äúout-\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7436774969100952, 'coordinates': {'points': ((np.float64(199.97216666666668), np.float64(1883.079345703125)), (np.float64(199.97216666666668), np.float64(1949.8743333333332)), (np.float64(1506.7638666666653), np.float64(1949.8743333333332)), (np.float64(1506.7638666666653), np.float64(1883.079345703125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Figure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG algorithm from the input of the ‚Äúuser question‚Äù to the ‚Äúoutputted answer of the user question.‚Äù\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8654903173446655, 'coordinates': {'points': ((np.float64(199.78990173339844), np.float64(214.16616666666673)), (np.float64(199.78990173339844), np.float64(252.9995000000001)), (np.float64(788.558), np.float64(252.9995000000001)), (np.float64(788.558), np.float64(214.16616666666673))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'c1a2323b922a0aff37859dc4c1997680'}\n",
      "Experimental Results and Discussion\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9537456035614014, 'coordinates': {'points': ((np.float64(194.01597595214844), np.float64(300.9338333333334)), (np.float64(194.01597595214844), np.float64(555.7671666666668)), (np.float64(1508.20068359375), np.float64(555.7671666666668)), (np.float64(1508.20068359375), np.float64(300.9338333333334))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "The purpose of this section is to lay down the diÔ¨Äerent steps and customizations used within our experiment in order to demonstrate the conclusive results of this experiment to the reader; our experiment using a RAG methodology accurately shows how each component of a LLM positively or negatively aÔ¨Äects the accuracy of the outcome itself. In the initial world of LLM, in order to incrementally increase its performance engineers of these models would have to Ô¨Åne tune them then retrain which took immense amounts of power and large amounts of null results. However, now as they become more and more complex to tune models like OpenAI‚Äôs GPT and Google‚Äôs Bard have been plateauing performance wise.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9249820113182068, 'coordinates': {'points': ((np.float64(190.36485290527344), np.float64(603.5848333333332)), (np.float64(190.36485290527344), np.float64(669.2514999999999)), (np.float64(1507.1143798828125), np.float64(669.2514999999999)), (np.float64(1507.1143798828125), np.float64(603.5848333333332))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "Table 1. Experimental Results. This table represents the various diÔ¨Äerent combinations of LLM components with respect to the average similarity score they each produced.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9514713883399963, 'coordinates': {'points': ((np.float64(196.515869140625), np.float64(1140.9338333333335)), (np.float64(196.515869140625), np.float64(1357.9338333333333)), (np.float64(1507.070033333333), np.float64(1357.9338333333333)), (np.float64(1507.070033333333), np.float64(1140.9338333333335))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "This paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to com- ponentize the models and break them down into smaller sections. This allows the user to add certain parts / combina- tions to test the performance of those then to substitute diÔ¨Äerent modules in to see which leads to the largest perfor- mance increase over the other. These customizable steps allow you to see minute diÔ¨Äerences in performance that slowly tuning a model couldn‚Äôt have shown you previously. This is a novel way to approach the tuning of LLM and will only serve to increase their accuracy as time moves forward.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9328656792640686, 'coordinates': {'points': ((np.float64(196.4347381591797), np.float64(1367.9338333333333)), (np.float64(196.4347381591797), np.float64(1471.4338333333333)), (np.float64(1506.8167499999993), np.float64(1471.4338333333333)), (np.float64(1506.8167499999993), np.float64(1367.9338333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "Another major component of our RAG methodology is the ability to switch out which embedding layer you use. The standard embedding (OpenAI) or Palm‚Äôs embed. When choosing between both of those some tradeoÔ¨Äs are made.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.935149610042572, 'coordinates': {'points': ((np.float64(200.0), np.float64(1481.4338333333333)), (np.float64(200.0), np.float64(1584.9338333333333)), (np.float64(1506.9396166666668), np.float64(1584.9338333333333)), (np.float64(1506.9396166666668), np.float64(1481.4338333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "When using no context, Palm‚Äôs embedding layer seems to perform much better across the board, allowing for a much higher average similarity score, however this drastically shifts when given context as now OpenAI‚Äôs embedding layer performs much more soundly. The evidence for these claims is discussed later in this paper.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9488053917884827, 'coordinates': {'points': ((np.float64(200.0), np.float64(1594.933833333333)), (np.float64(200.0), np.float64(1774.1005)), (np.float64(1506.9893166666664), np.float64(1774.1005)), (np.float64(1506.9893166666664), np.float64(1594.933833333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "Additionally, another beneÔ¨Åcial feature of the RAG optimization and breakdown style is the ability to cus- tomize which similarity score the LLM uses to decide which answer to base its response oÔ¨Ä from the Q-A list. To go into further detail, the code when prompted with a user question compares the user question to the Q-A list and reorders the list based oÔ¨Ä highest similarity score to lowest, this allows the LLM to select the top 2-3 answers to the highest ranked questions and continue generating its own response from there.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9461125135421753, 'coordinates': {'points': ((np.float64(200.0), np.float64(1784.1004999999998)), (np.float64(200.0), np.float64(1963.2671666666668)), (np.float64(1506.9865333333332), np.float64(1963.2671666666668)), (np.float64(1506.9865333333332), np.float64(1784.1004999999998))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "The Ô¨Årst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In terms of the data when GPT (for the purposes of precision all of the following results include context) and STS OpenAI were combined, you got an average similarity score of 0.997. If you instead pair this with Palm STS Score instead, the average score drops to 0.92, a 0.077 decrease in performance. A similar eÔ¨Äect when using Palm with Palm STS and Palm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.6884109973907471, 'coordinates': {'points': ((np.float64(198.56924438476562), np.float64(2041.5426025390625)), (np.float64(198.56924438476562), np.float64(2064.440673828125)), (np.float64(394.0348815917969), np.float64(2064.440673828125)), (np.float64(394.0348815917969), np.float64(2041.5426025390625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "ISSN: 2167-1907\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9441673159599304, 'coordinates': {'points': ((np.float64(197.56947326660156), np.float64(210.1004999999999)), (np.float64(197.56947326660156), np.float64(389.2671666666666)), (np.float64(1506.9475666666667), np.float64(389.2671666666666)), (np.float64(1506.9475666666667), np.float64(210.1004999999999))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "to reach very high accuracy levels when paired with a similarity score calculated from the same program (this means GPT worked better with OpenAI STS Score and that Palm worked better with Palm STS Score). What is also interest- ing to note is that although Palm produced a 0.001 lower performance than GPT it seemed to be more Ô¨Çexible, working better with its competitor (OpenAI STS Score + Palm produced 0.93) than the GPT with its competitor (Palm STS Score + GPT produced 0.92).\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9382283091545105, 'coordinates': {'points': ((np.float64(200.0), np.float64(399.26716666666664)), (np.float64(200.0), np.float64(540.6004999999999)), (np.float64(1507.0060166666663), np.float64(540.6004999999999)), (np.float64(1507.0060166666663), np.float64(399.26716666666664))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "To recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap or replace things like the embedding layer used, the similarity score used, and the base language model used, also whether it was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM and the data collected speaks a lot to the importance of each aspect of an LLM.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9523764848709106, 'coordinates': {'points': ((np.float64(200.0), np.float64(550.6005)), (np.float64(200.0), np.float64(767.6005)), (np.float64(1506.9698333333329), np.float64(767.6005)), (np.float64(1506.9698333333329), np.float64(550.6005))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "In terms of expected results, two things were noticed, Ô¨Årst that Palm had a slightly lower performance than GPT (0.996 versus 0.997 respectively) on their top runs, however, there was also some contradictory data as it seemed that Palm worked signiÔ¨Åcantly better when given no context to work with compared to GPT, Palm produced an average score of 0.88 and 0.91 when given no context while GPT produced an average score of 0.75 and 0.897. Although GPT may perform much better when given context, Palm seems to beat it out just given its own proprietary dataset (no context).\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9552424550056458, 'coordinates': {'points': ((np.float64(200.0), np.float64(777.6005000000001)), (np.float64(200.0), np.float64(1070.2671666666665)), (np.float64(1506.9809666666663), np.float64(1070.2671666666665)), (np.float64(1506.9809666666663), np.float64(777.6005000000001))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "Some unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, pair- ing Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually performed better when paired with OpenAI‚Äôs embed. 0.91 (with OpenAI embed) versus 0.88 (Palm embed)‚Äînote that this is without context given. A similar eÔ¨Äect was noticed going the other way around as well, without context, GPT performed much better with Palm embedding layer than with its own OpenAI embedding layer (0.897 versus 0.75 respectively). This data shows how Palm embedding layer tends to perform much better given no context when com- pared to OpenAI‚Äôs embed. Similarly, to explored above, it is the opposite when given context, however. OpenAI‚Äôs embedding layer performs a bit better when given context across the board than Palm embed.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9496419429779053, 'coordinates': {'points': ((np.float64(200.0), np.float64(1080.2671666666668)), (np.float64(200.0), np.float64(1259.4338333333333)), (np.float64(1506.9758), np.float64(1259.4338333333333)), (np.float64(1506.9758), np.float64(1080.2671666666668))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "Most notably from this experiment was two realizations. First, that Palm Embedding layer tends to work much better when just given its own proprietary dataset (and no context), when compared to OpenAI‚Äôs embed. Additionally, when given context, the playing Ô¨Åeld switches: Palm tends to perform much worse when given context when compared to OpenAI. Lastly, it is important to note that a combination of Palm/GPT with OpenAI‚Äôs embedding layer and context yielded extremely accurate results when its similarity scores were averaged.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8832968473434448, 'coordinates': {'points': ((np.float64(200.0), np.float64(1312.3036666666667)), (np.float64(200.0), np.float64(1351.137)), (np.float64(393.0405), np.float64(1351.137)), (np.float64(393.0405), np.float64(1312.3036666666667))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "Conclusion\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9510577917098999, 'coordinates': {'points': ((np.float64(200.0), np.float64(1397.6414794921875)), (np.float64(200.0), np.float64(1615.1005)), (np.float64(1506.9392166666667), np.float64(1615.1005)), (np.float64(1506.9392166666667), np.float64(1397.6414794921875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '94bc826e9877b9de2bb6079c421d594a'}\n",
      "Building upon the foundation laid by our initial Ô¨Åndings, it is paramount to recognize the exceptional performance of the Palm model when utilizing its proprietary dataset in conjunction with the Palm embed. This speciÔ¨Åcity in data and technology synchronization has shown that Palm outshines OpenAI in terms of model accuracy in a context-free envi- ronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT with its native OpenAI embedding layer excels, leveraging the additional context to produce responses of remarkable accuracy that resonate with the cultural and ecological nuances of the Amazon.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9465005397796631, 'coordinates': {'points': ((np.float64(198.11744689941406), np.float64(1625.1005)), (np.float64(198.11744689941406), np.float64(1804.2671666666668)), (np.float64(1506.9447833333327), np.float64(1804.2671666666668)), (np.float64(1506.9447833333327), np.float64(1625.1005))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '94bc826e9877b9de2bb6079c421d594a'}\n",
      "This pivot in performance based on context underscores the signiÔ¨Åcance of tailored datasets and embedding mechanisms in the optimization of Large Language Models (LLMs). The adaptability of the Retrieval-Augmented Generation (RAG) framework emerges as a cornerstone for future enhancements in LLMs. By enabling the seamless interchange of model components, RAG presents an evolutionary leap in the Ô¨Åne-tuning of language models, catering to the intricate demands of culturally rich and contextually complex datasets.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9392344951629639, 'coordinates': {'points': ((np.float64(200.0), np.float64(1814.2671666666665)), (np.float64(200.0), np.float64(1993.4338333333333)), (np.float64(1506.9698333333333), np.float64(1993.4338333333333)), (np.float64(1506.9698333333333), np.float64(1814.2671666666665))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '94bc826e9877b9de2bb6079c421d594a'}\n",
      "In light of these advancements, our research signiÔ¨Åes a pivotal moment for LLMs. The evidence suggests that when models are Ô¨Ånely tuned with an awareness of the dataset's inherent context and the corresponding embedding layers, they reach new heights of linguistic precision. Therefore, the path forward for LLMs lies in embracing the modular and contextually aware RAG framework, which promises to reÔ¨Åne the capabilities of language models to an unprecedented degree, ensuring the preservation and celebration of the world's diverse linguistic and cultural heritage.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.6504687070846558, 'coordinates': {'points': ((np.float64(199.09689331054688), np.float64(2041.906494140625)), (np.float64(199.09689331054688), np.float64(2064.438720703125)), (np.float64(394.32049560546875), np.float64(2064.438720703125)), (np.float64(394.32049560546875), np.float64(2041.906494140625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '94bc826e9877b9de2bb6079c421d594a'}\n",
      "ISSN: 2167-1907\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8782328963279724, 'coordinates': {'points': ((np.float64(199.386962890625), np.float64(215.13699999999983)), (np.float64(199.386962890625), np.float64(253.97033333333323)), (np.float64(397.54516666666666), np.float64(253.97033333333323)), (np.float64(397.54516666666666), np.float64(215.13699999999983))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '7f3974627d669c2a2a480412b0cd17a8'}\n",
      "Limitations\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9373156428337097, 'coordinates': {'points': ((np.float64(198.4431915283203), np.float64(300.9338333333334)), (np.float64(198.4431915283203), np.float64(404.4338333333333)), (np.float64(1506.9503499999998), np.float64(404.4338333333333)), (np.float64(1506.9503499999998), np.float64(300.9338333333334))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "The results of the ‚Äúoutputted answer‚Äù of this algorithm largely reÔ¨Çect the quality of the data. If the LLM is trained oÔ¨Ä low-quality data, then the answer will reÔ¨Çect this bias. The results of the experiment will Ô¨Çuctuate with diÔ¨Äerent results should a diÔ¨Äerent dataset be used to train the LLM.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9341223835945129, 'coordinates': {'points': ((np.float64(199.07540893554688), np.float64(414.42599999999993)), (np.float64(199.07540893554688), np.float64(517.9181666666665)), (np.float64(1506.853333333332), np.float64(517.9181666666665)), (np.float64(1506.853333333332), np.float64(414.42599999999993))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "In the future there is a lot of potential to expand on this research by breaking down the RAG algorithm into even more separate components to further see the diÔ¨Äerences in average similarity score that adding or removing each component makes.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7257945537567139, 'coordinates': {'points': ((np.float64(199.97216666666668), np.float64(656.3143333333333)), (np.float64(199.97216666666668), np.float64(722.2514999999996)), (np.float64(1486.3360595703125), np.float64(722.2514999999996)), (np.float64(1486.3360595703125), np.float64(656.3143333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "Bahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP), 4945-4949.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7626606225967407, 'coordinates': {'points': ((np.float64(199.97216666666668), np.float64(731.965333333333)), (np.float64(199.97216666666668), np.float64(790.7501831054688)), (np.float64(1504.1391833333332), np.float64(790.7501831054688)), (np.float64(1504.1391833333332), np.float64(731.965333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "Devlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint,\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.8412608504295349, 'coordinates': {'points': ((np.float64(196.36825561523438), np.float64(807.6163333333329)), (np.float64(196.36825561523438), np.float64(873.5534999999996)), (np.float64(1394.230224609375), np.float64(873.5534999999996)), (np.float64(1394.230224609375), np.float64(807.6163333333329))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "Lewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 9459-9474.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.5641652345657349, 'coordinates': {'points': ((np.float64(193.1947479248047), np.float64(883.267333333333)), (np.float64(193.1947479248047), np.float64(1135.1868333333332)), (np.float64(1478.4712833333322), np.float64(1135.1868333333332)), (np.float64(1478.4712833333322), np.float64(883.267333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "Radford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. Siriwardhana, S. W. (2023). Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics, 1-17. Vaswani, A. S. (2017). Attention is all you need. Advances in neural information processing systems. Yu, W. (2022). Retrieval-augmented generation across heterogeneous knowledge. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, 52-58.\n"
     ]
    }
   ],
   "source": [
    "for el in text_elements:\n",
    "    print(f\"Category:{el.category}\")\n",
    "    print(f\"Metadata:{el.metadata.to_dict()}\")\n",
    "    print(el.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74e3fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put text elements into docs\n",
    "text_docs = [\n",
    "    Document(page_content=el.text, metadata={\"source\":f\"{el.metadata.to_dict().get('file_directory', '')}/{el.metadata.to_dict().get('filename', '')}\", \"type\":\"text\", \"page_number\": el.metadata.to_dict().get(\"page_number\") })\n",
    "    for el in text_elements\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd802d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='A Retrieval-Augmented Generation Based Large Language Model Benchmarked on a Novel Dataset'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Kieran Pichai'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Menlo School'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='ABSTRACT'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='The evolution of natural language processing has seen marked advancements, particularly with the advent of models like BERT, Transformers, and GPT variants, with recent additions like GPT and Bard. This paper investigates the Retrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its constituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our research demonstrates the signiÔ¨Åcance of preserving indigenous cultures and biodiversity. The experiment employs a customizable RAG methodology, allowing for the interchangeability of various components, such as the base language model and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm exhibits superior performance without context. The results also suggest that models tend to perform optimally when paired with similarity scores from their native platforms. Conclusively, our approach showcases the potential of a modular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to tra- ditional Ô¨Åne-tuning of large language models.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Introduction'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='The evolution of natural language processing models has seen signiÔ¨Åcant strides from rule-based approaches in the early stages of language understanding, eventually leading to the advent of neural networks. However, the full potential of these neural networks awaited the computational infrastructure to catch up. The pivotal moment arrived with the emergence of neural machine translation (NMT), exempliÔ¨Åed by Google Translate, which marked a turning point in machine language comprehension (Bahdanau, 2016). Subsequently, a plethora of advanced models, including BERT, Transformers, GPT-2, and GPT-3, have emerged, driving the Ô¨Åeld forward. Recent notable additions to this landscape are models like GPT and Bard (Devlin, 2018) (Vaswani, 2017) (Radford, 2018). While Ô¨Åne-tuning such models has proven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG) oÔ¨Äers a prom- ising alternative (Lewis, 2020) (Siriwardhana, 2023) (Yu, 2022).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Curiously, little attention has been devoted to dissecting the individual components of RAG and their respec- tive impacts on overall performance. In response to this gap, our paper undertakes a comprehensive investigation of the RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity and replaceability of its constituent modules. This research seeks to contribute to a deeper understanding of the mech- anisms underlying RAG and its potential for enhancing natural language understanding and generation. These Large Language Models (LLMs) exhibit a remarkable proÔ¨Åciency in replicating human language styles, achieving a level of linguistic verisimilitude that borders on the impeccable. In light of these capabilities, it is prudent to delve into the profound signiÔ¨Åcance of the Amazon rainforest, which equates to the importance of any ethnically or racially diverse nation across the globe. Within the vast expanse of the Amazon, an intricate tapestry of life unfolds, where millions of distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the indigenous populations who have made this ecosystem their home. The Amazon rainforest is not only a cradle of biological diversity but also a sanctuary for an array of religions and cultures, many of which teeter on the brink of'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice to the indigenous communities whose ancestral lands are enshrined within its boundaries. It is a call to safeguard the memories of the land, the traditions that have evolved within its embrace, and the very essence of their cultures. However, certain regions of the Amazon remain shrouded in obscurity, their Ô¨Çora and fauna so rare that reliable and readily available information is conspicuously lacking in the vast repository of knowledge available on the internet. In this context, advanced LLMs play an instrumental role in addressing this deÔ¨Åcit by facilitating the dissemination of indigenous narratives and thereby amplifying awareness and appreciation of the rich tapestry of beliefs, practices, and traditional knowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures with the global community, emphasizing the paramount importance of preserving the cultural diversity interwoven within this vast rainforest. In sum, the overarching mission of this endeavor is twofold: to document and educate the Western world about hitherto unknown cultures while concurrently ensuring the enduring preservation of these inval- uable facets of human heritage and biodiversity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Proposed Experiment'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Background and Importance'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally rich as the Amazon Rainforest, cannot be overstated. This knowledge, passed down through generations, encompasses not only cultural and religious beliefs but also practical insights into the local Ô¨Çora and fauna. As the modern world encroaches on these lands, this wisdom is in peril of being lost forever. Recognizing this, our proposed experiment aims to employ a state- of-the-art Retrieval-Augmented Generation (RAG) framework to capture and leverage this vast, yet vulnerable, knowledge base.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Our dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled in its depth and breadth. It includes detailed discussions on religious practices, cultural nuances, and the integral role of the surrounding ecosystem in the daily lives of these communities. This data is not just a scientiÔ¨Åc or anthropological resource; it is a repository of living history and an urgent call to action for preservation eÔ¨Äorts.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='By integrating this unique dataset into the RAG framework, we anticipate not only the preservation of knowledge but also the generation of responses that reÔ¨Çect the rich tapestry of Amazonian life. The experiment is designed to evaluate how diÔ¨Äerent components within the RAG setup‚Äîsuch as base language models and similarity scoring algorithms‚Äîcan be optimized to reÔ¨Çect the nuances captured within our dataset. In doing so, we aim to bridge the gap between advanced language models and the profound human insights found within the Amazon.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The central objective of our experiment is twofold: to analyze the performance implications of modular design within the RAG framework and to demonstrate the profound capability of such a system to preserve and communicate the wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper understanding of the data but also allow us to Ô¨Åne-tune the system for optimal performance across diÔ¨Äerent conÔ¨Ågu- rations. To achieve this, we will systematically explore the interchangeability of various RAG components. We will assess diÔ¨Äerent base language models such as GPT and Palm and compare the eÔ¨Écacy of similarity scoring tools from diverse platforms. The experiment will rigorously test these combinations, identifying which synergies most eÔ¨Äectively capture the essence of the dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The ultimate goal is to showcase the potential of a modular RAG system in processing culturally signiÔ¨Åcant information, paving the way for future applications that can beneÔ¨Åt from such tailored language models. We anticipate that our Ô¨Åndings will contribute signiÔ¨Åcantly to the Ô¨Åelds of computational linguistics and cultural preservation, demon- strating a novel approach to the application of large language models.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Source and Composition'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Our proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of verbal histories, inter- views, and ecological insights gathered from the indigenous peoples of the Amazon Rainforest, as well as from biolo- gists and ecologists dedicated to studying this unique biome. The dataset is characterized by its diversity, comprising narratives that elucidate the intricate relationship between the natives and their environment, including the religious and cultural signiÔ¨Åcance of plant and animal life.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The data collection was an extensive process, where linguists and researchers engaged in deep conversations with the natives, recording their dialects, translating their stories, and documenting their knowledge of the ecological system. Similarly, biologists contributed their decades of research on the Ô¨Çora and fauna, providing a scientiÔ¨Åc per- spective to the indigenous narratives. The data thus forms a conÔ¨Çuence of traditional wisdom and modern scientiÔ¨Åc understanding, oÔ¨Äering a 360-degree view of the Amazon Rainforest's ecosystem.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Cultural and Environmental SigniÔ¨Åcance'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a race against time, as globalization and environmental degradation threaten to erase unique cultures and the wisdom they hold. Our dataset serves as a digital ark, a means to preserve and perpetuate the knowledge that has sustained the Amazon's communities\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The environmental signiÔ¨Åcance of the Amazon Rainforest cannot be overstated‚Äîit is a keystone of global biodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling the ecological interdependencies that are vital for the rainforest's survival. This dataset, therefore, is not just an aca- demic or technological asset; it is a critical record for environmental conservationists and policymakers.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"Through our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under- standing of their habitat is unmatched. By integrating their knowledge into the RAG framework, we hope to create a model that not only responds with information but also with wisdom that respects the interconnectedness of life and culture.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Retrieval-Augmented Generation Framework'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='The heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophisticated algorithm that enables the deconstruction of the language model into discrete, interchangeable components. This framework integrates a retriever model that sources relevant context and a generator model that synthesizes the retrieved infor- mation into coherent responses.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='In mathematical terms, given an input query , the retriever model searches a knowledge base and retrieves a set of relevant documents . Each document is represented as a vector in a high-dimensional ùí¶ùí¶ ùëûùëû space, obtained from an embedding layer. This process transforms the raw text data into a structured form amenable ùê∑ùê∑ = {ùëëùëë1,ùëëùëë2,‚Ä¶,ùëëùëëùëòùëò} ùêØùêØùëëùëë ùëëùëë to computational manipulation.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='To examine the eÔ¨Äects of component interchangeability, we adopt various base language models and similar- ity scoring mechanisms. For instance, if denotes the embedding function, and and represent the input and target text sequences, respectively, their vector representations would be ùê∏ùê∏ larity as the basis for our similarity score, deÔ¨Åned by the formula: and ùë†ùë† ùë°ùë° . We employ cosine simi-'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='similarity(ùêØùêØùë†ùë†,ùêØùêØùë°ùë°) = denotes the dot product between the two vectors, and Here, denotes the Euclidean norm. This score quantiÔ¨Åes ÔøΩ|vùë†ùë†|ÔøΩ ÔøΩ|vùë°ùë°|ÔøΩ the closeness of the semantic meaning represented by the vectors, with a value of 1 indicating identical directionality ÔøΩ|‚ãÖ|ÔøΩ ‚ãÖ and thus, maximal similarity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The experiment tests diÔ¨Äerent conÔ¨Ågurations by substituting with embedding functions from various mod- els (e.g., GPT, Palm), allowing us to discern the impact of the embedding layer on the Ô¨Ånal similarity score. By com- ùê∏ùê∏ paring the performance of diÔ¨Äerent choices, we can identify which embeddings yield the most semantically rich representations for our unique dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The experiment commences with the training of the language models using our unique dataset. For the training phase, we deÔ¨Åne the following:'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=': The base language model, which can be either GPT or Palm.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=': The training dataset, consisting of pairs ‚Ñí answer. where is a query from the dataset and is the corresponding'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The language model is Ô¨Åne-tuned on , optimizing the weights to minimize the loss function, typically a cross-entropy loss between the predicted and actual answers.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Following training, the question-answering process involves feeding a new query to the trained model and retrieving the answer . This answer is then compared to a predeÔ¨Åned list of correct answers using the similarity score, which is ùëûùëû‚Ä≤ fundamental to evaluating the model's performance.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='Benchmarking and Evaluation'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The evaluation metric for our experiment is based on the similarity scores between the generated responses and a set of reference answers. Let be the set of answers generated by the model, and be the set of reference answers. We deÔ¨Åne the average similarity score as follows: ‚Ä≤ ‚Ä≤ ‚Ä≤ ‚àó ‚àó ‚àó'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=') This average score acts as the primary benchmark for comparing diÔ¨Äerent model conÔ¨Ågurations. We systematically n ùëñùëñ=1 record the scores across various combinations of language models and similarity scoring mechanisms to assess which conÔ¨Ågurations yield the highest average similarity, indicating the most eÔ¨Äective model setup for our dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Additionally, we account for the presence or absence of context in the model's training and response genera- tion. This is critical, as the presence of context has been shown to signiÔ¨Åcantly inÔ¨Çuence model performance, particu- larly in the domain of indigenous knowledge and biodiversity, where context provides essential background information that can drastically aÔ¨Äect the meaning and relevance of a response.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Through this meticulous experimental setup, we aim to illuminate the intricate dynamics between diÔ¨Äerent components of the RAG framework and their collective impact on the model's ability to accurately replicate and convey the richness of the Amazon Rainforest's cultural and ecological knowledge.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='Pre-Experiment Performance Expectations and Discussion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='In the landscape of varying conÔ¨Ågurations, we hypothesize that certain setups will yield higher average similarity scores than others, indicative of more nuanced and accurate language generation. Particularly, we expect that: The similarity scores for models trained with contextual data will surpass those trained without, due to the enriched understanding and background the model has of the subject matter.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='When aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em- bed), the semantic vector representations should align more closely, thus producing higher similarity scores. The modular nature of the RAG setup will reveal that certain combinations of base language models and similarity (ùêØùêØùë†ùë†,ùêØùêØùë°ùë°) scoring mechanisms are more eÔ¨Äective than others, depending on whether context is included or not.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='We denote the expected performance increase due to context as , and the alignment of native embeddings as . Mathematically, we can represent our hypothesis as:'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='These hypotheses will be tested through a series of experiments, allowing us to determine the optimal model conÔ¨Ågu- Scoreavg,native > Scoreavg,non‚àínative + ‚àÜnative ration for processing and generating responses reÔ¨Çective of the Amazon Rainforest dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Potential Implications for LLMs'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='The results of this experiment are expected to have signiÔ¨Åcant implications for the development and Ô¨Åne-tuning of Large Language Models (LLMs). By identifying the most eÔ¨Äective conÔ¨Ågurations, we can oÔ¨Äer insights into the adapt- ability of these models to specialized datasets, which is crucial for applications that require a high degree of cultural and contextual sensitivity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Moreover, the experiment is poised to challenge the prevailing approach to LLM training and Ô¨Åne-tuning, which often relies on static, one-size-Ô¨Åts-all models. Our Ô¨Åndings could suggest a shift towards a more dynamic, com- ponent-based approach, allowing for greater Ô¨Çexibility and precision in model performance across diverse domains.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"The potential success of the RAG framework in this context may also pave the way for more granular im- provements in LLMs, beyond the standard metrics of accuracy and Ô¨Çuency. It may, for instance, enhance the models' ability to engage with and preserve less-represented languages and dialects, fostering greater inclusivity and diversity in the realm of natural language processing.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Implications for Indigenous Knowledge Preservation'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"The signiÔ¨Åcance of our experiment extends beyond the technical accomplishments within the Ô¨Åeld of natural language processing. It serves as a testament to the power of advanced computational techniques in preserving the rich tapestry of human culture, particularly the imperiled knowledge of the Amazon Rainforest's indigenous peoples. By success- fully training a language model to accurately reÔ¨Çect and communicate this knowledge, we not only preserve it for future generations but also validate the importance of linguistic and cultural diversity in our global ecosystem.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"This experiment, should it succeed, will demonstrate a practical application of LLMs in the service of cultural preservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns with the broader objectives of UNESCO's Intangible Cultural Heritage initiatives. It serves as a model for how com- munities around the world can leverage technology to protect and share their unique cultural identities and knowledge systems.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Advancements in RAG Framework'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='From a methodological standpoint, our experiment is poised to contribute to the advancement of the RAG framework within the realm of AI language models. By dissecting the RAG components and examining their interplay, we will gain insights into the mechanics of modular design in language models, oÔ¨Äering a blueprint for future research and development.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='The outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable framework, one that can be customized for specialized datasets and applications. This adaptability is critical as the demand for LLMs expands into increasingly varied and complex domains, from legal and medical to historical and anthropological.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"Furthermore, the experiment's focus on modularity could inspire a new wave of research into component- based architectures for LLMs. Such architectures may provide a more sustainable and eÔ¨Écient pathway to model im- provement, as opposed to the computationally intensive process of training large models from scratch.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='In conclusion, the proposed experiment holds the potential to make signiÔ¨Åcant contributions to both the Ô¨Åeld of AI and the preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative future for LLMs, where the voices of all communities can be heard and understood.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='Figure 1. Venn Diagram of Data Sources for RAG. This Ô¨Ågure represents a venn diagram of 3 sources of information (google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the ‚Äúout-'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='Figure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG algorithm from the input of the ‚Äúuser question‚Äù to the ‚Äúoutputted answer of the user question.‚Äù'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Experimental Results and Discussion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='The purpose of this section is to lay down the diÔ¨Äerent steps and customizations used within our experiment in order to demonstrate the conclusive results of this experiment to the reader; our experiment using a RAG methodology accurately shows how each component of a LLM positively or negatively aÔ¨Äects the accuracy of the outcome itself. In the initial world of LLM, in order to incrementally increase its performance engineers of these models would have to Ô¨Åne tune them then retrain which took immense amounts of power and large amounts of null results. However, now as they become more and more complex to tune models like OpenAI‚Äôs GPT and Google‚Äôs Bard have been plateauing performance wise.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Table 1. Experimental Results. This table represents the various diÔ¨Äerent combinations of LLM components with respect to the average similarity score they each produced.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='This paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to com- ponentize the models and break them down into smaller sections. This allows the user to add certain parts / combina- tions to test the performance of those then to substitute diÔ¨Äerent modules in to see which leads to the largest perfor- mance increase over the other. These customizable steps allow you to see minute diÔ¨Äerences in performance that slowly tuning a model couldn‚Äôt have shown you previously. This is a novel way to approach the tuning of LLM and will only serve to increase their accuracy as time moves forward.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Another major component of our RAG methodology is the ability to switch out which embedding layer you use. The standard embedding (OpenAI) or Palm‚Äôs embed. When choosing between both of those some tradeoÔ¨Äs are made.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='When using no context, Palm‚Äôs embedding layer seems to perform much better across the board, allowing for a much higher average similarity score, however this drastically shifts when given context as now OpenAI‚Äôs embedding layer performs much more soundly. The evidence for these claims is discussed later in this paper.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Additionally, another beneÔ¨Åcial feature of the RAG optimization and breakdown style is the ability to cus- tomize which similarity score the LLM uses to decide which answer to base its response oÔ¨Ä from the Q-A list. To go into further detail, the code when prompted with a user question compares the user question to the Q-A list and reorders the list based oÔ¨Ä highest similarity score to lowest, this allows the LLM to select the top 2-3 answers to the highest ranked questions and continue generating its own response from there.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='The Ô¨Årst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In terms of the data when GPT (for the purposes of precision all of the following results include context) and STS OpenAI were combined, you got an average similarity score of 0.997. If you instead pair this with Palm STS Score instead, the average score drops to 0.92, a 0.077 decrease in performance. A similar eÔ¨Äect when using Palm with Palm STS and Palm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='to reach very high accuracy levels when paired with a similarity score calculated from the same program (this means GPT worked better with OpenAI STS Score and that Palm worked better with Palm STS Score). What is also interest- ing to note is that although Palm produced a 0.001 lower performance than GPT it seemed to be more Ô¨Çexible, working better with its competitor (OpenAI STS Score + Palm produced 0.93) than the GPT with its competitor (Palm STS Score + GPT produced 0.92).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='To recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap or replace things like the embedding layer used, the similarity score used, and the base language model used, also whether it was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM and the data collected speaks a lot to the importance of each aspect of an LLM.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='In terms of expected results, two things were noticed, Ô¨Årst that Palm had a slightly lower performance than GPT (0.996 versus 0.997 respectively) on their top runs, however, there was also some contradictory data as it seemed that Palm worked signiÔ¨Åcantly better when given no context to work with compared to GPT, Palm produced an average score of 0.88 and 0.91 when given no context while GPT produced an average score of 0.75 and 0.897. Although GPT may perform much better when given context, Palm seems to beat it out just given its own proprietary dataset (no context).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Some unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, pair- ing Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually performed better when paired with OpenAI‚Äôs embed. 0.91 (with OpenAI embed) versus 0.88 (Palm embed)‚Äînote that this is without context given. A similar eÔ¨Äect was noticed going the other way around as well, without context, GPT performed much better with Palm embedding layer than with its own OpenAI embedding layer (0.897 versus 0.75 respectively). This data shows how Palm embedding layer tends to perform much better given no context when com- pared to OpenAI‚Äôs embed. Similarly, to explored above, it is the opposite when given context, however. OpenAI‚Äôs embedding layer performs a bit better when given context across the board than Palm embed.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Most notably from this experiment was two realizations. First, that Palm Embedding layer tends to work much better when just given its own proprietary dataset (and no context), when compared to OpenAI‚Äôs embed. Additionally, when given context, the playing Ô¨Åeld switches: Palm tends to perform much worse when given context when compared to OpenAI. Lastly, it is important to note that a combination of Palm/GPT with OpenAI‚Äôs embedding layer and context yielded extremely accurate results when its similarity scores were averaged.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Conclusion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Building upon the foundation laid by our initial Ô¨Åndings, it is paramount to recognize the exceptional performance of the Palm model when utilizing its proprietary dataset in conjunction with the Palm embed. This speciÔ¨Åcity in data and technology synchronization has shown that Palm outshines OpenAI in terms of model accuracy in a context-free envi- ronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT with its native OpenAI embedding layer excels, leveraging the additional context to produce responses of remarkable accuracy that resonate with the cultural and ecological nuances of the Amazon.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='This pivot in performance based on context underscores the signiÔ¨Åcance of tailored datasets and embedding mechanisms in the optimization of Large Language Models (LLMs). The adaptability of the Retrieval-Augmented Generation (RAG) framework emerges as a cornerstone for future enhancements in LLMs. By enabling the seamless interchange of model components, RAG presents an evolutionary leap in the Ô¨Åne-tuning of language models, catering to the intricate demands of culturally rich and contextually complex datasets.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content=\"In light of these advancements, our research signiÔ¨Åes a pivotal moment for LLMs. The evidence suggests that when models are Ô¨Ånely tuned with an awareness of the dataset's inherent context and the corresponding embedding layers, they reach new heights of linguistic precision. Therefore, the path forward for LLMs lies in embracing the modular and contextually aware RAG framework, which promises to reÔ¨Åne the capabilities of language models to an unprecedented degree, ensuring the preservation and celebration of the world's diverse linguistic and cultural heritage.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Limitations'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='The results of the ‚Äúoutputted answer‚Äù of this algorithm largely reÔ¨Çect the quality of the data. If the LLM is trained oÔ¨Ä low-quality data, then the answer will reÔ¨Çect this bias. The results of the experiment will Ô¨Çuctuate with diÔ¨Äerent results should a diÔ¨Äerent dataset be used to train the LLM.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='In the future there is a lot of potential to expand on this research by breaking down the RAG algorithm into even more separate components to further see the diÔ¨Äerences in average similarity score that adding or removing each component makes.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Bahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP), 4945-4949.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Devlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint,'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Lewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 9459-9474.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Radford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. Siriwardhana, S. W. (2023). Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics, 1-17. Vaswani, A. S. (2017). Attention is all you need. Advances in neural information processing systems. Yu, W. (2022). Retrieval-augmented generation across heterogeneous knowledge. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, 52-58.')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8c92ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='A Retrieval-Augmented Generation Based Large Language Model Benchmarked on a Novel Dataset'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Kieran Pichai'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Menlo School'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='ABSTRACT'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='The evolution of natural language processing has seen marked advancements, particularly with the advent of models like BERT, Transformers, and GPT variants, with recent additions like GPT and Bard. This paper investigates the Retrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its constituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our research demonstrates the signiÔ¨Åcance of preserving indigenous cultures and biodiversity. The experiment employs a customizable RAG methodology, allowing for the interchangeability of various components, such as the base language model and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm exhibits superior performance without context. The results also suggest that models tend to perform optimally when paired with similarity scores from their native platforms. Conclusively, our approach showcases the potential of a modular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to tra- ditional Ô¨Åne-tuning of large language models.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Introduction'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='The evolution of natural language processing models has seen signiÔ¨Åcant strides from rule-based approaches in the early stages of language understanding, eventually leading to the advent of neural networks. However, the full potential of these neural networks awaited the computational infrastructure to catch up. The pivotal moment arrived with the emergence of neural machine translation (NMT), exempliÔ¨Åed by Google Translate, which marked a turning point in machine language comprehension (Bahdanau, 2016). Subsequently, a plethora of advanced models, including BERT, Transformers, GPT-2, and GPT-3, have emerged, driving the Ô¨Åeld forward. Recent notable additions to this landscape are models like GPT and Bard (Devlin, 2018) (Vaswani, 2017) (Radford, 2018). While Ô¨Åne-tuning such models has proven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG) oÔ¨Äers a prom- ising alternative (Lewis, 2020) (Siriwardhana, 2023) (Yu, 2022).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Curiously, little attention has been devoted to dissecting the individual components of RAG and their respec- tive impacts on overall performance. In response to this gap, our paper undertakes a comprehensive investigation of the RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity and replaceability of its constituent modules. This research seeks to contribute to a deeper understanding of the mech- anisms underlying RAG and its potential for enhancing natural language understanding and generation. These Large Language Models (LLMs) exhibit a remarkable proÔ¨Åciency in replicating human language styles, achieving a level of linguistic verisimilitude that borders on the impeccable. In light of these capabilities, it is prudent to delve into the profound signiÔ¨Åcance of the Amazon rainforest, which equates to the importance of any ethnically or racially diverse nation across the globe. Within the vast expanse of the Amazon, an intricate tapestry of life unfolds, where millions of distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the indigenous populations who have made this ecosystem their home. The Amazon rainforest is not only a cradle of biological diversity but also a sanctuary for an array of religions and cultures, many of which teeter on the brink of'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice to the indigenous communities whose ancestral lands are enshrined within its boundaries. It is a call to safeguard the memories of the land, the traditions that have evolved within its embrace, and the very essence of their cultures. However, certain regions of the Amazon remain shrouded in obscurity, their Ô¨Çora and fauna so rare that reliable and readily available information is conspicuously lacking in the vast repository of knowledge available on the internet. In this context, advanced LLMs play an instrumental role in addressing this deÔ¨Åcit by facilitating the dissemination of indigenous narratives and thereby amplifying awareness and appreciation of the rich tapestry of beliefs, practices, and traditional knowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures with the global community, emphasizing the paramount importance of preserving the cultural diversity interwoven within this vast rainforest. In sum, the overarching mission of this endeavor is twofold: to document and educate the Western world about hitherto unknown cultures while concurrently ensuring the enduring preservation of these inval- uable facets of human heritage and biodiversity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Proposed Experiment'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Background and Importance'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally rich as the Amazon Rainforest, cannot be overstated. This knowledge, passed down through generations, encompasses not only cultural and religious beliefs but also practical insights into the local Ô¨Çora and fauna. As the modern world encroaches on these lands, this wisdom is in peril of being lost forever. Recognizing this, our proposed experiment aims to employ a state- of-the-art Retrieval-Augmented Generation (RAG) framework to capture and leverage this vast, yet vulnerable, knowledge base.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Our dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled in its depth and breadth. It includes detailed discussions on religious practices, cultural nuances, and the integral role of the surrounding ecosystem in the daily lives of these communities. This data is not just a scientiÔ¨Åc or anthropological resource; it is a repository of living history and an urgent call to action for preservation eÔ¨Äorts.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='By integrating this unique dataset into the RAG framework, we anticipate not only the preservation of knowledge but also the generation of responses that reÔ¨Çect the rich tapestry of Amazonian life. The experiment is designed to evaluate how diÔ¨Äerent components within the RAG setup‚Äîsuch as base language models and similarity scoring algorithms‚Äîcan be optimized to reÔ¨Çect the nuances captured within our dataset. In doing so, we aim to bridge the gap between advanced language models and the profound human insights found within the Amazon.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The central objective of our experiment is twofold: to analyze the performance implications of modular design within the RAG framework and to demonstrate the profound capability of such a system to preserve and communicate the wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper understanding of the data but also allow us to Ô¨Åne-tune the system for optimal performance across diÔ¨Äerent conÔ¨Ågu- rations. To achieve this, we will systematically explore the interchangeability of various RAG components. We will assess diÔ¨Äerent base language models such as GPT and Palm and compare the eÔ¨Écacy of similarity scoring tools from diverse platforms. The experiment will rigorously test these combinations, identifying which synergies most eÔ¨Äectively capture the essence of the dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The ultimate goal is to showcase the potential of a modular RAG system in processing culturally signiÔ¨Åcant information, paving the way for future applications that can beneÔ¨Åt from such tailored language models. We anticipate that our Ô¨Åndings will contribute signiÔ¨Åcantly to the Ô¨Åelds of computational linguistics and cultural preservation, demon- strating a novel approach to the application of large language models.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Source and Composition'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Our proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of verbal histories, inter- views, and ecological insights gathered from the indigenous peoples of the Amazon Rainforest, as well as from biolo- gists and ecologists dedicated to studying this unique biome. The dataset is characterized by its diversity, comprising narratives that elucidate the intricate relationship between the natives and their environment, including the religious and cultural signiÔ¨Åcance of plant and animal life.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The data collection was an extensive process, where linguists and researchers engaged in deep conversations with the natives, recording their dialects, translating their stories, and documenting their knowledge of the ecological system. Similarly, biologists contributed their decades of research on the Ô¨Çora and fauna, providing a scientiÔ¨Åc per- spective to the indigenous narratives. The data thus forms a conÔ¨Çuence of traditional wisdom and modern scientiÔ¨Åc understanding, oÔ¨Äering a 360-degree view of the Amazon Rainforest's ecosystem.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Cultural and Environmental SigniÔ¨Åcance'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a race against time, as globalization and environmental degradation threaten to erase unique cultures and the wisdom they hold. Our dataset serves as a digital ark, a means to preserve and perpetuate the knowledge that has sustained the Amazon's communities\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The environmental signiÔ¨Åcance of the Amazon Rainforest cannot be overstated‚Äîit is a keystone of global biodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling the ecological interdependencies that are vital for the rainforest's survival. This dataset, therefore, is not just an aca- demic or technological asset; it is a critical record for environmental conservationists and policymakers.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"Through our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under- standing of their habitat is unmatched. By integrating their knowledge into the RAG framework, we hope to create a model that not only responds with information but also with wisdom that respects the interconnectedness of life and culture.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Retrieval-Augmented Generation Framework'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='The heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophisticated algorithm that enables the deconstruction of the language model into discrete, interchangeable components. This framework integrates a retriever model that sources relevant context and a generator model that synthesizes the retrieved infor- mation into coherent responses.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='In mathematical terms, given an input query , the retriever model searches a knowledge base and retrieves a set of relevant documents . Each document is represented as a vector in a high-dimensional ùí¶ùí¶ ùëûùëû space, obtained from an embedding layer. This process transforms the raw text data into a structured form amenable ùê∑ùê∑ = {ùëëùëë1,ùëëùëë2,‚Ä¶,ùëëùëëùëòùëò} ùêØùêØùëëùëë ùëëùëë to computational manipulation.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='To examine the eÔ¨Äects of component interchangeability, we adopt various base language models and similar- ity scoring mechanisms. For instance, if denotes the embedding function, and and represent the input and target text sequences, respectively, their vector representations would be ùê∏ùê∏ larity as the basis for our similarity score, deÔ¨Åned by the formula: and ùë†ùë† ùë°ùë° . We employ cosine simi-'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='similarity(ùêØùêØùë†ùë†,ùêØùêØùë°ùë°) = denotes the dot product between the two vectors, and Here, denotes the Euclidean norm. This score quantiÔ¨Åes ÔøΩ|vùë†ùë†|ÔøΩ ÔøΩ|vùë°ùë°|ÔøΩ the closeness of the semantic meaning represented by the vectors, with a value of 1 indicating identical directionality ÔøΩ|‚ãÖ|ÔøΩ ‚ãÖ and thus, maximal similarity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The experiment tests diÔ¨Äerent conÔ¨Ågurations by substituting with embedding functions from various mod- els (e.g., GPT, Palm), allowing us to discern the impact of the embedding layer on the Ô¨Ånal similarity score. By com- ùê∏ùê∏ paring the performance of diÔ¨Äerent choices, we can identify which embeddings yield the most semantically rich representations for our unique dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The experiment commences with the training of the language models using our unique dataset. For the training phase, we deÔ¨Åne the following:'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=': The base language model, which can be either GPT or Palm.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=': The training dataset, consisting of pairs ‚Ñí answer. where is a query from the dataset and is the corresponding'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The language model is Ô¨Åne-tuned on , optimizing the weights to minimize the loss function, typically a cross-entropy loss between the predicted and actual answers.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Following training, the question-answering process involves feeding a new query to the trained model and retrieving the answer . This answer is then compared to a predeÔ¨Åned list of correct answers using the similarity score, which is ùëûùëû‚Ä≤ fundamental to evaluating the model's performance.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='Benchmarking and Evaluation'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The evaluation metric for our experiment is based on the similarity scores between the generated responses and a set of reference answers. Let be the set of answers generated by the model, and be the set of reference answers. We deÔ¨Åne the average similarity score as follows: ‚Ä≤ ‚Ä≤ ‚Ä≤ ‚àó ‚àó ‚àó'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=') This average score acts as the primary benchmark for comparing diÔ¨Äerent model conÔ¨Ågurations. We systematically n ùëñùëñ=1 record the scores across various combinations of language models and similarity scoring mechanisms to assess which conÔ¨Ågurations yield the highest average similarity, indicating the most eÔ¨Äective model setup for our dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Additionally, we account for the presence or absence of context in the model's training and response genera- tion. This is critical, as the presence of context has been shown to signiÔ¨Åcantly inÔ¨Çuence model performance, particu- larly in the domain of indigenous knowledge and biodiversity, where context provides essential background information that can drastically aÔ¨Äect the meaning and relevance of a response.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Through this meticulous experimental setup, we aim to illuminate the intricate dynamics between diÔ¨Äerent components of the RAG framework and their collective impact on the model's ability to accurately replicate and convey the richness of the Amazon Rainforest's cultural and ecological knowledge.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='Pre-Experiment Performance Expectations and Discussion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='In the landscape of varying conÔ¨Ågurations, we hypothesize that certain setups will yield higher average similarity scores than others, indicative of more nuanced and accurate language generation. Particularly, we expect that: The similarity scores for models trained with contextual data will surpass those trained without, due to the enriched understanding and background the model has of the subject matter.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='When aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em- bed), the semantic vector representations should align more closely, thus producing higher similarity scores. The modular nature of the RAG setup will reveal that certain combinations of base language models and similarity (ùêØùêØùë†ùë†,ùêØùêØùë°ùë°) scoring mechanisms are more eÔ¨Äective than others, depending on whether context is included or not.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='We denote the expected performance increase due to context as , and the alignment of native embeddings as . Mathematically, we can represent our hypothesis as:'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='These hypotheses will be tested through a series of experiments, allowing us to determine the optimal model conÔ¨Ågu- Scoreavg,native > Scoreavg,non‚àínative + ‚àÜnative ration for processing and generating responses reÔ¨Çective of the Amazon Rainforest dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Potential Implications for LLMs'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='The results of this experiment are expected to have signiÔ¨Åcant implications for the development and Ô¨Åne-tuning of Large Language Models (LLMs). By identifying the most eÔ¨Äective conÔ¨Ågurations, we can oÔ¨Äer insights into the adapt- ability of these models to specialized datasets, which is crucial for applications that require a high degree of cultural and contextual sensitivity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Moreover, the experiment is poised to challenge the prevailing approach to LLM training and Ô¨Åne-tuning, which often relies on static, one-size-Ô¨Åts-all models. Our Ô¨Åndings could suggest a shift towards a more dynamic, com- ponent-based approach, allowing for greater Ô¨Çexibility and precision in model performance across diverse domains.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"The potential success of the RAG framework in this context may also pave the way for more granular im- provements in LLMs, beyond the standard metrics of accuracy and Ô¨Çuency. It may, for instance, enhance the models' ability to engage with and preserve less-represented languages and dialects, fostering greater inclusivity and diversity in the realm of natural language processing.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Implications for Indigenous Knowledge Preservation'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"The signiÔ¨Åcance of our experiment extends beyond the technical accomplishments within the Ô¨Åeld of natural language processing. It serves as a testament to the power of advanced computational techniques in preserving the rich tapestry of human culture, particularly the imperiled knowledge of the Amazon Rainforest's indigenous peoples. By success- fully training a language model to accurately reÔ¨Çect and communicate this knowledge, we not only preserve it for future generations but also validate the importance of linguistic and cultural diversity in our global ecosystem.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"This experiment, should it succeed, will demonstrate a practical application of LLMs in the service of cultural preservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns with the broader objectives of UNESCO's Intangible Cultural Heritage initiatives. It serves as a model for how com- munities around the world can leverage technology to protect and share their unique cultural identities and knowledge systems.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Advancements in RAG Framework'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='From a methodological standpoint, our experiment is poised to contribute to the advancement of the RAG framework within the realm of AI language models. By dissecting the RAG components and examining their interplay, we will gain insights into the mechanics of modular design in language models, oÔ¨Äering a blueprint for future research and development.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='The outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable framework, one that can be customized for specialized datasets and applications. This adaptability is critical as the demand for LLMs expands into increasingly varied and complex domains, from legal and medical to historical and anthropological.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"Furthermore, the experiment's focus on modularity could inspire a new wave of research into component- based architectures for LLMs. Such architectures may provide a more sustainable and eÔ¨Écient pathway to model im- provement, as opposed to the computationally intensive process of training large models from scratch.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='In conclusion, the proposed experiment holds the potential to make signiÔ¨Åcant contributions to both the Ô¨Åeld of AI and the preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative future for LLMs, where the voices of all communities can be heard and understood.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='Figure 1. Venn Diagram of Data Sources for RAG. This Ô¨Ågure represents a venn diagram of 3 sources of information (google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the ‚Äúout-'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='Figure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG algorithm from the input of the ‚Äúuser question‚Äù to the ‚Äúoutputted answer of the user question.‚Äù'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Experimental Results and Discussion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='The purpose of this section is to lay down the diÔ¨Äerent steps and customizations used within our experiment in order to demonstrate the conclusive results of this experiment to the reader; our experiment using a RAG methodology accurately shows how each component of a LLM positively or negatively aÔ¨Äects the accuracy of the outcome itself. In the initial world of LLM, in order to incrementally increase its performance engineers of these models would have to Ô¨Åne tune them then retrain which took immense amounts of power and large amounts of null results. However, now as they become more and more complex to tune models like OpenAI‚Äôs GPT and Google‚Äôs Bard have been plateauing performance wise.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Table 1. Experimental Results. This table represents the various diÔ¨Äerent combinations of LLM components with respect to the average similarity score they each produced.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='This paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to com- ponentize the models and break them down into smaller sections. This allows the user to add certain parts / combina- tions to test the performance of those then to substitute diÔ¨Äerent modules in to see which leads to the largest perfor- mance increase over the other. These customizable steps allow you to see minute diÔ¨Äerences in performance that slowly tuning a model couldn‚Äôt have shown you previously. This is a novel way to approach the tuning of LLM and will only serve to increase their accuracy as time moves forward.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Another major component of our RAG methodology is the ability to switch out which embedding layer you use. The standard embedding (OpenAI) or Palm‚Äôs embed. When choosing between both of those some tradeoÔ¨Äs are made.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='When using no context, Palm‚Äôs embedding layer seems to perform much better across the board, allowing for a much higher average similarity score, however this drastically shifts when given context as now OpenAI‚Äôs embedding layer performs much more soundly. The evidence for these claims is discussed later in this paper.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Additionally, another beneÔ¨Åcial feature of the RAG optimization and breakdown style is the ability to cus- tomize which similarity score the LLM uses to decide which answer to base its response oÔ¨Ä from the Q-A list. To go into further detail, the code when prompted with a user question compares the user question to the Q-A list and reorders the list based oÔ¨Ä highest similarity score to lowest, this allows the LLM to select the top 2-3 answers to the highest ranked questions and continue generating its own response from there.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='The Ô¨Årst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In terms of the data when GPT (for the purposes of precision all of the following results include context) and STS OpenAI were combined, you got an average similarity score of 0.997. If you instead pair this with Palm STS Score instead, the average score drops to 0.92, a 0.077 decrease in performance. A similar eÔ¨Äect when using Palm with Palm STS and Palm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='to reach very high accuracy levels when paired with a similarity score calculated from the same program (this means GPT worked better with OpenAI STS Score and that Palm worked better with Palm STS Score). What is also interest- ing to note is that although Palm produced a 0.001 lower performance than GPT it seemed to be more Ô¨Çexible, working better with its competitor (OpenAI STS Score + Palm produced 0.93) than the GPT with its competitor (Palm STS Score + GPT produced 0.92).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='To recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap or replace things like the embedding layer used, the similarity score used, and the base language model used, also whether it was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM and the data collected speaks a lot to the importance of each aspect of an LLM.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='In terms of expected results, two things were noticed, Ô¨Årst that Palm had a slightly lower performance than GPT (0.996 versus 0.997 respectively) on their top runs, however, there was also some contradictory data as it seemed that Palm worked signiÔ¨Åcantly better when given no context to work with compared to GPT, Palm produced an average score of 0.88 and 0.91 when given no context while GPT produced an average score of 0.75 and 0.897. Although GPT may perform much better when given context, Palm seems to beat it out just given its own proprietary dataset (no context).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Some unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, pair- ing Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually performed better when paired with OpenAI‚Äôs embed. 0.91 (with OpenAI embed) versus 0.88 (Palm embed)‚Äînote that this is without context given. A similar eÔ¨Äect was noticed going the other way around as well, without context, GPT performed much better with Palm embedding layer than with its own OpenAI embedding layer (0.897 versus 0.75 respectively). This data shows how Palm embedding layer tends to perform much better given no context when com- pared to OpenAI‚Äôs embed. Similarly, to explored above, it is the opposite when given context, however. OpenAI‚Äôs embedding layer performs a bit better when given context across the board than Palm embed.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Most notably from this experiment was two realizations. First, that Palm Embedding layer tends to work much better when just given its own proprietary dataset (and no context), when compared to OpenAI‚Äôs embed. Additionally, when given context, the playing Ô¨Åeld switches: Palm tends to perform much worse when given context when compared to OpenAI. Lastly, it is important to note that a combination of Palm/GPT with OpenAI‚Äôs embedding layer and context yielded extremely accurate results when its similarity scores were averaged.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Conclusion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Building upon the foundation laid by our initial Ô¨Åndings, it is paramount to recognize the exceptional performance of the Palm model when utilizing its proprietary dataset in conjunction with the Palm embed. This speciÔ¨Åcity in data and technology synchronization has shown that Palm outshines OpenAI in terms of model accuracy in a context-free envi- ronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT with its native OpenAI embedding layer excels, leveraging the additional context to produce responses of remarkable accuracy that resonate with the cultural and ecological nuances of the Amazon.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='This pivot in performance based on context underscores the signiÔ¨Åcance of tailored datasets and embedding mechanisms in the optimization of Large Language Models (LLMs). The adaptability of the Retrieval-Augmented Generation (RAG) framework emerges as a cornerstone for future enhancements in LLMs. By enabling the seamless interchange of model components, RAG presents an evolutionary leap in the Ô¨Åne-tuning of language models, catering to the intricate demands of culturally rich and contextually complex datasets.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content=\"In light of these advancements, our research signiÔ¨Åes a pivotal moment for LLMs. The evidence suggests that when models are Ô¨Ånely tuned with an awareness of the dataset's inherent context and the corresponding embedding layers, they reach new heights of linguistic precision. Therefore, the path forward for LLMs lies in embracing the modular and contextually aware RAG framework, which promises to reÔ¨Åne the capabilities of language models to an unprecedented degree, ensuring the preservation and celebration of the world's diverse linguistic and cultural heritage.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Limitations'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='The results of the ‚Äúoutputted answer‚Äù of this algorithm largely reÔ¨Çect the quality of the data. If the LLM is trained oÔ¨Ä low-quality data, then the answer will reÔ¨Çect this bias. The results of the experiment will Ô¨Çuctuate with diÔ¨Äerent results should a diÔ¨Äerent dataset be used to train the LLM.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='In the future there is a lot of potential to expand on this research by breaking down the RAG algorithm into even more separate components to further see the diÔ¨Äerences in average similarity score that adding or removing each component makes.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Bahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP), 4945-4949.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Devlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint,'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Lewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 9459-9474.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Radford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. Siriwardhana, S. W. (2023). Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics, 1-17. Vaswani, A. S. (2017). Attention is all you need. Advances in neural information processing systems. Yu, W. (2022). Retrieval-augmented generation across heterogeneous knowledge. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, 52-58.'),\n",
       " Document(metadata={'source': './extracted_images/figure-3-3.jpg', 'type': 'image_or_table'}, page_content='Journal of Student Research, High School Edition'),\n",
       " Document(metadata={'source': './extracted_images/figure-6-8.jpg', 'type': 'image_or_table'}, page_content=\"Flowchart of a system processing user questions.  The system uses several API calls (SerpAPI for Google Search Results, Palm, and ChatGPT) and embedding methods (Palm Embed and OpenAI Embed) to find and process information relevant to the user's query.  It also scrapes a premade Q&A list.  All of this information is fed into a large language model to produce an outputted answer.\\n\"),\n",
       " Document(metadata={'source': './extracted_images/figure-6-7.jpg', 'type': 'image_or_table'}, page_content='Outputted answers combine: Google search results (SerpApi), existing Palm/OpenAI data, and primary data gathered from Amazon inhabitants.'),\n",
       " Document(metadata={'source': './extracted_images/table-7-1.jpg', 'type': 'image_or_table'}, page_content='This table shows similarity scores for 8 different comparisons.  The comparisons vary by whether context was used (yes/no), which LLM was used (GPT/Palm), and which embedding model was used (OpenAI/Palm). Scores range from 0.75 to 0.997. Notably, using Palm embeddings consistently yields higher scores than OpenAI embeddings.')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge two texts\n",
    "combined_content = text_docs + image_docs\n",
    "combined_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb7438",
   "metadata": {},
   "source": [
    "### Store embeddings in chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e3b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb2fcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/argha-ds/datascience/ai-assistant/RAG Use Cases/rag-use-cases-lc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e6bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store Chroma DB\n",
    "persist_directory = \"./chroma_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55e3041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embedded documents\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=combined_content,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3288e",
   "metadata": {},
   "source": [
    "### Create Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482520af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cbc5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135391a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7522/2086921267.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n",
      "/home/argha-ds/datascience/ai-assistant/RAG Use Cases/rag-use-cases-lc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='43218715-be66-41a3-92e4-57363d9fcdd2', metadata={'page_number': 5, 'source': './pdf-docs/rag_llm.pdf', 'type': 'text'}, page_content='Potential Implications for LLMs'),\n",
       " Document(id='c92bd696-571c-4f39-b9c4-0c635e7e2ba3', metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Potential Implications for LLMs'),\n",
       " Document(id='5bcd5e9a-a254-4793-a051-3096343ceb26', metadata={'source': '/tmp/gradio/05ca13cd17065c23018d59941e0a2747f0a67dc09b668272a6eacd28fae7a724/rag_llm.pdf', 'page_number': 5, 'type': 'text'}, page_content='Potential Implications for LLMs'),\n",
       " Document(id='83f71180-a792-4abf-8a7d-e02304687f7c', metadata={'page_number': 5, 'source': '/tmp/gradio/05ca13cd17065c23018d59941e0a2747f0a67dc09b668272a6eacd28fae7a724/rag_llm.pdf', 'type': 'text'}, page_content='Potential Implications for LLMs')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test retriever\n",
    "query = \"What are the Potential Implications for LLMs?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce03d97d",
   "metadata": {},
   "source": [
    "### Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c852b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7ea55ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-pro', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7fbd54b07680>, default_metadata=())"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f507b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "template = \"\"\"You are a helpful assistant for question answering task.\n",
    "your task is to answer the query of the user from the provided context.\n",
    "Your answer should be concise and to the point.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2033a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difine RAG chain\n",
    "chain = ({\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "         | prompt\n",
    "         | llm\n",
    "         | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bdf65c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/argha-ds/datascience/ai-assistant/RAG Use Cases/rag-use-cases-lc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What are the Potential Implications for LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef2c79ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG could evolve into a more nuanced and adaptable framework customizable for specialized datasets and applications across diverse domains (legal, medical, historical, anthropological).'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f7e22d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/argha-ds/datascience/ai-assistant/RAG Use Cases/rag-use-cases-lc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2167-1907'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the ISSN of this document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a381f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-use-cases-lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
