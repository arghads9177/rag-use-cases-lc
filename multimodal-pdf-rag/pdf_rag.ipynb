{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e64821",
   "metadata": {},
   "source": [
    "# Multimodal PDF RAG\n",
    "\n",
    "## Problem Statement\n",
    "RAG Application that can answer any query related to an uploaded PDF(containing Text, Images and Tables).\n",
    "\n",
    "## Solution\n",
    "\n",
    "1. Upload a PDF file containing Text, Images and Tables. \n",
    "2. Load and separate text, images and tables data from the PDF.\n",
    "3. Summarize the text and images using LLM.\n",
    "4. Embed and Store the text, summarized images & tables in a inmemory vectorDB.\n",
    "5. Create a Retriever for this indexed vectorDB.\n",
    "6. Create a text chain to answer related to text using LLM.\n",
    "7. Create a full chain with a multimodal LLM and text chain that can answer related to both text and images both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6431f3c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce7b8712",
   "metadata": {},
   "source": [
    "### Load Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54e84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cebe46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d15090",
   "metadata": {},
   "source": [
    "### Load PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a204a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/argha-ds/datascience/ai-assistant/RAG Use Cases/rag-use-cases-lc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a465129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PDF path\n",
    "pdf_path = \"./pdf-docs/rag_llm.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6835be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "# Use unstructured to read the PDF\n",
    "elements = partition_pdf(\n",
    "    filename=pdf_path,\n",
    "    strategy=\"hi_res\",                             # High-resolution layout parser (best for complex PDFs)\n",
    "    extract_images_in_pdf=True,                    # Extract images from the PDF\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],  # Extract image blocks that are images or tables\n",
    "    extract_image_block_to_payload=False,          # Don't embed image bytes directly into `el.metadata[\"image\"]`\n",
    "    extract_image_block_output_dir=\"./extracted_images\"  # Store extracted images here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287999ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: A Retrieval-Augmented Generation Based Large Language Model Benchmarked on a Novel Dataset...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Kieran Pichai...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Menlo School...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: ABSTRACT...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The evolution of natural language processing has seen marked advancements, particularly with the adv...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Introduction...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The evolution of natural language processing models has seen signiﬁcant strides from rule-based appr...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Curiously, little attention has been devoted to dissecting the individual components of RAG and thei...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 1...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice t...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Proposed Experiment...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Background and Importance...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally ri...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Our dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled ...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: By integrating this unique dataset into the RAG framework, we anticipate not only the preservation o...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The central objective of our experiment is twofold: to analyze the performance implications of modul...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The ultimate goal is to showcase the potential of a modular RAG system in processing culturally sign...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: 2...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Source and Composition...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Our proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of ver...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The data collection was an extensive process, where linguists and researchers engaged in deep conver...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Cultural and Environmental Signiﬁcance...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a ...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: for millennia....\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The environmental signiﬁcance of the Amazon Rainforest cannot be overstated—it is a keystone of glob...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Through our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Retrieval-Augmented Generation Framework...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophistica...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In mathematical terms, given an input query , the retriever model searches a knowledge base and retr...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: To examine the eﬀects of component interchangeability, we adopt various base language models and sim...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝐯𝐯𝑠𝑠 = 𝐸𝐸(𝑠𝑠)...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝐯𝐯𝑡𝑡 = 𝐸𝐸(𝑡𝑡)...\n",
      "================================================================================\n",
      "Type: Formula\n",
      "Preview: eye Vs * Ve similarity(v,, v,) = ——~—— IIvsll [Ivel]...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝐯𝐯𝑠𝑠 ⋅ 𝐯𝐯𝑡𝑡...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: similarity(𝐯𝐯𝑠𝑠,𝐯𝐯𝑡𝑡) = denotes the dot product between the two vectors, and Here, denotes the Eucli...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The experiment tests diﬀerent conﬁgurations by substituting with embedding functions from various mo...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝐸𝐸...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: Experiment Setup...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The experiment commences with the training of the language models using our unique dataset. For the ...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: : The base language model, which can be either GPT or Palm....\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: : The training dataset, consisting of pairs ℒ answer. where is a query from the dataset and is the c...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: (𝑞𝑞𝑖𝑖,𝑎𝑎𝑖𝑖)...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝑎𝑎𝑖𝑖...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝑞𝑞𝑖𝑖...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝒟𝒟...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The language model is ﬁne-tuned on , optimizing the weights to minimize the loss function, typically...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ℒ...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝒟𝒟...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Following training, the question-answering process involves feeding a new query to the trained model...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝑎𝑎′...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Benchmarking and Evaluation...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The evaluation metric for our experiment is based on the similarity scores between the generated res...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝐴𝐴 = {𝑎𝑎1...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ,𝑎𝑎2...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ,…,𝑎𝑎𝑛𝑛...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ,𝑎𝑎2...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝐴𝐴ref = {𝑎𝑎1...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: }...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: n...\n",
      "================================================================================\n",
      "Type: Formula\n",
      "Preview: 𝑛𝑛 ∗...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 1...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ′...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: Scoreavg =...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ,𝐯𝐯𝑎𝑎𝑖𝑖...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: �similarity(𝐯𝐯𝑎𝑎𝑖𝑖...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ) This average score acts as the primary benchmark for comparing diﬀerent model conﬁgurations. We sy...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ,…,𝑎𝑎𝑛𝑛...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Additionally, we account for the presence or absence of context in the model's training and response...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Through this meticulous experimental setup, we aim to illuminate the intricate dynamics between diﬀe...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Pre-Experiment Performance Expectations and Discussion...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In the landscape of varying conﬁgurations, we hypothesize that certain setups will yield higher aver...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 𝐯𝐯context...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: When aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em- b...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: }...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 4...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: We denote the expected performance increase due to context as , and the alignment of native embeddin...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ∆context...\n",
      "================================================================================\n",
      "Type: Formula\n",
      "Preview: Scoreavg context > Scoreavgno context + Acontext Scoreaygnative > Scoreavg non-native + Anative...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: ∆native...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: Scoreavg,context > Scoreavg,no context + ∆context...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: These hypotheses will be tested through a series of experiments, allowing us to determine the optima...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Potential Implications for LLMs...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The results of this experiment are expected to have signiﬁcant implications for the development and ...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Moreover, the experiment is poised to challenge the prevailing approach to LLM training and ﬁne-tuni...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The potential success of the RAG framework in this context may also pave the way for more granular i...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Implications for Indigenous Knowledge Preservation...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The signiﬁcance of our experiment extends beyond the technical accomplishments within the ﬁeld of na...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: This experiment, should it succeed, will demonstrate a practical application of LLMs in the service ...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Advancements in RAG Framework...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: From a methodological standpoint, our experiment is poised to contribute to the advancement of the R...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Furthermore, the experiment's focus on modularity could inspire a new wave of research into componen...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 5...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION e Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In conclusion, the proposed experiment holds the potential to make signiﬁcant contributions to both ...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: Outputted Answer Primary data gathered from inhabitants of the amazon...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Figure 1. Venn Diagram of Data Sources for RAG. This ﬁgure represents a venn diagram of 3 sources of...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: putted answer.”...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: User Question ChatGPT Different API Calls SerpAPI (Google Search Results) Scrape through a premade Q...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Figure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures ...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 6...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Experimental Results and Discussion...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The purpose of this section is to lay down the diﬀerent steps and customizations used within our exp...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Table 1. Experimental Results. This table represents the various diﬀerent combinations of LLM compon...\n",
      "================================================================================\n",
      "Type: Table\n",
      "Preview: Context LLM Embed for Similarity Score Yes No GPT Palm OpenAI Embedding Palm Embedding 1 x x x 2 x x...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: This paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to co...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Another major component of our RAG methodology is the ability to switch out which embedding layer yo...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: When using no context, Palm’s embedding layer seems to perform much better across the board, allowin...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Additionally, another beneﬁcial feature of the RAG optimization and breakdown style is the ability t...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The ﬁrst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In ter...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 7...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: to reach very high accuracy levels when paired with a similarity score calculated from the same prog...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: To recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In terms of expected results, two things were noticed, ﬁrst that Palm had a slightly lower performan...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Some unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, ...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Most notably from this experiment was two realizations. First, that Palm Embedding layer tends to wo...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Conclusion...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Building upon the foundation laid by our initial ﬁndings, it is paramount to recognize the exception...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: This pivot in performance based on context underscores the signiﬁcance of tailored datasets and embe...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In light of these advancements, our research signiﬁes a pivotal moment for LLMs. The evidence sugges...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: ISSN: 2167-1907...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 8...\n",
      "================================================================================\n",
      "Type: Image\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research...\n",
      "================================================================================\n",
      "Type: Header\n",
      "Preview: Volume 12 Issue 4 (2023)...\n",
      "================================================================================\n",
      "Type: Title\n",
      "Preview: Limitations...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: The results of the “outputted answer” of this algorithm largely reﬂect the quality of the data. If t...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: In the future there is a lot of potential to expand on this research by breaking down the RAG algori...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: References...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Bahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE in...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Devlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understandi...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: arXiv:1810.04805....\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Lewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in N...\n",
      "================================================================================\n",
      "Type: NarrativeText\n",
      "Preview: Radford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. Siriward...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: www.JSR.org/hs...\n",
      "================================================================================\n",
      "Type: UncategorizedText\n",
      "Preview: 9...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print the content types and preview\n",
    "for el in elements:\n",
    "    print(f\"Type: {el.category}\")\n",
    "    print(f\"Preview: {el.text[:100]}...\")\n",
    "    print(\"=\"* 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5bf6547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'image_path': './extracted_images/figure-1-1.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'image_path': './extracted_images/figure-2-2.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'image_path': './extracted_images/figure-3-3.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'image_path': './extracted_images/figure-4-4.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'image_path': './extracted_images/figure-5-5.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'image_path': './extracted_images/figure-6-6.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION e Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(570.8333333333333), np.float64(389.16812472222216)), (np.float64(570.8333333333333), np.float64(947.3625458333333)), (np.float64(1129.0277544444443), np.float64(947.3625458333333)), (np.float64(1129.0277544444443), np.float64(389.16812472222216))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'image_path': './extracted_images/figure-6-7.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: Outputted Answer Primary data gathered from inhabitants of the amazon\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(350.0), np.float64(1136.6699216666666)), (np.float64(350.0), np.float64(1873.9728108333331)), (np.float64(1349.9998805555556), np.float64(1873.9728108333331)), (np.float64(1349.9998805555556), np.float64(1136.6699216666666))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'image_path': './extracted_images/figure-6-8.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: User Question ChatGPT Different API Calls SerpAPI (Google Search Results) Scrape through a premade QA list to find which most closely fits the user question Palm Embed Outputted Answer of User Question\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'image_path': './extracted_images/figure-7-9.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'image_path': './extracted_images/figure-8-10.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'image_path': './extracted_images/figure-9-11.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print the content of images\n",
    "for el in elements:\n",
    "    if el.category == 'Image':\n",
    "        print(f\"Type: {el.category}\")\n",
    "        print(f\"Metadata: {el.metadata.to_dict()}\")\n",
    "        print(f\"Preview: {el.text}\")\n",
    "        print(\"=\"* 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ef9cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Table\n",
      "Preview: Context LLM Embed for Similarity Score Yes No GPT Palm OpenAI Embedding Palm Embedding 1 x x x 2 x x x 3 x x x 4 x x x 5 x x x 6 x x x 7 x x x 8 x x x Score 0.75 0.92 0.93 0.88 0.997 0.996 0.91 0.897\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print the content of tables\n",
    "for el in elements:\n",
    "    if el.category == 'Table':\n",
    "        print(f\"Type: {el.category}\")\n",
    "        print(f\"Preview: {el.text}\")\n",
    "        print(\"=\"* 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8885febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group each element type\n",
    "text_elements = [el for el in elements if el.category in [\"NarrativeText\", \"Title\", \"List\"] and el.text]\n",
    "table_elements = [el for el in elements if el.category == \"Table\"]\n",
    "image_elements = [el for el in elements if el.category == \"Image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe38fa",
   "metadata": {},
   "source": [
    "#### Remove Duplicate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f47f7d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicate image: ./extracted_images/figure-6-6.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-4-4.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-2-2.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-9-11.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-8-10.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-1-1.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-5-5.jpg\n",
      "Removed duplicate image: ./extracted_images/figure-7-9.jpg\n"
     ]
    }
   ],
   "source": [
    "# Hash and Filter Unique Image Files\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "def hash_image(file_path):\n",
    "    \"\"\"Generate a hash for an image file\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "# Directory where images are saved\n",
    "image_dir = \"./extracted_images\"\n",
    "\n",
    "# Track hashes and remove duplicates\n",
    "hash_set = set()\n",
    "unique_files = []\n",
    "\n",
    "for file_name in os.listdir(image_dir):\n",
    "    file_path = os.path.join(image_dir, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        img_hash = hash_image(file_path)\n",
    "        if img_hash not in hash_set:\n",
    "            hash_set.add(img_hash)\n",
    "            unique_files.append(file_path)\n",
    "        else:\n",
    "            os.remove(file_path)  # Delete duplicate file\n",
    "            print(f\"Removed duplicate image: {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10cedea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Duplicates in image_elements\n",
    "# Filter image elements to only include unique image files\n",
    "unique_image_elements = []\n",
    "\n",
    "valid_image_paths = set(os.path.basename(path) for path in unique_files)\n",
    "\n",
    "for el in elements:\n",
    "    if el.category == \"Image\":\n",
    "        image_metadata = el.metadata.to_dict().get(\"image_path\", \"\")\n",
    "        if image_metadata and os.path.basename(image_metadata) in valid_image_paths:\n",
    "            unique_image_elements.append(el)\n",
    "    else:\n",
    "        continue  # keep non-image elements untouched\n",
    "\n",
    "# Rebuild elements with deduplicated images\n",
    "image_elements = [\n",
    "    el for el in elements if el.category != \"Image\"\n",
    "] + unique_image_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5813f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(200.0), np.float64(80.00087805555565)), (np.float64(200.0), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(164.9987113888888)), (np.float64(624.9891666666667), np.float64(80.00087805555565))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'image_path': './extracted_images/figure-3-3.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: HIGH SCHOOL EDITION @® Journal of Student Research\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(570.8333333333333), np.float64(389.16812472222216)), (np.float64(570.8333333333333), np.float64(947.3625458333333)), (np.float64(1129.0277544444443), np.float64(947.3625458333333)), (np.float64(1129.0277544444443), np.float64(389.16812472222216))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'image_path': './extracted_images/figure-6-7.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: Outputted Answer Primary data gathered from inhabitants of the amazon\n",
      "================================================================================\n",
      "Type: Image\n",
      "Metadata: {'coordinates': {'points': ((np.float64(350.0), np.float64(1136.6699216666666)), (np.float64(350.0), np.float64(1873.9728108333331)), (np.float64(1349.9998805555556), np.float64(1873.9728108333331)), (np.float64(1349.9998805555556), np.float64(1136.6699216666666))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'image_path': './extracted_images/figure-6-8.jpg', 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Preview: User Question ChatGPT Different API Calls SerpAPI (Google Search Results) Scrape through a premade QA list to find which most closely fits the user question Palm Embed Outputted Answer of User Question\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print the content of images\n",
    "for el in image_elements:\n",
    "    if el.category == 'Image':\n",
    "        print(f\"Type: {el.category}\")\n",
    "        print(f\"Metadata: {el.metadata.to_dict()}\")\n",
    "        print(f\"Preview: {el.text}\")\n",
    "        print(\"=\"* 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb121eb6",
   "metadata": {},
   "source": [
    "### Summarize the tables and Images with Vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0eebafaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001 ['embedText', 'countTextTokens']\n",
      "models/gemini-1.5-pro-latest ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-002 ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-latest ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-002 ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-8b ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-001 ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-latest ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-2.5-pro-preview-03-25 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-05-20 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-06-17 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-05-06 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-06-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp-image-generation ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-preview-image-generation ['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite-preview-02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp-02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-exp-1206 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-1219 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-tts ['countTokens', 'generateContent']\n",
      "models/gemini-2.5-pro-preview-tts ['countTokens', 'generateContent']\n",
      "models/learnlm-2.0-flash-experimental ['generateContent', 'countTokens']\n",
      "models/gemma-3-1b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e2b-it ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/embedding-001 ['embedContent']\n",
      "models/text-embedding-004 ['embedContent']\n",
      "models/gemini-embedding-exp-03-07 ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-exp ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-001 ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/aqa ['generateAnswer']\n",
      "models/imagen-3.0-generate-002 ['predict']\n",
      "models/imagen-4.0-generate-preview-06-06 ['predict']\n",
      "models/imagen-4.0-ultra-generate-preview-06-06 ['predict']\n",
      "models/veo-2.0-generate-001 ['predictLongRunning']\n",
      "models/veo-3.0-generate-preview ['predictLongRunning']\n",
      "models/veo-3.0-fast-generate-preview ['predictLongRunning']\n",
      "models/gemini-2.5-flash-preview-native-audio-dialog ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.5-flash-exp-native-audio-thinking-dialog ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-live-001 ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-live-2.5-flash-preview ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-live-preview ['bidiGenerateContent', 'countTokens']\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    print(model.name, model.supported_generation_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "316e6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Image\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9a9da25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-pro-latest',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Vision Model\n",
    "# vision_model = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
    "vision_model =  genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n",
    "\n",
    "vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c56baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prompt\n",
    "prompt_text = \"\"\"You are an assitant tasked with summarizing tables and images for retrieval.\n",
    "        These summaries will be embeded and used to retrieve the raw image or raw table elements.\n",
    "        Give a concise summary of the image or table that will be optimized for retrieval.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize Image or Table from Image\n",
    "\n",
    "\n",
    "def summarize_image(image_path):\n",
    "    \"\"\"Summarize an image (including tables) using Gemini Vision.\"\"\"\n",
    "    try:\n",
    "\n",
    "        img = Image.open(image_path)\n",
    "        response = vision_model.generate_content(\n",
    "            [prompt_text, img]\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd8cec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing: ./extracted_images/figure-3-3.jpg\n",
      ">>> Summary: Journal of Student Research, High School Edition...\n",
      "\n",
      "Summarizing: ./extracted_images/figure-6-8.jpg\n",
      ">>> Summary: Flowchart of a system processing user questions.  The system uses several API calls (SerpAPI for Google Search Results, Palm, and ChatGPT) and embedding methods (Palm Embed and OpenAI Embed) to find a...\n",
      "\n",
      "Summarizing: ./extracted_images/figure-6-7.jpg\n",
      ">>> Summary: Outputted answers combine: Google search results (SerpApi), existing Palm/OpenAI data, and primary data gathered from Amazon inhabitants....\n",
      "\n",
      "Summarizing: ./extracted_images/table-7-1.jpg\n",
      ">>> Summary: This table shows similarity scores for 8 different comparisons.  The comparisons vary by whether context was used (yes/no), which LLM was used (GPT/Palm), and which embedding model was used (OpenAI/Pa...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply to All Unique Image Files\n",
    "\n",
    "# Store results\n",
    "image_summaries = {}\n",
    "\n",
    "for img_path in unique_files:\n",
    "    print(f\"Summarizing: {img_path}\")\n",
    "    summary = summarize_image(img_path)\n",
    "    if summary:\n",
    "        image_summaries[img_path] = summary\n",
    "        print(f\">>> Summary: {summary[:200]}...\\n\")\n",
    "    else:\n",
    "        print(f\">>> Failed to summarize {img_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac03238",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "812e7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e826e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the image summaries into docs\n",
    "image_docs = [\n",
    "    Document(page_content=summary, metadata={\"source\": path, \"type\": \"image_or_table\"})\n",
    "    for path, summary in image_summaries.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a339dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './extracted_images/figure-3-3.jpg', 'type': 'image_or_table'}, page_content='Journal of Student Research, High School Edition'),\n",
       " Document(metadata={'source': './extracted_images/figure-6-8.jpg', 'type': 'image_or_table'}, page_content=\"Flowchart of a system processing user questions.  The system uses several API calls (SerpAPI for Google Search Results, Palm, and ChatGPT) and embedding methods (Palm Embed and OpenAI Embed) to find and process information relevant to the user's query.  It also scrapes a premade Q&A list.  All of this information is fed into a large language model to produce an outputted answer.\\n\"),\n",
       " Document(metadata={'source': './extracted_images/figure-6-7.jpg', 'type': 'image_or_table'}, page_content='Outputted answers combine: Google search results (SerpApi), existing Palm/OpenAI data, and primary data gathered from Amazon inhabitants.'),\n",
       " Document(metadata={'source': './extracted_images/table-7-1.jpg', 'type': 'image_or_table'}, page_content='This table shows similarity scores for 8 different comparisons.  The comparisons vary by whether context was used (yes/no), which LLM was used (GPT/Palm), and which embedding model was used (OpenAI/Palm). Scores range from 0.75 to 0.997. Notably, using Palm embeddings consistently yields higher scores than OpenAI embeddings.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14b23405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.5880249738693237, 'coordinates': {'points': ((np.float64(200.0), np.float64(207.7227020263672)), (np.float64(200.0), np.float64(339.69999999999976)), (np.float64(1411.8000000000002), np.float64(339.69999999999976)), (np.float64(1411.8000000000002), np.float64(207.7227020263672))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '8727570aa228bd375e82f4dd71a6eb10'}\n",
      "A Retrieval-Augmented Generation Based Large Language Model Benchmarked on a Novel Dataset\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.763323962688446, 'coordinates': {'points': ((np.float64(200.0), np.float64(387.167236328125)), (np.float64(200.0), np.float64(423.79999999999995)), (np.float64(404.93333333333334), np.float64(423.79999999999995)), (np.float64(404.93333333333334), np.float64(387.167236328125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '57383d799582a8e4de79ca8099c779cf'}\n",
      "Kieran Pichai\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.8045752644538879, 'coordinates': {'points': ((np.float64(199.34686279296875), np.float64(471.9896545410156)), (np.float64(199.34686279296875), np.float64(501.65550000000013)), (np.float64(376.0736666666666), np.float64(501.65550000000013)), (np.float64(376.0736666666666), np.float64(471.9896545410156))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '57383d799582a8e4de79ca8099c779cf'}\n",
      "Menlo School\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.7151427865028381, 'coordinates': {'points': ((np.float64(200.0), np.float64(548.317138671875)), (np.float64(200.0), np.float64(585.8000000000001)), (np.float64(382.9333333333334), np.float64(585.8000000000001)), (np.float64(382.9333333333334), np.float64(548.317138671875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '8727570aa228bd375e82f4dd71a6eb10'}\n",
      "ABSTRACT\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9544898271560669, 'coordinates': {'points': ((np.float64(200.0), np.float64(632.935791015625)), (np.float64(200.0), np.float64(1039.9338333333333)), (np.float64(1506.9865333333335), np.float64(1039.9338333333333)), (np.float64(1506.9865333333335), np.float64(632.935791015625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '7777393319ddadddb0e2c044c0332443'}\n",
      "The evolution of natural language processing has seen marked advancements, particularly with the advent of models like BERT, Transformers, and GPT variants, with recent additions like GPT and Bard. This paper investigates the Retrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its constituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our research demonstrates the signiﬁcance of preserving indigenous cultures and biodiversity. The experiment employs a customizable RAG methodology, allowing for the interchangeability of various components, such as the base language model and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm exhibits superior performance without context. The results also suggest that models tend to perform optimally when paired with similarity scores from their native platforms. Conclusively, our approach showcases the potential of a modular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to tra- ditional ﬁne-tuning of large language models.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8799284100532532, 'coordinates': {'points': ((np.float64(200.0), np.float64(1092.8036666666667)), (np.float64(200.0), np.float64(1131.6370000000002)), (np.float64(411.05916666666667), np.float64(1131.6370000000002)), (np.float64(411.05916666666667), np.float64(1092.8036666666667))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '8727570aa228bd375e82f4dd71a6eb10'}\n",
      "Introduction\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9549375176429749, 'coordinates': {'points': ((np.float64(200.0), np.float64(1178.203857421875)), (np.float64(200.0), np.float64(1509.1005000000002)), (np.float64(1506.9948833333333), np.float64(1509.1005000000002)), (np.float64(1506.9948833333333), np.float64(1178.203857421875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87d99a6d77158f90f024b012d545c164'}\n",
      "The evolution of natural language processing models has seen signiﬁcant strides from rule-based approaches in the early stages of language understanding, eventually leading to the advent of neural networks. However, the full potential of these neural networks awaited the computational infrastructure to catch up. The pivotal moment arrived with the emergence of neural machine translation (NMT), exempliﬁed by Google Translate, which marked a turning point in machine language comprehension (Bahdanau, 2016). Subsequently, a plethora of advanced models, including BERT, Transformers, GPT-2, and GPT-3, have emerged, driving the ﬁeld forward. Recent notable additions to this landscape are models like GPT and Bard (Devlin, 2018) (Vaswani, 2017) (Radford, 2018). While ﬁne-tuning such models has proven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG) oﬀers a prom- ising alternative (Lewis, 2020) (Siriwardhana, 2023) (Yu, 2022).\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9535836577415466, 'coordinates': {'points': ((np.float64(199.32568359375), np.float64(1519.1004999999998)), (np.float64(199.32568359375), np.float64(1963.1005)), (np.float64(1506.9197333333332), np.float64(1963.1005)), (np.float64(1506.9197333333332), np.float64(1519.1004999999998))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87d99a6d77158f90f024b012d545c164'}\n",
      "Curiously, little attention has been devoted to dissecting the individual components of RAG and their respec- tive impacts on overall performance. In response to this gap, our paper undertakes a comprehensive investigation of the RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity and replaceability of its constituent modules. This research seeks to contribute to a deeper understanding of the mech- anisms underlying RAG and its potential for enhancing natural language understanding and generation. These Large Language Models (LLMs) exhibit a remarkable proﬁciency in replicating human language styles, achieving a level of linguistic verisimilitude that borders on the impeccable. In light of these capabilities, it is prudent to delve into the profound signiﬁcance of the Amazon rainforest, which equates to the importance of any ethnically or racially diverse nation across the globe. Within the vast expanse of the Amazon, an intricate tapestry of life unfolds, where millions of distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the indigenous populations who have made this ecosystem their home. The Amazon rainforest is not only a cradle of biological diversity but also a sanctuary for an array of religions and cultures, many of which teeter on the brink of\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7187889814376831, 'coordinates': {'points': ((np.float64(198.84555053710938), np.float64(2041.5390625)), (np.float64(198.84555053710938), np.float64(2064.449462890625)), (np.float64(394.1135559082031), np.float64(2064.449462890625)), (np.float64(394.1135559082031), np.float64(2041.5390625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87d99a6d77158f90f024b012d545c164'}\n",
      "ISSN: 2167-1907\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9530729651451111, 'coordinates': {'points': ((np.float64(199.2046356201172), np.float64(210.1004999999999)), (np.float64(199.2046356201172), np.float64(654.1005)), (np.float64(1506.9058166666666), np.float64(654.1005)), (np.float64(1506.9058166666666), np.float64(210.1004999999999))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'fba5bd666aa06416d7fb23c773927a1d'}\n",
      "oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice to the indigenous communities whose ancestral lands are enshrined within its boundaries. It is a call to safeguard the memories of the land, the traditions that have evolved within its embrace, and the very essence of their cultures. However, certain regions of the Amazon remain shrouded in obscurity, their ﬂora and fauna so rare that reliable and readily available information is conspicuously lacking in the vast repository of knowledge available on the internet. In this context, advanced LLMs play an instrumental role in addressing this deﬁcit by facilitating the dissemination of indigenous narratives and thereby amplifying awareness and appreciation of the rich tapestry of beliefs, practices, and traditional knowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures with the global community, emphasizing the paramount importance of preserving the cultural diversity interwoven within this vast rainforest. In sum, the overarching mission of this endeavor is twofold: to document and educate the Western world about hitherto unknown cultures while concurrently ensuring the enduring preservation of these inval- uable facets of human heritage and biodiversity.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8712294101715088, 'coordinates': {'points': ((np.float64(200.0), np.float64(706.9703333333333)), (np.float64(200.0), np.float64(745.8036666666667)), (np.float64(560.7228333333333), np.float64(745.8036666666667)), (np.float64(560.7228333333333), np.float64(706.9703333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'fba5bd666aa06416d7fb23c773927a1d'}\n",
      "Proposed Experiment\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.7549414038658142, 'coordinates': {'points': ((np.float64(200.0), np.float64(794.6333333333332)), (np.float64(200.0), np.float64(827.9666666666667)), (np.float64(587.5000000000001), np.float64(827.9666666666667)), (np.float64(587.5000000000001), np.float64(794.6333333333332))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'fba5bd666aa06416d7fb23c773927a1d'}\n",
      "Background and Importance\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9518823027610779, 'coordinates': {'points': ((np.float64(200.0), np.float64(871.5422973632812)), (np.float64(200.0), np.float64(1089.9338333333333)), (np.float64(1506.980966666667), np.float64(1089.9338333333333)), (np.float64(1506.980966666667), np.float64(871.5422973632812))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '164f05502c751520de7c2ad97e3d16f4'}\n",
      "The intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally rich as the Amazon Rainforest, cannot be overstated. This knowledge, passed down through generations, encompasses not only cultural and religious beliefs but also practical insights into the local ﬂora and fauna. As the modern world encroaches on these lands, this wisdom is in peril of being lost forever. Recognizing this, our proposed experiment aims to employ a state- of-the-art Retrieval-Augmented Generation (RAG) framework to capture and leverage this vast, yet vulnerable, knowledge base.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9425076842308044, 'coordinates': {'points': ((np.float64(200.0), np.float64(1099.9338333333333)), (np.float64(200.0), np.float64(1241.2671666666665)), (np.float64(1506.7749999999992), np.float64(1241.2671666666665)), (np.float64(1506.7749999999992), np.float64(1099.9338333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '164f05502c751520de7c2ad97e3d16f4'}\n",
      "Our dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled in its depth and breadth. It includes detailed discussions on religious practices, cultural nuances, and the integral role of the surrounding ecosystem in the daily lives of these communities. This data is not just a scientiﬁc or anthropological resource; it is a repository of living history and an urgent call to action for preservation eﬀorts.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9473974108695984, 'coordinates': {'points': ((np.float64(200.0), np.float64(1251.2671666666668)), (np.float64(200.0), np.float64(1430.4338333333333)), (np.float64(1506.9113833333333), np.float64(1430.4338333333333)), (np.float64(1506.9113833333333), np.float64(1251.2671666666668))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '164f05502c751520de7c2ad97e3d16f4'}\n",
      "By integrating this unique dataset into the RAG framework, we anticipate not only the preservation of knowledge but also the generation of responses that reﬂect the rich tapestry of Amazonian life. The experiment is designed to evaluate how diﬀerent components within the RAG setup—such as base language models and similarity scoring algorithms—can be optimized to reﬂect the nuances captured within our dataset. In doing so, we aim to bridge the gap between advanced language models and the profound human insights found within the Amazon.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9563410878181458, 'coordinates': {'points': ((np.float64(199.37210083007812), np.float64(1439.8302001953125)), (np.float64(199.37210083007812), np.float64(1733.1004999999998)), (np.float64(1507.0060166666665), np.float64(1733.1004999999998)), (np.float64(1507.0060166666665), np.float64(1439.8302001953125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '164f05502c751520de7c2ad97e3d16f4'}\n",
      "The central objective of our experiment is twofold: to analyze the performance implications of modular design within the RAG framework and to demonstrate the profound capability of such a system to preserve and communicate the wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper understanding of the data but also allow us to ﬁne-tune the system for optimal performance across diﬀerent conﬁgu- rations. To achieve this, we will systematically explore the interchangeability of various RAG components. We will assess diﬀerent base language models such as GPT and Palm and compare the eﬃcacy of similarity scoring tools from diverse platforms. The experiment will rigorously test these combinations, identifying which synergies most eﬀectively capture the essence of the dataset.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9368348717689514, 'coordinates': {'points': ((np.float64(200.0), np.float64(1743.1004999999998)), (np.float64(200.0), np.float64(1884.433833333333)), (np.float64(1506.8946833333334), np.float64(1884.433833333333)), (np.float64(1506.8946833333334), np.float64(1743.1004999999998))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '164f05502c751520de7c2ad97e3d16f4'}\n",
      "The ultimate goal is to showcase the potential of a modular RAG system in processing culturally signiﬁcant information, paving the way for future applications that can beneﬁt from such tailored language models. We anticipate that our ﬁndings will contribute signiﬁcantly to the ﬁelds of computational linguistics and cultural preservation, demon- strating a novel approach to the application of large language models.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8222460150718689, 'coordinates': {'points': ((np.float64(200.0), np.float64(211.96666666666664)), (np.float64(200.0), np.float64(245.29999999999998)), (np.float64(536.5000000000001), np.float64(245.29999999999998)), (np.float64(536.5000000000001), np.float64(211.96666666666664))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'd53b1a52f43c2d233621cd4813a3e3b8'}\n",
      "Source and Composition\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.945088267326355, 'coordinates': {'points': ((np.float64(200.0), np.float64(290.08062744140625)), (np.float64(200.0), np.float64(469.43383333333327)), (np.float64(1506.7638666666655), np.float64(469.43383333333327)), (np.float64(1506.7638666666655), np.float64(290.08062744140625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '6f62cd8ffcc721c0fbad4e4ef7866cce'}\n",
      "Our proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of verbal histories, inter- views, and ecological insights gathered from the indigenous peoples of the Amazon Rainforest, as well as from biolo- gists and ecologists dedicated to studying this unique biome. The dataset is characterized by its diversity, comprising narratives that elucidate the intricate relationship between the natives and their environment, including the religious and cultural signiﬁcance of plant and animal life.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9435852766036987, 'coordinates': {'points': ((np.float64(200.0), np.float64(479.4338333333333)), (np.float64(200.0), np.float64(658.6005)), (np.float64(1506.911783333333), np.float64(658.6005)), (np.float64(1506.911783333333), np.float64(479.4338333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '6f62cd8ffcc721c0fbad4e4ef7866cce'}\n",
      "The data collection was an extensive process, where linguists and researchers engaged in deep conversations with the natives, recording their dialects, translating their stories, and documenting their knowledge of the ecological system. Similarly, biologists contributed their decades of research on the ﬂora and fauna, providing a scientiﬁc per- spective to the indigenous narratives. The data thus forms a conﬂuence of traditional wisdom and modern scientiﬁc understanding, oﬀering a 360-degree view of the Amazon Rainforest's ecosystem.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8764854073524475, 'coordinates': {'points': ((np.float64(200.0), np.float64(708.2999999999998)), (np.float64(200.0), np.float64(741.6333333333331)), (np.float64(750.3333333333335), np.float64(741.6333333333331)), (np.float64(750.3333333333335), np.float64(708.2999999999998))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'd53b1a52f43c2d233621cd4813a3e3b8'}\n",
      "Cultural and Environmental Signiﬁcance\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9359689354896545, 'coordinates': {'points': ((np.float64(200.0), np.float64(789.6005)), (np.float64(200.0), np.float64(923.6652221679688)), (np.float64(1507.0088), np.float64(923.6652221679688)), (np.float64(1507.0088), np.float64(789.6005))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '723212a0d98d13268e9c610c2d321db8'}\n",
      "The urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a race against time, as globalization and environmental degradation threaten to erase unique cultures and the wisdom they hold. Our dataset serves as a digital ark, a means to preserve and perpetuate the knowledge that has sustained the Amazon's communities\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9390075206756592, 'coordinates': {'points': ((np.float64(196.1522979736328), np.float64(940.9338333333334)), (np.float64(196.1522979736328), np.float64(1082.2671666666665)), (np.float64(1506.8195333333326), np.float64(1082.2671666666665)), (np.float64(1506.8195333333326), np.float64(940.9338333333334))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '723212a0d98d13268e9c610c2d321db8'}\n",
      "The environmental signiﬁcance of the Amazon Rainforest cannot be overstated—it is a keystone of global biodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling the ecological interdependencies that are vital for the rainforest's survival. This dataset, therefore, is not just an aca- demic or technological asset; it is a critical record for environmental conservationists and policymakers.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.937074601650238, 'coordinates': {'points': ((np.float64(200.0), np.float64(1092.2671666666668)), (np.float64(200.0), np.float64(1233.6005)), (np.float64(1506.8612833333332), np.float64(1233.6005)), (np.float64(1506.8612833333332), np.float64(1092.2671666666668))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '723212a0d98d13268e9c610c2d321db8'}\n",
      "Through our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under- standing of their habitat is unmatched. By integrating their knowledge into the RAG framework, we hope to create a model that not only responds with information but also with wisdom that respects the interconnectedness of life and culture.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8728882670402527, 'coordinates': {'points': ((np.float64(199.95449829101562), np.float64(1282.9666666666667)), (np.float64(199.95449829101562), np.float64(1316.6333333333332)), (np.float64(807.3333333333336), np.float64(1316.6333333333332)), (np.float64(807.3333333333336), np.float64(1282.9666666666667))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'd53b1a52f43c2d233621cd4813a3e3b8'}\n",
      "Retrieval-Augmented Generation Framework\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9367923140525818, 'coordinates': {'points': ((np.float64(198.63092041015625), np.float64(1364.7671666666668)), (np.float64(198.63092041015625), np.float64(1506.1005)), (np.float64(1506.9419999999996), np.float64(1506.1005)), (np.float64(1506.9419999999996), np.float64(1364.7671666666668))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '62b9e6d1f361a969f5f7e3a1d0bf537b'}\n",
      "The heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophisticated algorithm that enables the deconstruction of the language model into discrete, interchangeable components. This framework integrates a retriever model that sources relevant context and a generator model that synthesizes the retrieved infor- mation into coherent responses.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9385144114494324, 'coordinates': {'points': ((np.float64(200.0), np.float64(1516.1005)), (np.float64(200.0), np.float64(1657.4338333333335)), (np.float64(1506.9638833333331), np.float64(1657.4338333333335)), (np.float64(1506.9638833333331), np.float64(1516.1005))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '62b9e6d1f361a969f5f7e3a1d0bf537b'}\n",
      "In mathematical terms, given an input query , the retriever model searches a knowledge base and retrieves a set of relevant documents . Each document is represented as a vector in a high-dimensional 𝒦𝒦 𝑞𝑞 space, obtained from an embedding layer. This process transforms the raw text data into a structured form amenable 𝐷𝐷 = {𝑑𝑑1,𝑑𝑑2,…,𝑑𝑑𝑘𝑘} 𝐯𝐯𝑑𝑑 𝑑𝑑 to computational manipulation.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9405785202980042, 'coordinates': {'points': ((np.float64(199.1611328125), np.float64(1667.113037109375)), (np.float64(199.1611328125), np.float64(1808.7671666666665)), (np.float64(1506.8959499999999), np.float64(1808.7671666666665)), (np.float64(1506.8959499999999), np.float64(1667.113037109375))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '62b9e6d1f361a969f5f7e3a1d0bf537b'}\n",
      "To examine the eﬀects of component interchangeability, we adopt various base language models and similar- ity scoring mechanisms. For instance, if denotes the embedding function, and and represent the input and target text sequences, respectively, their vector representations would be 𝐸𝐸 larity as the basis for our similarity score, deﬁned by the formula: and 𝑠𝑠 𝑡𝑡 . We employ cosine simi-\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9368948340415955, 'coordinates': {'points': ((np.float64(200.0), np.float64(1886.381333333333)), (np.float64(200.0), np.float64(1996.1005)), (np.float64(1506.9308666666668), np.float64(1996.1005)), (np.float64(1506.9308666666668), np.float64(1886.381333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "similarity(𝐯𝐯𝑠𝑠,𝐯𝐯𝑡𝑡) = denotes the dot product between the two vectors, and Here, denotes the Euclidean norm. This score quantiﬁes �|v𝑠𝑠|� �|v𝑡𝑡|� the closeness of the semantic meaning represented by the vectors, with a value of 1 indicating identical directionality �|⋅|� ⋅ and thus, maximal similarity.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7907972931861877, 'coordinates': {'points': ((np.float64(199.06227111816406), np.float64(2041.833740234375)), (np.float64(199.06227111816406), np.float64(2064.45751953125)), (np.float64(393.99896240234375), np.float64(2064.45751953125)), (np.float64(393.99896240234375), np.float64(2041.833740234375))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 3, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "ISSN: 2167-1907\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9403257966041565, 'coordinates': {'points': ((np.float64(199.77186584472656), np.float64(210.1004999999999)), (np.float64(199.77186584472656), np.float64(351.4338333333332)), (np.float64(1506.7661666666654), np.float64(351.4338333333332)), (np.float64(1506.7661666666654), np.float64(210.1004999999999))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      "The experiment tests diﬀerent conﬁgurations by substituting with embedding functions from various mod- els (e.g., GPT, Palm), allowing us to discern the impact of the embedding layer on the ﬁnal similarity score. By com- 𝐸𝐸 paring the performance of diﬀerent choices, we can identify which embeddings yield the most semantically rich representations for our unique dataset.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9085817933082581, 'coordinates': {'points': ((np.float64(197.5093536376953), np.float64(482.43383333333344)), (np.float64(197.5093536376953), np.float64(548.1005)), (np.float64(1506.8863333333325), np.float64(548.1005)), (np.float64(1506.8863333333325), np.float64(482.43383333333344))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      "The experiment commences with the training of the language models using our unique dataset. For the training phase, we deﬁne the following:\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7520370483398438, 'coordinates': {'points': ((np.float64(205.61119079589844), np.float64(558.1005000000001)), (np.float64(205.61119079589844), np.float64(585.9338333333334)), (np.float64(910.4736666666665), np.float64(585.9338333333334)), (np.float64(910.4736666666665), np.float64(558.1005000000001))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      ": The base language model, which can be either GPT or Palm.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7297148704528809, 'coordinates': {'points': ((np.float64(200.0), np.float64(595.9338333333335)), (np.float64(200.0), np.float64(661.6005000000001)), (np.float64(1506.9224333333332), np.float64(661.6005000000001)), (np.float64(1506.9224333333332), np.float64(595.9338333333335))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      ": The training dataset, consisting of pairs ℒ answer. where is a query from the dataset and is the corresponding\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.857230544090271, 'coordinates': {'points': ((np.float64(200.0), np.float64(671.6005000000001)), (np.float64(200.0), np.float64(737.2671666666668)), (np.float64(1506.7946499999996), np.float64(737.2671666666668)), (np.float64(1506.7946499999996), np.float64(671.6005000000001))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      "The language model is ﬁne-tuned on , optimizing the weights to minimize the loss function, typically a cross-entropy loss between the predicted and actual answers.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9097127318382263, 'coordinates': {'points': ((np.float64(196.37156677246094), np.float64(746.5118408203125)), (np.float64(196.37156677246094), np.float64(850.7671666666666)), (np.float64(1506.9382166666664), np.float64(850.7671666666666)), (np.float64(1506.9382166666664), np.float64(746.5118408203125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      "Following training, the question-answering process involves feeding a new query to the trained model and retrieving the answer . This answer is then compared to a predeﬁned list of correct answers using the similarity score, which is 𝑞𝑞′ fundamental to evaluating the model's performance.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8631711006164551, 'coordinates': {'points': ((np.float64(200.0), np.float64(900.4666666666667)), (np.float64(200.0), np.float64(933.8)), (np.float64(610.1666666666667), np.float64(933.8)), (np.float64(610.1666666666667), np.float64(900.4666666666667))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '5fad331112928b678e6d25d59eb49f1f'}\n",
      "Benchmarking and Evaluation\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9365326166152954, 'coordinates': {'points': ((np.float64(200.0), np.float64(978.1532592773438)), (np.float64(200.0), np.float64(1082.2671666666665)), (np.float64(1506.9197333333334), np.float64(1082.2671666666665)), (np.float64(1506.9197333333334), np.float64(978.1532592773438))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '92b2b0c686c9253bc2fe5fcddd2254ff'}\n",
      "The evaluation metric for our experiment is based on the similarity scores between the generated responses and a set of reference answers. Let be the set of answers generated by the model, and be the set of reference answers. We deﬁne the average similarity score as follows: ′ ′ ′ ∗ ∗ ∗\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9335139393806458, 'coordinates': {'points': ((np.float64(194.30596923828125), np.float64(1178.0813333333333)), (np.float64(194.30596923828125), np.float64(1286.6005)), (np.float64(1506.9252999999999), np.float64(1286.6005)), (np.float64(1506.9252999999999), np.float64(1178.0813333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      ") This average score acts as the primary benchmark for comparing diﬀerent model conﬁgurations. We systematically n 𝑖𝑖=1 record the scores across various combinations of language models and similarity scoring mechanisms to assess which conﬁgurations yield the highest average similarity, indicating the most eﬀective model setup for our dataset.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9331151843070984, 'coordinates': {'points': ((np.float64(198.49351501464844), np.float64(1296.436279296875)), (np.float64(198.49351501464844), np.float64(1437.933833333333)), (np.float64(1506.9531333333332), np.float64(1437.933833333333)), (np.float64(1506.9531333333332), np.float64(1296.436279296875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Additionally, we account for the presence or absence of context in the model's training and response genera- tion. This is critical, as the presence of context has been shown to signiﬁcantly inﬂuence model performance, particu- larly in the domain of indigenous knowledge and biodiversity, where context provides essential background information that can drastically aﬀect the meaning and relevance of a response.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9340912103652954, 'coordinates': {'points': ((np.float64(200.0), np.float64(1447.9338333333335)), (np.float64(200.0), np.float64(1551.4338333333333)), (np.float64(1507.0310666666662), np.float64(1551.4338333333333)), (np.float64(1507.0310666666662), np.float64(1447.9338333333335))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Through this meticulous experimental setup, we aim to illuminate the intricate dynamics between diﬀerent components of the RAG framework and their collective impact on the model's ability to accurately replicate and convey the richness of the Amazon Rainforest's cultural and ecological knowledge.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.859574556350708, 'coordinates': {'points': ((np.float64(200.0), np.float64(1601.1333333333332)), (np.float64(200.0), np.float64(1634.4666666666667)), (np.float64(986.625), np.float64(1634.4666666666667)), (np.float64(986.625), np.float64(1601.1333333333332))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Pre-Experiment Performance Expectations and Discussion\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9320123195648193, 'coordinates': {'points': ((np.float64(200.0), np.float64(1681.2344970703125)), (np.float64(200.0), np.float64(1823.7671666666668)), (np.float64(1506.87365), np.float64(1823.7671666666668)), (np.float64(1506.87365), np.float64(1681.2344970703125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '27d1690292d036e8523fca1658e455d7'}\n",
      "In the landscape of varying conﬁgurations, we hypothesize that certain setups will yield higher average similarity scores than others, indicative of more nuanced and accurate language generation. Particularly, we expect that: The similarity scores for models trained with contextual data will surpass those trained without, due to the enriched understanding and background the model has of the subject matter.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9324306845664978, 'coordinates': {'points': ((np.float64(200.0), np.float64(1833.7671666666665)), (np.float64(200.0), np.float64(1976.4338333333333)), (np.float64(1506.6747999999989), np.float64(1976.4338333333333)), (np.float64(1506.6747999999989), np.float64(1833.7671666666665))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '27d1690292d036e8523fca1658e455d7'}\n",
      "When aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em- bed), the semantic vector representations should align more closely, thus producing higher similarity scores. The modular nature of the RAG setup will reveal that certain combinations of base language models and similarity (𝐯𝐯𝑠𝑠,𝐯𝐯𝑡𝑡) scoring mechanisms are more eﬀective than others, depending on whether context is included or not.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.6920047998428345, 'coordinates': {'points': ((np.float64(198.96026611328125), np.float64(2041.7052001953125)), (np.float64(198.96026611328125), np.float64(2064.4912109375)), (np.float64(393.9510803222656), np.float64(2064.4912109375)), (np.float64(393.9510803222656), np.float64(2041.7052001953125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 4, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '27d1690292d036e8523fca1658e455d7'}\n",
      "ISSN: 2167-1907\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9217211008071899, 'coordinates': {'points': ((np.float64(199.63858032226562), np.float64(210.1004999999999)), (np.float64(199.63858032226562), np.float64(275.7671666666665)), (np.float64(1506.8817666666666), np.float64(275.7671666666665)), (np.float64(1506.8817666666666), np.float64(210.1004999999999))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '693e507e0a18eba8508f6769e97b0318'}\n",
      "We denote the expected performance increase due to context as , and the alignment of native embeddings as . Mathematically, we can represent our hypothesis as:\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9192385077476501, 'coordinates': {'points': ((np.float64(200.0), np.float64(358.4338333333335)), (np.float64(200.0), np.float64(424.1005000000001)), (np.float64(1499.7415166666665), np.float64(424.1005000000001)), (np.float64(1499.7415166666665), np.float64(358.4338333333335))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "These hypotheses will be tested through a series of experiments, allowing us to determine the optimal model conﬁgu- Scoreavg,native > Scoreavg,non−native + ∆native ration for processing and generating responses reﬂective of the Amazon Rainforest dataset.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8624512553215027, 'coordinates': {'points': ((np.float64(200.0), np.float64(473.79999999999995)), (np.float64(200.0), np.float64(507.13333333333327)), (np.float64(636.4583333333333), np.float64(507.13333333333327)), (np.float64(636.4583333333333), np.float64(473.79999999999995))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Potential Implications for LLMs\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9328924417495728, 'coordinates': {'points': ((np.float64(200.0), np.float64(554.8651123046875)), (np.float64(200.0), np.float64(696.4338333333334)), (np.float64(1506.9419999999998), np.float64(696.4338333333334)), (np.float64(1506.9419999999998), np.float64(554.8651123046875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '7c0b6861e44723d5687f177358333262'}\n",
      "The results of this experiment are expected to have signiﬁcant implications for the development and ﬁne-tuning of Large Language Models (LLMs). By identifying the most eﬀective conﬁgurations, we can oﬀer insights into the adapt- ability of these models to specialized datasets, which is crucial for applications that require a high degree of cultural and contextual sensitivity.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9336577653884888, 'coordinates': {'points': ((np.float64(200.0), np.float64(706.4338333333334)), (np.float64(200.0), np.float64(809.9338333333334)), (np.float64(1506.9396166666666), np.float64(809.9338333333334)), (np.float64(1506.9396166666666), np.float64(706.4338333333334))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '7c0b6861e44723d5687f177358333262'}\n",
      "Moreover, the experiment is poised to challenge the prevailing approach to LLM training and ﬁne-tuning, which often relies on static, one-size-ﬁts-all models. Our ﬁndings could suggest a shift towards a more dynamic, com- ponent-based approach, allowing for greater ﬂexibility and precision in model performance across diverse domains.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9370346069335938, 'coordinates': {'points': ((np.float64(200.0), np.float64(819.9338333333334)), (np.float64(200.0), np.float64(961.2671666666666)), (np.float64(1506.8334499999987), np.float64(961.2671666666666)), (np.float64(1506.8334499999987), np.float64(819.9338333333334))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '7c0b6861e44723d5687f177358333262'}\n",
      "The potential success of the RAG framework in this context may also pave the way for more granular im- provements in LLMs, beyond the standard metrics of accuracy and ﬂuency. It may, for instance, enhance the models' ability to engage with and preserve less-represented languages and dialects, fostering greater inclusivity and diversity in the realm of natural language processing.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8610591888427734, 'coordinates': {'points': ((np.float64(200.0), np.float64(1010.9666666666666)), (np.float64(200.0), np.float64(1044.3)), (np.float64(905.4583333333333), np.float64(1044.3)), (np.float64(905.4583333333333), np.float64(1010.9666666666666))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Implications for Indigenous Knowledge Preservation\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9484277963638306, 'coordinates': {'points': ((np.float64(198.09974670410156), np.float64(1091.2169189453125)), (np.float64(198.09974670410156), np.float64(1271.4338333333333)), (np.float64(1506.9921), np.float64(1271.4338333333333)), (np.float64(1506.9921), np.float64(1091.2169189453125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87631654c36409c301800ae12174efa3'}\n",
      "The signiﬁcance of our experiment extends beyond the technical accomplishments within the ﬁeld of natural language processing. It serves as a testament to the power of advanced computational techniques in preserving the rich tapestry of human culture, particularly the imperiled knowledge of the Amazon Rainforest's indigenous peoples. By success- fully training a language model to accurately reﬂect and communicate this knowledge, we not only preserve it for future generations but also validate the importance of linguistic and cultural diversity in our global ecosystem.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9468027949333191, 'coordinates': {'points': ((np.float64(200.0), np.float64(1281.4338333333333)), (np.float64(200.0), np.float64(1460.893310546875)), (np.float64(1506.911383333333), np.float64(1460.893310546875)), (np.float64(1506.911383333333), np.float64(1281.4338333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87631654c36409c301800ae12174efa3'}\n",
      "This experiment, should it succeed, will demonstrate a practical application of LLMs in the service of cultural preservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns with the broader objectives of UNESCO's Intangible Cultural Heritage initiatives. It serves as a model for how com- munities around the world can leverage technology to protect and share their unique cultural identities and knowledge systems.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8661386370658875, 'coordinates': {'points': ((np.float64(200.0), np.float64(1510.3)), (np.float64(200.0), np.float64(1543.6333333333332)), (np.float64(673.125), np.float64(1543.6333333333332)), (np.float64(673.125), np.float64(1510.3))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Advancements in RAG Framework\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9382249116897583, 'coordinates': {'points': ((np.float64(200.0), np.float64(1591.669921875)), (np.float64(200.0), np.float64(1733.1004999999998)), (np.float64(1506.9614833333328), np.float64(1733.1004999999998)), (np.float64(1506.9614833333328), np.float64(1591.669921875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '76e2ebfd0a7eb2a1bb6e88696ed6bfe6'}\n",
      "From a methodological standpoint, our experiment is poised to contribute to the advancement of the RAG framework within the realm of AI language models. By dissecting the RAG components and examining their interplay, we will gain insights into the mechanics of modular design in language models, oﬀering a blueprint for future research and development.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9353484511375427, 'coordinates': {'points': ((np.float64(200.0), np.float64(1743.1004999999998)), (np.float64(200.0), np.float64(1884.433833333333)), (np.float64(1506.8422000000005), np.float64(1884.433833333333)), (np.float64(1506.8422000000005), np.float64(1743.1004999999998))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '76e2ebfd0a7eb2a1bb6e88696ed6bfe6'}\n",
      "The outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable framework, one that can be customized for specialized datasets and applications. This adaptability is critical as the demand for LLMs expands into increasingly varied and complex domains, from legal and medical to historical and anthropological.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9395981431007385, 'coordinates': {'points': ((np.float64(198.3177490234375), np.float64(1894.4136962890625)), (np.float64(198.3177490234375), np.float64(1997.9338333333333)), (np.float64(1501.3900146484375), np.float64(1997.9338333333333)), (np.float64(1501.3900146484375), np.float64(1894.4136962890625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '76e2ebfd0a7eb2a1bb6e88696ed6bfe6'}\n",
      "Furthermore, the experiment's focus on modularity could inspire a new wave of research into component- based architectures for LLMs. Such architectures may provide a more sustainable and eﬃcient pathway to model im- provement, as opposed to the computationally intensive process of training large models from scratch.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7013646364212036, 'coordinates': {'points': ((np.float64(199.24911499023438), np.float64(2041.9556884765625)), (np.float64(199.24911499023438), np.float64(2064.46435546875)), (np.float64(394.8359375), np.float64(2064.46435546875)), (np.float64(394.8359375), np.float64(2041.9556884765625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '76e2ebfd0a7eb2a1bb6e88696ed6bfe6'}\n",
      "ISSN: 2167-1907\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9353991746902466, 'coordinates': {'points': ((np.float64(200.0), np.float64(210.1004999999999)), (np.float64(200.0), np.float64(313.6004999999999)), (np.float64(1507.0199333333328), np.float64(313.6004999999999)), (np.float64(1507.0199333333328), np.float64(210.1004999999999))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'a6cb6cb932678cc1906b79982556992b'}\n",
      "In conclusion, the proposed experiment holds the potential to make signiﬁcant contributions to both the ﬁeld of AI and the preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative future for LLMs, where the voices of all communities can be heard and understood.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9215583205223083, 'coordinates': {'points': ((np.float64(199.0110321044922), np.float64(995.4058333333329)), (np.float64(199.0110321044922), np.float64(1088.6031494140625)), (np.float64(1507.1117833333333), np.float64(1088.6031494140625)), (np.float64(1507.1117833333333), np.float64(995.4058333333329))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Figure 1. Venn Diagram of Data Sources for RAG. This ﬁgure represents a venn diagram of 3 sources of information (google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the “out-\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7436774969100952, 'coordinates': {'points': ((np.float64(199.97216666666668), np.float64(1883.079345703125)), (np.float64(199.97216666666668), np.float64(1949.8743333333332)), (np.float64(1506.7638666666653), np.float64(1949.8743333333332)), (np.float64(1506.7638666666653), np.float64(1883.079345703125))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf'}\n",
      "Figure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG algorithm from the input of the “user question” to the “outputted answer of the user question.”\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8654903173446655, 'coordinates': {'points': ((np.float64(199.78990173339844), np.float64(214.16616666666673)), (np.float64(199.78990173339844), np.float64(252.9995000000001)), (np.float64(788.558), np.float64(252.9995000000001)), (np.float64(788.558), np.float64(214.16616666666673))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'c1a2323b922a0aff37859dc4c1997680'}\n",
      "Experimental Results and Discussion\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9537456035614014, 'coordinates': {'points': ((np.float64(194.01597595214844), np.float64(300.9338333333334)), (np.float64(194.01597595214844), np.float64(555.7671666666668)), (np.float64(1508.20068359375), np.float64(555.7671666666668)), (np.float64(1508.20068359375), np.float64(300.9338333333334))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "The purpose of this section is to lay down the diﬀerent steps and customizations used within our experiment in order to demonstrate the conclusive results of this experiment to the reader; our experiment using a RAG methodology accurately shows how each component of a LLM positively or negatively aﬀects the accuracy of the outcome itself. In the initial world of LLM, in order to incrementally increase its performance engineers of these models would have to ﬁne tune them then retrain which took immense amounts of power and large amounts of null results. However, now as they become more and more complex to tune models like OpenAI’s GPT and Google’s Bard have been plateauing performance wise.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9249820113182068, 'coordinates': {'points': ((np.float64(190.36485290527344), np.float64(603.5848333333332)), (np.float64(190.36485290527344), np.float64(669.2514999999999)), (np.float64(1507.1143798828125), np.float64(669.2514999999999)), (np.float64(1507.1143798828125), np.float64(603.5848333333332))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "Table 1. Experimental Results. This table represents the various diﬀerent combinations of LLM components with respect to the average similarity score they each produced.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9514713883399963, 'coordinates': {'points': ((np.float64(196.515869140625), np.float64(1140.9338333333335)), (np.float64(196.515869140625), np.float64(1357.9338333333333)), (np.float64(1507.070033333333), np.float64(1357.9338333333333)), (np.float64(1507.070033333333), np.float64(1140.9338333333335))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "This paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to com- ponentize the models and break them down into smaller sections. This allows the user to add certain parts / combina- tions to test the performance of those then to substitute diﬀerent modules in to see which leads to the largest perfor- mance increase over the other. These customizable steps allow you to see minute diﬀerences in performance that slowly tuning a model couldn’t have shown you previously. This is a novel way to approach the tuning of LLM and will only serve to increase their accuracy as time moves forward.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9328656792640686, 'coordinates': {'points': ((np.float64(196.4347381591797), np.float64(1367.9338333333333)), (np.float64(196.4347381591797), np.float64(1471.4338333333333)), (np.float64(1506.8167499999993), np.float64(1471.4338333333333)), (np.float64(1506.8167499999993), np.float64(1367.9338333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "Another major component of our RAG methodology is the ability to switch out which embedding layer you use. The standard embedding (OpenAI) or Palm’s embed. When choosing between both of those some tradeoﬀs are made.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.935149610042572, 'coordinates': {'points': ((np.float64(200.0), np.float64(1481.4338333333333)), (np.float64(200.0), np.float64(1584.9338333333333)), (np.float64(1506.9396166666668), np.float64(1584.9338333333333)), (np.float64(1506.9396166666668), np.float64(1481.4338333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "When using no context, Palm’s embedding layer seems to perform much better across the board, allowing for a much higher average similarity score, however this drastically shifts when given context as now OpenAI’s embedding layer performs much more soundly. The evidence for these claims is discussed later in this paper.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9488053917884827, 'coordinates': {'points': ((np.float64(200.0), np.float64(1594.933833333333)), (np.float64(200.0), np.float64(1774.1005)), (np.float64(1506.9893166666664), np.float64(1774.1005)), (np.float64(1506.9893166666664), np.float64(1594.933833333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "Additionally, another beneﬁcial feature of the RAG optimization and breakdown style is the ability to cus- tomize which similarity score the LLM uses to decide which answer to base its response oﬀ from the Q-A list. To go into further detail, the code when prompted with a user question compares the user question to the Q-A list and reorders the list based oﬀ highest similarity score to lowest, this allows the LLM to select the top 2-3 answers to the highest ranked questions and continue generating its own response from there.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9461125135421753, 'coordinates': {'points': ((np.float64(200.0), np.float64(1784.1004999999998)), (np.float64(200.0), np.float64(1963.2671666666668)), (np.float64(1506.9865333333332), np.float64(1963.2671666666668)), (np.float64(1506.9865333333332), np.float64(1784.1004999999998))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "The ﬁrst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In terms of the data when GPT (for the purposes of precision all of the following results include context) and STS OpenAI were combined, you got an average similarity score of 0.997. If you instead pair this with Palm STS Score instead, the average score drops to 0.92, a 0.077 decrease in performance. A similar eﬀect when using Palm with Palm STS and Palm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.6884109973907471, 'coordinates': {'points': ((np.float64(198.56924438476562), np.float64(2041.5426025390625)), (np.float64(198.56924438476562), np.float64(2064.440673828125)), (np.float64(394.0348815917969), np.float64(2064.440673828125)), (np.float64(394.0348815917969), np.float64(2041.5426025390625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 7, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': 'b7ee96980869976674c61a8c7b88ac8d'}\n",
      "ISSN: 2167-1907\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9441673159599304, 'coordinates': {'points': ((np.float64(197.56947326660156), np.float64(210.1004999999999)), (np.float64(197.56947326660156), np.float64(389.2671666666666)), (np.float64(1506.9475666666667), np.float64(389.2671666666666)), (np.float64(1506.9475666666667), np.float64(210.1004999999999))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "to reach very high accuracy levels when paired with a similarity score calculated from the same program (this means GPT worked better with OpenAI STS Score and that Palm worked better with Palm STS Score). What is also interest- ing to note is that although Palm produced a 0.001 lower performance than GPT it seemed to be more ﬂexible, working better with its competitor (OpenAI STS Score + Palm produced 0.93) than the GPT with its competitor (Palm STS Score + GPT produced 0.92).\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9382283091545105, 'coordinates': {'points': ((np.float64(200.0), np.float64(399.26716666666664)), (np.float64(200.0), np.float64(540.6004999999999)), (np.float64(1507.0060166666663), np.float64(540.6004999999999)), (np.float64(1507.0060166666663), np.float64(399.26716666666664))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "To recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap or replace things like the embedding layer used, the similarity score used, and the base language model used, also whether it was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM and the data collected speaks a lot to the importance of each aspect of an LLM.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9523764848709106, 'coordinates': {'points': ((np.float64(200.0), np.float64(550.6005)), (np.float64(200.0), np.float64(767.6005)), (np.float64(1506.9698333333329), np.float64(767.6005)), (np.float64(1506.9698333333329), np.float64(550.6005))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "In terms of expected results, two things were noticed, ﬁrst that Palm had a slightly lower performance than GPT (0.996 versus 0.997 respectively) on their top runs, however, there was also some contradictory data as it seemed that Palm worked signiﬁcantly better when given no context to work with compared to GPT, Palm produced an average score of 0.88 and 0.91 when given no context while GPT produced an average score of 0.75 and 0.897. Although GPT may perform much better when given context, Palm seems to beat it out just given its own proprietary dataset (no context).\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9552424550056458, 'coordinates': {'points': ((np.float64(200.0), np.float64(777.6005000000001)), (np.float64(200.0), np.float64(1070.2671666666665)), (np.float64(1506.9809666666663), np.float64(1070.2671666666665)), (np.float64(1506.9809666666663), np.float64(777.6005000000001))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "Some unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, pair- ing Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually performed better when paired with OpenAI’s embed. 0.91 (with OpenAI embed) versus 0.88 (Palm embed)—note that this is without context given. A similar eﬀect was noticed going the other way around as well, without context, GPT performed much better with Palm embedding layer than with its own OpenAI embedding layer (0.897 versus 0.75 respectively). This data shows how Palm embedding layer tends to perform much better given no context when com- pared to OpenAI’s embed. Similarly, to explored above, it is the opposite when given context, however. OpenAI’s embedding layer performs a bit better when given context across the board than Palm embed.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9496419429779053, 'coordinates': {'points': ((np.float64(200.0), np.float64(1080.2671666666668)), (np.float64(200.0), np.float64(1259.4338333333333)), (np.float64(1506.9758), np.float64(1259.4338333333333)), (np.float64(1506.9758), np.float64(1080.2671666666668))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "Most notably from this experiment was two realizations. First, that Palm Embedding layer tends to work much better when just given its own proprietary dataset (and no context), when compared to OpenAI’s embed. Additionally, when given context, the playing ﬁeld switches: Palm tends to perform much worse when given context when compared to OpenAI. Lastly, it is important to note that a combination of Palm/GPT with OpenAI’s embedding layer and context yielded extremely accurate results when its similarity scores were averaged.\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8832968473434448, 'coordinates': {'points': ((np.float64(200.0), np.float64(1312.3036666666667)), (np.float64(200.0), np.float64(1351.137)), (np.float64(393.0405), np.float64(1351.137)), (np.float64(393.0405), np.float64(1312.3036666666667))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '87970ec898abec7cd8347dfceee3a2e1'}\n",
      "Conclusion\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9510577917098999, 'coordinates': {'points': ((np.float64(200.0), np.float64(1397.6414794921875)), (np.float64(200.0), np.float64(1615.1005)), (np.float64(1506.9392166666667), np.float64(1615.1005)), (np.float64(1506.9392166666667), np.float64(1397.6414794921875))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '94bc826e9877b9de2bb6079c421d594a'}\n",
      "Building upon the foundation laid by our initial ﬁndings, it is paramount to recognize the exceptional performance of the Palm model when utilizing its proprietary dataset in conjunction with the Palm embed. This speciﬁcity in data and technology synchronization has shown that Palm outshines OpenAI in terms of model accuracy in a context-free envi- ronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT with its native OpenAI embedding layer excels, leveraging the additional context to produce responses of remarkable accuracy that resonate with the cultural and ecological nuances of the Amazon.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9465005397796631, 'coordinates': {'points': ((np.float64(198.11744689941406), np.float64(1625.1005)), (np.float64(198.11744689941406), np.float64(1804.2671666666668)), (np.float64(1506.9447833333327), np.float64(1804.2671666666668)), (np.float64(1506.9447833333327), np.float64(1625.1005))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '94bc826e9877b9de2bb6079c421d594a'}\n",
      "This pivot in performance based on context underscores the signiﬁcance of tailored datasets and embedding mechanisms in the optimization of Large Language Models (LLMs). The adaptability of the Retrieval-Augmented Generation (RAG) framework emerges as a cornerstone for future enhancements in LLMs. By enabling the seamless interchange of model components, RAG presents an evolutionary leap in the ﬁne-tuning of language models, catering to the intricate demands of culturally rich and contextually complex datasets.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9392344951629639, 'coordinates': {'points': ((np.float64(200.0), np.float64(1814.2671666666665)), (np.float64(200.0), np.float64(1993.4338333333333)), (np.float64(1506.9698333333333), np.float64(1993.4338333333333)), (np.float64(1506.9698333333333), np.float64(1814.2671666666665))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '94bc826e9877b9de2bb6079c421d594a'}\n",
      "In light of these advancements, our research signiﬁes a pivotal moment for LLMs. The evidence suggests that when models are ﬁnely tuned with an awareness of the dataset's inherent context and the corresponding embedding layers, they reach new heights of linguistic precision. Therefore, the path forward for LLMs lies in embracing the modular and contextually aware RAG framework, which promises to reﬁne the capabilities of language models to an unprecedented degree, ensuring the preservation and celebration of the world's diverse linguistic and cultural heritage.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.6504687070846558, 'coordinates': {'points': ((np.float64(199.09689331054688), np.float64(2041.906494140625)), (np.float64(199.09689331054688), np.float64(2064.438720703125)), (np.float64(394.32049560546875), np.float64(2064.438720703125)), (np.float64(394.32049560546875), np.float64(2041.906494140625))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 8, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '94bc826e9877b9de2bb6079c421d594a'}\n",
      "ISSN: 2167-1907\n",
      "Category:Title\n",
      "Metadata:{'detection_class_prob': 0.8782328963279724, 'coordinates': {'points': ((np.float64(199.386962890625), np.float64(215.13699999999983)), (np.float64(199.386962890625), np.float64(253.97033333333323)), (np.float64(397.54516666666666), np.float64(253.97033333333323)), (np.float64(397.54516666666666), np.float64(215.13699999999983))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '7f3974627d669c2a2a480412b0cd17a8'}\n",
      "Limitations\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9373156428337097, 'coordinates': {'points': ((np.float64(198.4431915283203), np.float64(300.9338333333334)), (np.float64(198.4431915283203), np.float64(404.4338333333333)), (np.float64(1506.9503499999998), np.float64(404.4338333333333)), (np.float64(1506.9503499999998), np.float64(300.9338333333334))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "The results of the “outputted answer” of this algorithm largely reﬂect the quality of the data. If the LLM is trained oﬀ low-quality data, then the answer will reﬂect this bias. The results of the experiment will ﬂuctuate with diﬀerent results should a diﬀerent dataset be used to train the LLM.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.9341223835945129, 'coordinates': {'points': ((np.float64(199.07540893554688), np.float64(414.42599999999993)), (np.float64(199.07540893554688), np.float64(517.9181666666665)), (np.float64(1506.853333333332), np.float64(517.9181666666665)), (np.float64(1506.853333333332), np.float64(414.42599999999993))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "In the future there is a lot of potential to expand on this research by breaking down the RAG algorithm into even more separate components to further see the diﬀerences in average similarity score that adding or removing each component makes.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7257945537567139, 'coordinates': {'points': ((np.float64(199.97216666666668), np.float64(656.3143333333333)), (np.float64(199.97216666666668), np.float64(722.2514999999996)), (np.float64(1486.3360595703125), np.float64(722.2514999999996)), (np.float64(1486.3360595703125), np.float64(656.3143333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "Bahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP), 4945-4949.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.7626606225967407, 'coordinates': {'points': ((np.float64(199.97216666666668), np.float64(731.965333333333)), (np.float64(199.97216666666668), np.float64(790.7501831054688)), (np.float64(1504.1391833333332), np.float64(790.7501831054688)), (np.float64(1504.1391833333332), np.float64(731.965333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "Devlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint,\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.8412608504295349, 'coordinates': {'points': ((np.float64(196.36825561523438), np.float64(807.6163333333329)), (np.float64(196.36825561523438), np.float64(873.5534999999996)), (np.float64(1394.230224609375), np.float64(873.5534999999996)), (np.float64(1394.230224609375), np.float64(807.6163333333329))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "Lewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 9459-9474.\n",
      "Category:NarrativeText\n",
      "Metadata:{'detection_class_prob': 0.5641652345657349, 'coordinates': {'points': ((np.float64(193.1947479248047), np.float64(883.267333333333)), (np.float64(193.1947479248047), np.float64(1135.1868333333332)), (np.float64(1478.4712833333322), np.float64(1135.1868333333332)), (np.float64(1478.4712833333322), np.float64(883.267333333333))), 'system': 'PixelSpace', 'layout_width': 1700, 'layout_height': 2200}, 'last_modified': '2025-08-01T09:08:08', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 9, 'file_directory': './pdf-docs', 'filename': 'rag_llm.pdf', 'parent_id': '342cbac2b96bbac18f60c1fd3b0a44db'}\n",
      "Radford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. Siriwardhana, S. W. (2023). Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics, 1-17. Vaswani, A. S. (2017). Attention is all you need. Advances in neural information processing systems. Yu, W. (2022). Retrieval-augmented generation across heterogeneous knowledge. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, 52-58.\n"
     ]
    }
   ],
   "source": [
    "for el in text_elements:\n",
    "    print(f\"Category:{el.category}\")\n",
    "    print(f\"Metadata:{el.metadata.to_dict()}\")\n",
    "    print(el.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74e3fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put text elements into docs\n",
    "text_docs = [\n",
    "    Document(page_content=el.text, metadata={\"source\":f\"{el.metadata.to_dict().get('file_directory', '')}/{el.metadata.to_dict().get('filename', '')}\", \"type\":\"text\", \"page_number\": el.metadata.to_dict().get(\"page_number\") })\n",
    "    for el in text_elements\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd802d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='A Retrieval-Augmented Generation Based Large Language Model Benchmarked on a Novel Dataset'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Kieran Pichai'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Menlo School'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='ABSTRACT'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='The evolution of natural language processing has seen marked advancements, particularly with the advent of models like BERT, Transformers, and GPT variants, with recent additions like GPT and Bard. This paper investigates the Retrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its constituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our research demonstrates the signiﬁcance of preserving indigenous cultures and biodiversity. The experiment employs a customizable RAG methodology, allowing for the interchangeability of various components, such as the base language model and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm exhibits superior performance without context. The results also suggest that models tend to perform optimally when paired with similarity scores from their native platforms. Conclusively, our approach showcases the potential of a modular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to tra- ditional ﬁne-tuning of large language models.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Introduction'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='The evolution of natural language processing models has seen signiﬁcant strides from rule-based approaches in the early stages of language understanding, eventually leading to the advent of neural networks. However, the full potential of these neural networks awaited the computational infrastructure to catch up. The pivotal moment arrived with the emergence of neural machine translation (NMT), exempliﬁed by Google Translate, which marked a turning point in machine language comprehension (Bahdanau, 2016). Subsequently, a plethora of advanced models, including BERT, Transformers, GPT-2, and GPT-3, have emerged, driving the ﬁeld forward. Recent notable additions to this landscape are models like GPT and Bard (Devlin, 2018) (Vaswani, 2017) (Radford, 2018). While ﬁne-tuning such models has proven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG) oﬀers a prom- ising alternative (Lewis, 2020) (Siriwardhana, 2023) (Yu, 2022).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Curiously, little attention has been devoted to dissecting the individual components of RAG and their respec- tive impacts on overall performance. In response to this gap, our paper undertakes a comprehensive investigation of the RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity and replaceability of its constituent modules. This research seeks to contribute to a deeper understanding of the mech- anisms underlying RAG and its potential for enhancing natural language understanding and generation. These Large Language Models (LLMs) exhibit a remarkable proﬁciency in replicating human language styles, achieving a level of linguistic verisimilitude that borders on the impeccable. In light of these capabilities, it is prudent to delve into the profound signiﬁcance of the Amazon rainforest, which equates to the importance of any ethnically or racially diverse nation across the globe. Within the vast expanse of the Amazon, an intricate tapestry of life unfolds, where millions of distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the indigenous populations who have made this ecosystem their home. The Amazon rainforest is not only a cradle of biological diversity but also a sanctuary for an array of religions and cultures, many of which teeter on the brink of'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice to the indigenous communities whose ancestral lands are enshrined within its boundaries. It is a call to safeguard the memories of the land, the traditions that have evolved within its embrace, and the very essence of their cultures. However, certain regions of the Amazon remain shrouded in obscurity, their ﬂora and fauna so rare that reliable and readily available information is conspicuously lacking in the vast repository of knowledge available on the internet. In this context, advanced LLMs play an instrumental role in addressing this deﬁcit by facilitating the dissemination of indigenous narratives and thereby amplifying awareness and appreciation of the rich tapestry of beliefs, practices, and traditional knowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures with the global community, emphasizing the paramount importance of preserving the cultural diversity interwoven within this vast rainforest. In sum, the overarching mission of this endeavor is twofold: to document and educate the Western world about hitherto unknown cultures while concurrently ensuring the enduring preservation of these inval- uable facets of human heritage and biodiversity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Proposed Experiment'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Background and Importance'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally rich as the Amazon Rainforest, cannot be overstated. This knowledge, passed down through generations, encompasses not only cultural and religious beliefs but also practical insights into the local ﬂora and fauna. As the modern world encroaches on these lands, this wisdom is in peril of being lost forever. Recognizing this, our proposed experiment aims to employ a state- of-the-art Retrieval-Augmented Generation (RAG) framework to capture and leverage this vast, yet vulnerable, knowledge base.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Our dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled in its depth and breadth. It includes detailed discussions on religious practices, cultural nuances, and the integral role of the surrounding ecosystem in the daily lives of these communities. This data is not just a scientiﬁc or anthropological resource; it is a repository of living history and an urgent call to action for preservation eﬀorts.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='By integrating this unique dataset into the RAG framework, we anticipate not only the preservation of knowledge but also the generation of responses that reﬂect the rich tapestry of Amazonian life. The experiment is designed to evaluate how diﬀerent components within the RAG setup—such as base language models and similarity scoring algorithms—can be optimized to reﬂect the nuances captured within our dataset. In doing so, we aim to bridge the gap between advanced language models and the profound human insights found within the Amazon.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The central objective of our experiment is twofold: to analyze the performance implications of modular design within the RAG framework and to demonstrate the profound capability of such a system to preserve and communicate the wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper understanding of the data but also allow us to ﬁne-tune the system for optimal performance across diﬀerent conﬁgu- rations. To achieve this, we will systematically explore the interchangeability of various RAG components. We will assess diﬀerent base language models such as GPT and Palm and compare the eﬃcacy of similarity scoring tools from diverse platforms. The experiment will rigorously test these combinations, identifying which synergies most eﬀectively capture the essence of the dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The ultimate goal is to showcase the potential of a modular RAG system in processing culturally signiﬁcant information, paving the way for future applications that can beneﬁt from such tailored language models. We anticipate that our ﬁndings will contribute signiﬁcantly to the ﬁelds of computational linguistics and cultural preservation, demon- strating a novel approach to the application of large language models.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Source and Composition'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Our proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of verbal histories, inter- views, and ecological insights gathered from the indigenous peoples of the Amazon Rainforest, as well as from biolo- gists and ecologists dedicated to studying this unique biome. The dataset is characterized by its diversity, comprising narratives that elucidate the intricate relationship between the natives and their environment, including the religious and cultural signiﬁcance of plant and animal life.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The data collection was an extensive process, where linguists and researchers engaged in deep conversations with the natives, recording their dialects, translating their stories, and documenting their knowledge of the ecological system. Similarly, biologists contributed their decades of research on the ﬂora and fauna, providing a scientiﬁc per- spective to the indigenous narratives. The data thus forms a conﬂuence of traditional wisdom and modern scientiﬁc understanding, oﬀering a 360-degree view of the Amazon Rainforest's ecosystem.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Cultural and Environmental Signiﬁcance'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a race against time, as globalization and environmental degradation threaten to erase unique cultures and the wisdom they hold. Our dataset serves as a digital ark, a means to preserve and perpetuate the knowledge that has sustained the Amazon's communities\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The environmental signiﬁcance of the Amazon Rainforest cannot be overstated—it is a keystone of global biodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling the ecological interdependencies that are vital for the rainforest's survival. This dataset, therefore, is not just an aca- demic or technological asset; it is a critical record for environmental conservationists and policymakers.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"Through our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under- standing of their habitat is unmatched. By integrating their knowledge into the RAG framework, we hope to create a model that not only responds with information but also with wisdom that respects the interconnectedness of life and culture.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Retrieval-Augmented Generation Framework'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='The heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophisticated algorithm that enables the deconstruction of the language model into discrete, interchangeable components. This framework integrates a retriever model that sources relevant context and a generator model that synthesizes the retrieved infor- mation into coherent responses.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='In mathematical terms, given an input query , the retriever model searches a knowledge base and retrieves a set of relevant documents . Each document is represented as a vector in a high-dimensional 𝒦𝒦 𝑞𝑞 space, obtained from an embedding layer. This process transforms the raw text data into a structured form amenable 𝐷𝐷 = {𝑑𝑑1,𝑑𝑑2,…,𝑑𝑑𝑘𝑘} 𝐯𝐯𝑑𝑑 𝑑𝑑 to computational manipulation.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='To examine the eﬀects of component interchangeability, we adopt various base language models and similar- ity scoring mechanisms. For instance, if denotes the embedding function, and and represent the input and target text sequences, respectively, their vector representations would be 𝐸𝐸 larity as the basis for our similarity score, deﬁned by the formula: and 𝑠𝑠 𝑡𝑡 . We employ cosine simi-'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='similarity(𝐯𝐯𝑠𝑠,𝐯𝐯𝑡𝑡) = denotes the dot product between the two vectors, and Here, denotes the Euclidean norm. This score quantiﬁes �|v𝑠𝑠|� �|v𝑡𝑡|� the closeness of the semantic meaning represented by the vectors, with a value of 1 indicating identical directionality �|⋅|� ⋅ and thus, maximal similarity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The experiment tests diﬀerent conﬁgurations by substituting with embedding functions from various mod- els (e.g., GPT, Palm), allowing us to discern the impact of the embedding layer on the ﬁnal similarity score. By com- 𝐸𝐸 paring the performance of diﬀerent choices, we can identify which embeddings yield the most semantically rich representations for our unique dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The experiment commences with the training of the language models using our unique dataset. For the training phase, we deﬁne the following:'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=': The base language model, which can be either GPT or Palm.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=': The training dataset, consisting of pairs ℒ answer. where is a query from the dataset and is the corresponding'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The language model is ﬁne-tuned on , optimizing the weights to minimize the loss function, typically a cross-entropy loss between the predicted and actual answers.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Following training, the question-answering process involves feeding a new query to the trained model and retrieving the answer . This answer is then compared to a predeﬁned list of correct answers using the similarity score, which is 𝑞𝑞′ fundamental to evaluating the model's performance.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='Benchmarking and Evaluation'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The evaluation metric for our experiment is based on the similarity scores between the generated responses and a set of reference answers. Let be the set of answers generated by the model, and be the set of reference answers. We deﬁne the average similarity score as follows: ′ ′ ′ ∗ ∗ ∗'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=') This average score acts as the primary benchmark for comparing diﬀerent model conﬁgurations. We systematically n 𝑖𝑖=1 record the scores across various combinations of language models and similarity scoring mechanisms to assess which conﬁgurations yield the highest average similarity, indicating the most eﬀective model setup for our dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Additionally, we account for the presence or absence of context in the model's training and response genera- tion. This is critical, as the presence of context has been shown to signiﬁcantly inﬂuence model performance, particu- larly in the domain of indigenous knowledge and biodiversity, where context provides essential background information that can drastically aﬀect the meaning and relevance of a response.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Through this meticulous experimental setup, we aim to illuminate the intricate dynamics between diﬀerent components of the RAG framework and their collective impact on the model's ability to accurately replicate and convey the richness of the Amazon Rainforest's cultural and ecological knowledge.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='Pre-Experiment Performance Expectations and Discussion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='In the landscape of varying conﬁgurations, we hypothesize that certain setups will yield higher average similarity scores than others, indicative of more nuanced and accurate language generation. Particularly, we expect that: The similarity scores for models trained with contextual data will surpass those trained without, due to the enriched understanding and background the model has of the subject matter.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='When aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em- bed), the semantic vector representations should align more closely, thus producing higher similarity scores. The modular nature of the RAG setup will reveal that certain combinations of base language models and similarity (𝐯𝐯𝑠𝑠,𝐯𝐯𝑡𝑡) scoring mechanisms are more eﬀective than others, depending on whether context is included or not.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='We denote the expected performance increase due to context as , and the alignment of native embeddings as . Mathematically, we can represent our hypothesis as:'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='These hypotheses will be tested through a series of experiments, allowing us to determine the optimal model conﬁgu- Scoreavg,native > Scoreavg,non−native + ∆native ration for processing and generating responses reﬂective of the Amazon Rainforest dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Potential Implications for LLMs'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='The results of this experiment are expected to have signiﬁcant implications for the development and ﬁne-tuning of Large Language Models (LLMs). By identifying the most eﬀective conﬁgurations, we can oﬀer insights into the adapt- ability of these models to specialized datasets, which is crucial for applications that require a high degree of cultural and contextual sensitivity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Moreover, the experiment is poised to challenge the prevailing approach to LLM training and ﬁne-tuning, which often relies on static, one-size-ﬁts-all models. Our ﬁndings could suggest a shift towards a more dynamic, com- ponent-based approach, allowing for greater ﬂexibility and precision in model performance across diverse domains.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"The potential success of the RAG framework in this context may also pave the way for more granular im- provements in LLMs, beyond the standard metrics of accuracy and ﬂuency. It may, for instance, enhance the models' ability to engage with and preserve less-represented languages and dialects, fostering greater inclusivity and diversity in the realm of natural language processing.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Implications for Indigenous Knowledge Preservation'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"The signiﬁcance of our experiment extends beyond the technical accomplishments within the ﬁeld of natural language processing. It serves as a testament to the power of advanced computational techniques in preserving the rich tapestry of human culture, particularly the imperiled knowledge of the Amazon Rainforest's indigenous peoples. By success- fully training a language model to accurately reﬂect and communicate this knowledge, we not only preserve it for future generations but also validate the importance of linguistic and cultural diversity in our global ecosystem.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"This experiment, should it succeed, will demonstrate a practical application of LLMs in the service of cultural preservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns with the broader objectives of UNESCO's Intangible Cultural Heritage initiatives. It serves as a model for how com- munities around the world can leverage technology to protect and share their unique cultural identities and knowledge systems.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Advancements in RAG Framework'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='From a methodological standpoint, our experiment is poised to contribute to the advancement of the RAG framework within the realm of AI language models. By dissecting the RAG components and examining their interplay, we will gain insights into the mechanics of modular design in language models, oﬀering a blueprint for future research and development.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='The outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable framework, one that can be customized for specialized datasets and applications. This adaptability is critical as the demand for LLMs expands into increasingly varied and complex domains, from legal and medical to historical and anthropological.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"Furthermore, the experiment's focus on modularity could inspire a new wave of research into component- based architectures for LLMs. Such architectures may provide a more sustainable and eﬃcient pathway to model im- provement, as opposed to the computationally intensive process of training large models from scratch.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='In conclusion, the proposed experiment holds the potential to make signiﬁcant contributions to both the ﬁeld of AI and the preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative future for LLMs, where the voices of all communities can be heard and understood.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='Figure 1. Venn Diagram of Data Sources for RAG. This ﬁgure represents a venn diagram of 3 sources of information (google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the “out-'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='Figure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG algorithm from the input of the “user question” to the “outputted answer of the user question.”'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Experimental Results and Discussion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='The purpose of this section is to lay down the diﬀerent steps and customizations used within our experiment in order to demonstrate the conclusive results of this experiment to the reader; our experiment using a RAG methodology accurately shows how each component of a LLM positively or negatively aﬀects the accuracy of the outcome itself. In the initial world of LLM, in order to incrementally increase its performance engineers of these models would have to ﬁne tune them then retrain which took immense amounts of power and large amounts of null results. However, now as they become more and more complex to tune models like OpenAI’s GPT and Google’s Bard have been plateauing performance wise.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Table 1. Experimental Results. This table represents the various diﬀerent combinations of LLM components with respect to the average similarity score they each produced.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='This paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to com- ponentize the models and break them down into smaller sections. This allows the user to add certain parts / combina- tions to test the performance of those then to substitute diﬀerent modules in to see which leads to the largest perfor- mance increase over the other. These customizable steps allow you to see minute diﬀerences in performance that slowly tuning a model couldn’t have shown you previously. This is a novel way to approach the tuning of LLM and will only serve to increase their accuracy as time moves forward.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Another major component of our RAG methodology is the ability to switch out which embedding layer you use. The standard embedding (OpenAI) or Palm’s embed. When choosing between both of those some tradeoﬀs are made.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='When using no context, Palm’s embedding layer seems to perform much better across the board, allowing for a much higher average similarity score, however this drastically shifts when given context as now OpenAI’s embedding layer performs much more soundly. The evidence for these claims is discussed later in this paper.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Additionally, another beneﬁcial feature of the RAG optimization and breakdown style is the ability to cus- tomize which similarity score the LLM uses to decide which answer to base its response oﬀ from the Q-A list. To go into further detail, the code when prompted with a user question compares the user question to the Q-A list and reorders the list based oﬀ highest similarity score to lowest, this allows the LLM to select the top 2-3 answers to the highest ranked questions and continue generating its own response from there.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='The ﬁrst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In terms of the data when GPT (for the purposes of precision all of the following results include context) and STS OpenAI were combined, you got an average similarity score of 0.997. If you instead pair this with Palm STS Score instead, the average score drops to 0.92, a 0.077 decrease in performance. A similar eﬀect when using Palm with Palm STS and Palm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='to reach very high accuracy levels when paired with a similarity score calculated from the same program (this means GPT worked better with OpenAI STS Score and that Palm worked better with Palm STS Score). What is also interest- ing to note is that although Palm produced a 0.001 lower performance than GPT it seemed to be more ﬂexible, working better with its competitor (OpenAI STS Score + Palm produced 0.93) than the GPT with its competitor (Palm STS Score + GPT produced 0.92).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='To recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap or replace things like the embedding layer used, the similarity score used, and the base language model used, also whether it was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM and the data collected speaks a lot to the importance of each aspect of an LLM.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='In terms of expected results, two things were noticed, ﬁrst that Palm had a slightly lower performance than GPT (0.996 versus 0.997 respectively) on their top runs, however, there was also some contradictory data as it seemed that Palm worked signiﬁcantly better when given no context to work with compared to GPT, Palm produced an average score of 0.88 and 0.91 when given no context while GPT produced an average score of 0.75 and 0.897. Although GPT may perform much better when given context, Palm seems to beat it out just given its own proprietary dataset (no context).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Some unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, pair- ing Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually performed better when paired with OpenAI’s embed. 0.91 (with OpenAI embed) versus 0.88 (Palm embed)—note that this is without context given. A similar eﬀect was noticed going the other way around as well, without context, GPT performed much better with Palm embedding layer than with its own OpenAI embedding layer (0.897 versus 0.75 respectively). This data shows how Palm embedding layer tends to perform much better given no context when com- pared to OpenAI’s embed. Similarly, to explored above, it is the opposite when given context, however. OpenAI’s embedding layer performs a bit better when given context across the board than Palm embed.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Most notably from this experiment was two realizations. First, that Palm Embedding layer tends to work much better when just given its own proprietary dataset (and no context), when compared to OpenAI’s embed. Additionally, when given context, the playing ﬁeld switches: Palm tends to perform much worse when given context when compared to OpenAI. Lastly, it is important to note that a combination of Palm/GPT with OpenAI’s embedding layer and context yielded extremely accurate results when its similarity scores were averaged.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Conclusion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Building upon the foundation laid by our initial ﬁndings, it is paramount to recognize the exceptional performance of the Palm model when utilizing its proprietary dataset in conjunction with the Palm embed. This speciﬁcity in data and technology synchronization has shown that Palm outshines OpenAI in terms of model accuracy in a context-free envi- ronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT with its native OpenAI embedding layer excels, leveraging the additional context to produce responses of remarkable accuracy that resonate with the cultural and ecological nuances of the Amazon.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='This pivot in performance based on context underscores the signiﬁcance of tailored datasets and embedding mechanisms in the optimization of Large Language Models (LLMs). The adaptability of the Retrieval-Augmented Generation (RAG) framework emerges as a cornerstone for future enhancements in LLMs. By enabling the seamless interchange of model components, RAG presents an evolutionary leap in the ﬁne-tuning of language models, catering to the intricate demands of culturally rich and contextually complex datasets.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content=\"In light of these advancements, our research signiﬁes a pivotal moment for LLMs. The evidence suggests that when models are ﬁnely tuned with an awareness of the dataset's inherent context and the corresponding embedding layers, they reach new heights of linguistic precision. Therefore, the path forward for LLMs lies in embracing the modular and contextually aware RAG framework, which promises to reﬁne the capabilities of language models to an unprecedented degree, ensuring the preservation and celebration of the world's diverse linguistic and cultural heritage.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Limitations'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='The results of the “outputted answer” of this algorithm largely reﬂect the quality of the data. If the LLM is trained oﬀ low-quality data, then the answer will reﬂect this bias. The results of the experiment will ﬂuctuate with diﬀerent results should a diﬀerent dataset be used to train the LLM.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='In the future there is a lot of potential to expand on this research by breaking down the RAG algorithm into even more separate components to further see the diﬀerences in average similarity score that adding or removing each component makes.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Bahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP), 4945-4949.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Devlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint,'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Lewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 9459-9474.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Radford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. Siriwardhana, S. W. (2023). Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics, 1-17. Vaswani, A. S. (2017). Attention is all you need. Advances in neural information processing systems. Yu, W. (2022). Retrieval-augmented generation across heterogeneous knowledge. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, 52-58.')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8c92ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='A Retrieval-Augmented Generation Based Large Language Model Benchmarked on a Novel Dataset'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Kieran Pichai'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Menlo School'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='ABSTRACT'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='The evolution of natural language processing has seen marked advancements, particularly with the advent of models like BERT, Transformers, and GPT variants, with recent additions like GPT and Bard. This paper investigates the Retrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its constituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our research demonstrates the signiﬁcance of preserving indigenous cultures and biodiversity. The experiment employs a customizable RAG methodology, allowing for the interchangeability of various components, such as the base language model and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm exhibits superior performance without context. The results also suggest that models tend to perform optimally when paired with similarity scores from their native platforms. Conclusively, our approach showcases the potential of a modular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to tra- ditional ﬁne-tuning of large language models.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Introduction'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='The evolution of natural language processing models has seen signiﬁcant strides from rule-based approaches in the early stages of language understanding, eventually leading to the advent of neural networks. However, the full potential of these neural networks awaited the computational infrastructure to catch up. The pivotal moment arrived with the emergence of neural machine translation (NMT), exempliﬁed by Google Translate, which marked a turning point in machine language comprehension (Bahdanau, 2016). Subsequently, a plethora of advanced models, including BERT, Transformers, GPT-2, and GPT-3, have emerged, driving the ﬁeld forward. Recent notable additions to this landscape are models like GPT and Bard (Devlin, 2018) (Vaswani, 2017) (Radford, 2018). While ﬁne-tuning such models has proven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG) oﬀers a prom- ising alternative (Lewis, 2020) (Siriwardhana, 2023) (Yu, 2022).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='Curiously, little attention has been devoted to dissecting the individual components of RAG and their respec- tive impacts on overall performance. In response to this gap, our paper undertakes a comprehensive investigation of the RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity and replaceability of its constituent modules. This research seeks to contribute to a deeper understanding of the mech- anisms underlying RAG and its potential for enhancing natural language understanding and generation. These Large Language Models (LLMs) exhibit a remarkable proﬁciency in replicating human language styles, achieving a level of linguistic verisimilitude that borders on the impeccable. In light of these capabilities, it is prudent to delve into the profound signiﬁcance of the Amazon rainforest, which equates to the importance of any ethnically or racially diverse nation across the globe. Within the vast expanse of the Amazon, an intricate tapestry of life unfolds, where millions of distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the indigenous populations who have made this ecosystem their home. The Amazon rainforest is not only a cradle of biological diversity but also a sanctuary for an array of religions and cultures, many of which teeter on the brink of'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 1}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice to the indigenous communities whose ancestral lands are enshrined within its boundaries. It is a call to safeguard the memories of the land, the traditions that have evolved within its embrace, and the very essence of their cultures. However, certain regions of the Amazon remain shrouded in obscurity, their ﬂora and fauna so rare that reliable and readily available information is conspicuously lacking in the vast repository of knowledge available on the internet. In this context, advanced LLMs play an instrumental role in addressing this deﬁcit by facilitating the dissemination of indigenous narratives and thereby amplifying awareness and appreciation of the rich tapestry of beliefs, practices, and traditional knowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures with the global community, emphasizing the paramount importance of preserving the cultural diversity interwoven within this vast rainforest. In sum, the overarching mission of this endeavor is twofold: to document and educate the Western world about hitherto unknown cultures while concurrently ensuring the enduring preservation of these inval- uable facets of human heritage and biodiversity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Proposed Experiment'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Background and Importance'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally rich as the Amazon Rainforest, cannot be overstated. This knowledge, passed down through generations, encompasses not only cultural and religious beliefs but also practical insights into the local ﬂora and fauna. As the modern world encroaches on these lands, this wisdom is in peril of being lost forever. Recognizing this, our proposed experiment aims to employ a state- of-the-art Retrieval-Augmented Generation (RAG) framework to capture and leverage this vast, yet vulnerable, knowledge base.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='Our dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled in its depth and breadth. It includes detailed discussions on religious practices, cultural nuances, and the integral role of the surrounding ecosystem in the daily lives of these communities. This data is not just a scientiﬁc or anthropological resource; it is a repository of living history and an urgent call to action for preservation eﬀorts.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='By integrating this unique dataset into the RAG framework, we anticipate not only the preservation of knowledge but also the generation of responses that reﬂect the rich tapestry of Amazonian life. The experiment is designed to evaluate how diﬀerent components within the RAG setup—such as base language models and similarity scoring algorithms—can be optimized to reﬂect the nuances captured within our dataset. In doing so, we aim to bridge the gap between advanced language models and the profound human insights found within the Amazon.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The central objective of our experiment is twofold: to analyze the performance implications of modular design within the RAG framework and to demonstrate the profound capability of such a system to preserve and communicate the wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper understanding of the data but also allow us to ﬁne-tune the system for optimal performance across diﬀerent conﬁgu- rations. To achieve this, we will systematically explore the interchangeability of various RAG components. We will assess diﬀerent base language models such as GPT and Palm and compare the eﬃcacy of similarity scoring tools from diverse platforms. The experiment will rigorously test these combinations, identifying which synergies most eﬀectively capture the essence of the dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 2}, page_content='The ultimate goal is to showcase the potential of a modular RAG system in processing culturally signiﬁcant information, paving the way for future applications that can beneﬁt from such tailored language models. We anticipate that our ﬁndings will contribute signiﬁcantly to the ﬁelds of computational linguistics and cultural preservation, demon- strating a novel approach to the application of large language models.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Source and Composition'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Our proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of verbal histories, inter- views, and ecological insights gathered from the indigenous peoples of the Amazon Rainforest, as well as from biolo- gists and ecologists dedicated to studying this unique biome. The dataset is characterized by its diversity, comprising narratives that elucidate the intricate relationship between the natives and their environment, including the religious and cultural signiﬁcance of plant and animal life.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The data collection was an extensive process, where linguists and researchers engaged in deep conversations with the natives, recording their dialects, translating their stories, and documenting their knowledge of the ecological system. Similarly, biologists contributed their decades of research on the ﬂora and fauna, providing a scientiﬁc per- spective to the indigenous narratives. The data thus forms a conﬂuence of traditional wisdom and modern scientiﬁc understanding, oﬀering a 360-degree view of the Amazon Rainforest's ecosystem.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Cultural and Environmental Signiﬁcance'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a race against time, as globalization and environmental degradation threaten to erase unique cultures and the wisdom they hold. Our dataset serves as a digital ark, a means to preserve and perpetuate the knowledge that has sustained the Amazon's communities\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"The environmental signiﬁcance of the Amazon Rainforest cannot be overstated—it is a keystone of global biodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling the ecological interdependencies that are vital for the rainforest's survival. This dataset, therefore, is not just an aca- demic or technological asset; it is a critical record for environmental conservationists and policymakers.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content=\"Through our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under- standing of their habitat is unmatched. By integrating their knowledge into the RAG framework, we hope to create a model that not only responds with information but also with wisdom that respects the interconnectedness of life and culture.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='Retrieval-Augmented Generation Framework'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='The heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophisticated algorithm that enables the deconstruction of the language model into discrete, interchangeable components. This framework integrates a retriever model that sources relevant context and a generator model that synthesizes the retrieved infor- mation into coherent responses.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='In mathematical terms, given an input query , the retriever model searches a knowledge base and retrieves a set of relevant documents . Each document is represented as a vector in a high-dimensional 𝒦𝒦 𝑞𝑞 space, obtained from an embedding layer. This process transforms the raw text data into a structured form amenable 𝐷𝐷 = {𝑑𝑑1,𝑑𝑑2,…,𝑑𝑑𝑘𝑘} 𝐯𝐯𝑑𝑑 𝑑𝑑 to computational manipulation.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='To examine the eﬀects of component interchangeability, we adopt various base language models and similar- ity scoring mechanisms. For instance, if denotes the embedding function, and and represent the input and target text sequences, respectively, their vector representations would be 𝐸𝐸 larity as the basis for our similarity score, deﬁned by the formula: and 𝑠𝑠 𝑡𝑡 . We employ cosine simi-'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='similarity(𝐯𝐯𝑠𝑠,𝐯𝐯𝑡𝑡) = denotes the dot product between the two vectors, and Here, denotes the Euclidean norm. This score quantiﬁes �|v𝑠𝑠|� �|v𝑡𝑡|� the closeness of the semantic meaning represented by the vectors, with a value of 1 indicating identical directionality �|⋅|� ⋅ and thus, maximal similarity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 3}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The experiment tests diﬀerent conﬁgurations by substituting with embedding functions from various mod- els (e.g., GPT, Palm), allowing us to discern the impact of the embedding layer on the ﬁnal similarity score. By com- 𝐸𝐸 paring the performance of diﬀerent choices, we can identify which embeddings yield the most semantically rich representations for our unique dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The experiment commences with the training of the language models using our unique dataset. For the training phase, we deﬁne the following:'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=': The base language model, which can be either GPT or Palm.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=': The training dataset, consisting of pairs ℒ answer. where is a query from the dataset and is the corresponding'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The language model is ﬁne-tuned on , optimizing the weights to minimize the loss function, typically a cross-entropy loss between the predicted and actual answers.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Following training, the question-answering process involves feeding a new query to the trained model and retrieving the answer . This answer is then compared to a predeﬁned list of correct answers using the similarity score, which is 𝑞𝑞′ fundamental to evaluating the model's performance.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='Benchmarking and Evaluation'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='The evaluation metric for our experiment is based on the similarity scores between the generated responses and a set of reference answers. Let be the set of answers generated by the model, and be the set of reference answers. We deﬁne the average similarity score as follows: ′ ′ ′ ∗ ∗ ∗'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=') This average score acts as the primary benchmark for comparing diﬀerent model conﬁgurations. We systematically n 𝑖𝑖=1 record the scores across various combinations of language models and similarity scoring mechanisms to assess which conﬁgurations yield the highest average similarity, indicating the most eﬀective model setup for our dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Additionally, we account for the presence or absence of context in the model's training and response genera- tion. This is critical, as the presence of context has been shown to signiﬁcantly inﬂuence model performance, particu- larly in the domain of indigenous knowledge and biodiversity, where context provides essential background information that can drastically aﬀect the meaning and relevance of a response.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content=\"Through this meticulous experimental setup, we aim to illuminate the intricate dynamics between diﬀerent components of the RAG framework and their collective impact on the model's ability to accurately replicate and convey the richness of the Amazon Rainforest's cultural and ecological knowledge.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='Pre-Experiment Performance Expectations and Discussion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='In the landscape of varying conﬁgurations, we hypothesize that certain setups will yield higher average similarity scores than others, indicative of more nuanced and accurate language generation. Particularly, we expect that: The similarity scores for models trained with contextual data will surpass those trained without, due to the enriched understanding and background the model has of the subject matter.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='When aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em- bed), the semantic vector representations should align more closely, thus producing higher similarity scores. The modular nature of the RAG setup will reveal that certain combinations of base language models and similarity (𝐯𝐯𝑠𝑠,𝐯𝐯𝑡𝑡) scoring mechanisms are more eﬀective than others, depending on whether context is included or not.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 4}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='We denote the expected performance increase due to context as , and the alignment of native embeddings as . Mathematically, we can represent our hypothesis as:'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='These hypotheses will be tested through a series of experiments, allowing us to determine the optimal model conﬁgu- Scoreavg,native > Scoreavg,non−native + ∆native ration for processing and generating responses reﬂective of the Amazon Rainforest dataset.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Potential Implications for LLMs'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='The results of this experiment are expected to have signiﬁcant implications for the development and ﬁne-tuning of Large Language Models (LLMs). By identifying the most eﬀective conﬁgurations, we can oﬀer insights into the adapt- ability of these models to specialized datasets, which is crucial for applications that require a high degree of cultural and contextual sensitivity.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Moreover, the experiment is poised to challenge the prevailing approach to LLM training and ﬁne-tuning, which often relies on static, one-size-ﬁts-all models. Our ﬁndings could suggest a shift towards a more dynamic, com- ponent-based approach, allowing for greater ﬂexibility and precision in model performance across diverse domains.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"The potential success of the RAG framework in this context may also pave the way for more granular im- provements in LLMs, beyond the standard metrics of accuracy and ﬂuency. It may, for instance, enhance the models' ability to engage with and preserve less-represented languages and dialects, fostering greater inclusivity and diversity in the realm of natural language processing.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Implications for Indigenous Knowledge Preservation'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"The signiﬁcance of our experiment extends beyond the technical accomplishments within the ﬁeld of natural language processing. It serves as a testament to the power of advanced computational techniques in preserving the rich tapestry of human culture, particularly the imperiled knowledge of the Amazon Rainforest's indigenous peoples. By success- fully training a language model to accurately reﬂect and communicate this knowledge, we not only preserve it for future generations but also validate the importance of linguistic and cultural diversity in our global ecosystem.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"This experiment, should it succeed, will demonstrate a practical application of LLMs in the service of cultural preservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns with the broader objectives of UNESCO's Intangible Cultural Heritage initiatives. It serves as a model for how com- munities around the world can leverage technology to protect and share their unique cultural identities and knowledge systems.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Advancements in RAG Framework'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='From a methodological standpoint, our experiment is poised to contribute to the advancement of the RAG framework within the realm of AI language models. By dissecting the RAG components and examining their interplay, we will gain insights into the mechanics of modular design in language models, oﬀering a blueprint for future research and development.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='The outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable framework, one that can be customized for specialized datasets and applications. This adaptability is critical as the demand for LLMs expands into increasingly varied and complex domains, from legal and medical to historical and anthropological.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content=\"Furthermore, the experiment's focus on modularity could inspire a new wave of research into component- based architectures for LLMs. Such architectures may provide a more sustainable and eﬃcient pathway to model im- provement, as opposed to the computationally intensive process of training large models from scratch.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='In conclusion, the proposed experiment holds the potential to make signiﬁcant contributions to both the ﬁeld of AI and the preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative future for LLMs, where the voices of all communities can be heard and understood.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='Figure 1. Venn Diagram of Data Sources for RAG. This ﬁgure represents a venn diagram of 3 sources of information (google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the “out-'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 6}, page_content='Figure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG algorithm from the input of the “user question” to the “outputted answer of the user question.”'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Experimental Results and Discussion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='The purpose of this section is to lay down the diﬀerent steps and customizations used within our experiment in order to demonstrate the conclusive results of this experiment to the reader; our experiment using a RAG methodology accurately shows how each component of a LLM positively or negatively aﬀects the accuracy of the outcome itself. In the initial world of LLM, in order to incrementally increase its performance engineers of these models would have to ﬁne tune them then retrain which took immense amounts of power and large amounts of null results. However, now as they become more and more complex to tune models like OpenAI’s GPT and Google’s Bard have been plateauing performance wise.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Table 1. Experimental Results. This table represents the various diﬀerent combinations of LLM components with respect to the average similarity score they each produced.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='This paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to com- ponentize the models and break them down into smaller sections. This allows the user to add certain parts / combina- tions to test the performance of those then to substitute diﬀerent modules in to see which leads to the largest perfor- mance increase over the other. These customizable steps allow you to see minute diﬀerences in performance that slowly tuning a model couldn’t have shown you previously. This is a novel way to approach the tuning of LLM and will only serve to increase their accuracy as time moves forward.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Another major component of our RAG methodology is the ability to switch out which embedding layer you use. The standard embedding (OpenAI) or Palm’s embed. When choosing between both of those some tradeoﬀs are made.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='When using no context, Palm’s embedding layer seems to perform much better across the board, allowing for a much higher average similarity score, however this drastically shifts when given context as now OpenAI’s embedding layer performs much more soundly. The evidence for these claims is discussed later in this paper.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='Additionally, another beneﬁcial feature of the RAG optimization and breakdown style is the ability to cus- tomize which similarity score the LLM uses to decide which answer to base its response oﬀ from the Q-A list. To go into further detail, the code when prompted with a user question compares the user question to the Q-A list and reorders the list based oﬀ highest similarity score to lowest, this allows the LLM to select the top 2-3 answers to the highest ranked questions and continue generating its own response from there.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='The ﬁrst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In terms of the data when GPT (for the purposes of precision all of the following results include context) and STS OpenAI were combined, you got an average similarity score of 0.997. If you instead pair this with Palm STS Score instead, the average score drops to 0.92, a 0.077 decrease in performance. A similar eﬀect when using Palm with Palm STS and Palm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 7}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='to reach very high accuracy levels when paired with a similarity score calculated from the same program (this means GPT worked better with OpenAI STS Score and that Palm worked better with Palm STS Score). What is also interest- ing to note is that although Palm produced a 0.001 lower performance than GPT it seemed to be more ﬂexible, working better with its competitor (OpenAI STS Score + Palm produced 0.93) than the GPT with its competitor (Palm STS Score + GPT produced 0.92).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='To recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap or replace things like the embedding layer used, the similarity score used, and the base language model used, also whether it was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM and the data collected speaks a lot to the importance of each aspect of an LLM.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='In terms of expected results, two things were noticed, ﬁrst that Palm had a slightly lower performance than GPT (0.996 versus 0.997 respectively) on their top runs, however, there was also some contradictory data as it seemed that Palm worked signiﬁcantly better when given no context to work with compared to GPT, Palm produced an average score of 0.88 and 0.91 when given no context while GPT produced an average score of 0.75 and 0.897. Although GPT may perform much better when given context, Palm seems to beat it out just given its own proprietary dataset (no context).'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Some unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, pair- ing Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually performed better when paired with OpenAI’s embed. 0.91 (with OpenAI embed) versus 0.88 (Palm embed)—note that this is without context given. A similar eﬀect was noticed going the other way around as well, without context, GPT performed much better with Palm embedding layer than with its own OpenAI embedding layer (0.897 versus 0.75 respectively). This data shows how Palm embedding layer tends to perform much better given no context when com- pared to OpenAI’s embed. Similarly, to explored above, it is the opposite when given context, however. OpenAI’s embedding layer performs a bit better when given context across the board than Palm embed.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Most notably from this experiment was two realizations. First, that Palm Embedding layer tends to work much better when just given its own proprietary dataset (and no context), when compared to OpenAI’s embed. Additionally, when given context, the playing ﬁeld switches: Palm tends to perform much worse when given context when compared to OpenAI. Lastly, it is important to note that a combination of Palm/GPT with OpenAI’s embedding layer and context yielded extremely accurate results when its similarity scores were averaged.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Conclusion'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='Building upon the foundation laid by our initial ﬁndings, it is paramount to recognize the exceptional performance of the Palm model when utilizing its proprietary dataset in conjunction with the Palm embed. This speciﬁcity in data and technology synchronization has shown that Palm outshines OpenAI in terms of model accuracy in a context-free envi- ronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT with its native OpenAI embedding layer excels, leveraging the additional context to produce responses of remarkable accuracy that resonate with the cultural and ecological nuances of the Amazon.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='This pivot in performance based on context underscores the signiﬁcance of tailored datasets and embedding mechanisms in the optimization of Large Language Models (LLMs). The adaptability of the Retrieval-Augmented Generation (RAG) framework emerges as a cornerstone for future enhancements in LLMs. By enabling the seamless interchange of model components, RAG presents an evolutionary leap in the ﬁne-tuning of language models, catering to the intricate demands of culturally rich and contextually complex datasets.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content=\"In light of these advancements, our research signiﬁes a pivotal moment for LLMs. The evidence suggests that when models are ﬁnely tuned with an awareness of the dataset's inherent context and the corresponding embedding layers, they reach new heights of linguistic precision. Therefore, the path forward for LLMs lies in embracing the modular and contextually aware RAG framework, which promises to reﬁne the capabilities of language models to an unprecedented degree, ensuring the preservation and celebration of the world's diverse linguistic and cultural heritage.\"),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 8}, page_content='ISSN: 2167-1907'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Limitations'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='The results of the “outputted answer” of this algorithm largely reﬂect the quality of the data. If the LLM is trained oﬀ low-quality data, then the answer will reﬂect this bias. The results of the experiment will ﬂuctuate with diﬀerent results should a diﬀerent dataset be used to train the LLM.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='In the future there is a lot of potential to expand on this research by breaking down the RAG algorithm into even more separate components to further see the diﬀerences in average similarity score that adding or removing each component makes.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Bahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP), 4945-4949.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Devlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint,'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Lewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 9459-9474.'),\n",
       " Document(metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 9}, page_content='Radford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. Siriwardhana, S. W. (2023). Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics, 1-17. Vaswani, A. S. (2017). Attention is all you need. Advances in neural information processing systems. Yu, W. (2022). Retrieval-augmented generation across heterogeneous knowledge. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, 52-58.'),\n",
       " Document(metadata={'source': './extracted_images/figure-3-3.jpg', 'type': 'image_or_table'}, page_content='Journal of Student Research, High School Edition'),\n",
       " Document(metadata={'source': './extracted_images/figure-6-8.jpg', 'type': 'image_or_table'}, page_content=\"Flowchart of a system processing user questions.  The system uses several API calls (SerpAPI for Google Search Results, Palm, and ChatGPT) and embedding methods (Palm Embed and OpenAI Embed) to find and process information relevant to the user's query.  It also scrapes a premade Q&A list.  All of this information is fed into a large language model to produce an outputted answer.\\n\"),\n",
       " Document(metadata={'source': './extracted_images/figure-6-7.jpg', 'type': 'image_or_table'}, page_content='Outputted answers combine: Google search results (SerpApi), existing Palm/OpenAI data, and primary data gathered from Amazon inhabitants.'),\n",
       " Document(metadata={'source': './extracted_images/table-7-1.jpg', 'type': 'image_or_table'}, page_content='This table shows similarity scores for 8 different comparisons.  The comparisons vary by whether context was used (yes/no), which LLM was used (GPT/Palm), and which embedding model was used (OpenAI/Palm). Scores range from 0.75 to 0.997. Notably, using Palm embeddings consistently yields higher scores than OpenAI embeddings.')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge two texts\n",
    "combined_content = text_docs + image_docs\n",
    "combined_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb7438",
   "metadata": {},
   "source": [
    "### Store embeddings in chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e3b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb2fcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/argha-ds/datascience/ai-assistant/RAG Use Cases/rag-use-cases-lc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e6bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store Chroma DB\n",
    "persist_directory = \"./chroma_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55e3041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embedded documents\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=combined_content,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3288e",
   "metadata": {},
   "source": [
    "### Create Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482520af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cbc5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135391a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7522/2086921267.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n",
      "/home/argha-ds/datascience/ai-assistant/RAG Use Cases/rag-use-cases-lc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='43218715-be66-41a3-92e4-57363d9fcdd2', metadata={'page_number': 5, 'source': './pdf-docs/rag_llm.pdf', 'type': 'text'}, page_content='Potential Implications for LLMs'),\n",
       " Document(id='c92bd696-571c-4f39-b9c4-0c635e7e2ba3', metadata={'source': './pdf-docs/rag_llm.pdf', 'type': 'text', 'page_number': 5}, page_content='Potential Implications for LLMs'),\n",
       " Document(id='5bcd5e9a-a254-4793-a051-3096343ceb26', metadata={'source': '/tmp/gradio/05ca13cd17065c23018d59941e0a2747f0a67dc09b668272a6eacd28fae7a724/rag_llm.pdf', 'page_number': 5, 'type': 'text'}, page_content='Potential Implications for LLMs'),\n",
       " Document(id='83f71180-a792-4abf-8a7d-e02304687f7c', metadata={'page_number': 5, 'source': '/tmp/gradio/05ca13cd17065c23018d59941e0a2747f0a67dc09b668272a6eacd28fae7a724/rag_llm.pdf', 'type': 'text'}, page_content='Potential Implications for LLMs')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test retriever\n",
    "query = \"What are the Potential Implications for LLMs?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce03d97d",
   "metadata": {},
   "source": [
    "### Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c852b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7ea55ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-pro', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7fbd54b07680>, default_metadata=())"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f507b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "template = \"\"\"You are a helpful assistant for question answering task.\n",
    "your task is to answer the query of the user from the provided context.\n",
    "Your answer should be concise and to the point.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2033a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difine RAG chain\n",
    "chain = ({\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "         | prompt\n",
    "         | llm\n",
    "         | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bdf65c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/argha-ds/datascience/ai-assistant/RAG Use Cases/rag-use-cases-lc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What are the Potential Implications for LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef2c79ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG could evolve into a more nuanced and adaptable framework customizable for specialized datasets and applications across diverse domains (legal, medical, historical, anthropological).'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f7e22d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/argha-ds/datascience/ai-assistant/RAG Use Cases/rag-use-cases-lc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2167-1907'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the ISSN of this document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a381f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-use-cases-lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
