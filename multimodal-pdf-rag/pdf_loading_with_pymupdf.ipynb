{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2f84eb",
   "metadata": {},
   "source": [
    "# Loading Multimodal PDF using PyMuPDF Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d210e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pymupdf\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c921375",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./pdf-docs/rag_llm.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e01ca3",
   "metadata": {},
   "source": [
    "### Extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d30b2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16fcf8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('./pdf-docs/rag_llm.pdf')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f78d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only text\n",
    "docs = []\n",
    "for i, page in enumerate(doc):\n",
    "    text = page.get_text()\n",
    "    if text.strip():\n",
    "        text_doc = Document(\n",
    "            metadata= {\n",
    "                \"page\": i,\n",
    "                \"source\": pdf_path,\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            page_content= text.strip()\n",
    "        )\n",
    "        docs.append(text_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78f7cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'source': './pdf-docs/rag_llm.pdf', 'type': 'text'}, page_content='A Retrieval-Augmented Generation Based Large \\nLanguage Model Benchmarked on a Novel Dataset \\nKieran Pichai \\nMenlo School \\nABSTRACT \\nThe evolution of natural language processing has seen marked advancements, particularly with the advent of models \\nlike BERT, Transformers, and GPT variants, with recent additions like GPT and Bard. This paper investigates the \\nRetrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its \\nconstituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our \\nresearch demonstrates the signiﬁcance of preserving indigenous cultures and biodiversity. The experiment employs a \\ncustomizable RAG methodology, allowing for the interchangeability of various components, such as the base language \\nmodel and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm \\nexhibits superior performance without context. The results also suggest that models tend to perform optimally when \\npaired with similarity scores from their native platforms. Conclusively, our approach showcases the potential of a \\nmodular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to tra-\\nditional ﬁne-tuning of large language models. \\nIntroduction \\nThe evolution of natural language processing models has seen signiﬁcant strides from rule-based approaches in the \\nearly stages of language understanding, eventually leading to the advent of neural networks. However, the full potential \\nof these neural networks awaited the computational infrastructure to catch up. The pivotal moment arrived with the \\nemergence of neural machine translation (NMT), exempliﬁed by Google Translate, which marked a turning point in \\nmachine language comprehension (Bahdanau, 2016). Subsequently, a plethora of advanced models, including BERT, \\nTransformers, GPT-2, and GPT-3, have emerged, driving the ﬁeld forward. Recent notable additions to this landscape \\nare models like GPT and Bard (Devlin, 2018) (Vaswani, 2017) (Radford, 2018). While ﬁne-tuning such models has \\nproven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG) oﬀers a prom-\\nising alternative (Lewis, 2020) (Siriwardhana, 2023) (Yu, 2022). \\nCuriously, little attention has been devoted to dissecting the individual components of RAG and their respec-\\ntive impacts on overall performance. In response to this gap, our paper undertakes a comprehensive investigation of \\nthe RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity \\nand replaceability of its constituent modules. This research seeks to contribute to a deeper understanding of the mech-\\nanisms underlying RAG and its potential for enhancing natural language understanding and generation. These Large \\nLanguage Models (LLMs) exhibit a remarkable proﬁciency in replicating human language styles, achieving a level of \\nlinguistic verisimilitude that borders on the impeccable. In light of these capabilities, it is prudent to delve into the \\nprofound signiﬁcance of the Amazon rainforest, which equates to the importance of any ethnically or racially diverse \\nnation across the globe. Within the vast expanse of the Amazon, an intricate tapestry of life unfolds, where millions \\nof distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the \\nindigenous populations who have made this ecosystem their home. The Amazon rainforest is not only a cradle of \\nbiological diversity but also a sanctuary for an array of religions and cultures, many of which teeter on the brink of \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n1'),\n",
       " Document(metadata={'page': 1, 'source': './pdf-docs/rag_llm.pdf', 'type': 'text'}, page_content='oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice to the indigenous \\ncommunities whose ancestral lands are enshrined within its boundaries. It is a call to safeguard the memories of the \\nland, the traditions that have evolved within its embrace, and the very essence of their cultures. However, certain \\nregions of the Amazon remain shrouded in obscurity, their ﬂora and fauna so rare that reliable and readily available \\ninformation is conspicuously lacking in the vast repository of knowledge available on the internet. In this context, \\nadvanced LLMs play an instrumental role in addressing this deﬁcit by facilitating the dissemination of indigenous \\nnarratives and thereby amplifying awareness and appreciation of the rich tapestry of beliefs, practices, and traditional \\nknowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures \\nwith the global community, emphasizing the paramount importance of preserving the cultural diversity interwoven \\nwithin this vast rainforest. In sum, the overarching mission of this endeavor is twofold: to document and educate the \\nWestern world about hitherto unknown cultures while concurrently ensuring the enduring preservation of these inval-\\nuable facets of human heritage and biodiversity. \\n \\nProposed Experiment \\n \\nBackground and Importance \\n \\nThe intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally rich as the Amazon \\nRainforest, cannot be overstated. This knowledge, passed down through generations, encompasses not only cultural \\nand religious beliefs but also practical insights into the local ﬂora and fauna. As the modern world encroaches on these \\nlands, this wisdom is in peril of being lost forever. Recognizing this, our proposed experiment aims to employ a state-\\nof-the-art Retrieval-Augmented Generation (RAG) framework to capture and leverage this vast, yet vulnerable, \\nknowledge base. \\nOur dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled in its \\ndepth and breadth. It includes detailed discussions on religious practices, cultural nuances, and the integral role of the \\nsurrounding ecosystem in the daily lives of these communities. This data is not just a scientiﬁc or anthropological \\nresource; it is a repository of living history and an urgent call to action for preservation eﬀorts. \\nBy integrating this unique dataset into the RAG framework, we anticipate not only the preservation of \\nknowledge but also the generation of responses that reﬂect the rich tapestry of Amazonian life. The experiment is \\ndesigned to evaluate how diﬀerent components within the RAG setup—such as base language models and similarity \\nscoring algorithms—can be optimized to reﬂect the nuances captured within our dataset. In doing so, we aim to bridge \\nthe gap between advanced language models and the profound human insights found within the Amazon. \\nThe central objective of our experiment is twofold: to analyze the performance implications of modular design \\nwithin the RAG framework and to demonstrate the profound capability of such a system to preserve and communicate \\nthe wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper \\nunderstanding of the data but also allow us to ﬁne-tune the system for optimal performance across diﬀerent conﬁgu-\\nrations. To achieve this, we will systematically explore the interchangeability of various RAG components. We will \\nassess diﬀerent base language models such as GPT and Palm and compare the eﬃcacy of similarity scoring tools from \\ndiverse platforms. The experiment will rigorously test these combinations, identifying which synergies most eﬀectively \\ncapture the essence of the dataset. \\nThe ultimate goal is to showcase the potential of a modular RAG system in processing culturally signiﬁcant \\ninformation, paving the way for future applications that can beneﬁt from such tailored language models. We anticipate \\nthat our ﬁndings will contribute signiﬁcantly to the ﬁelds of computational linguistics and cultural preservation, demon-\\nstrating a novel approach to the application of large language models. \\n \\n \\n \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n2'),\n",
       " Document(metadata={'page': 2, 'source': './pdf-docs/rag_llm.pdf', 'type': 'text'}, page_content=\"Source and Composition \\n \\nOur proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of verbal histories, inter-\\nviews, and ecological insights gathered from the indigenous peoples of the Amazon Rainforest, as well as from biolo-\\ngists and ecologists dedicated to studying this unique biome. The dataset is characterized by its diversity, comprising \\nnarratives that elucidate the intricate relationship between the natives and their environment, including the religious \\nand cultural signiﬁcance of plant and animal life. \\nThe data collection was an extensive process, where linguists and researchers engaged in deep conversations \\nwith the natives, recording their dialects, translating their stories, and documenting their knowledge of the ecological \\nsystem. Similarly, biologists contributed their decades of research on the ﬂora and fauna, providing a scientiﬁc per-\\nspective to the indigenous narratives. The data thus forms a conﬂuence of traditional wisdom and modern scientiﬁc \\nunderstanding, oﬀering a 360-degree view of the Amazon Rainforest's ecosystem. \\n \\nCultural and Environmental Signiﬁcance \\n \\nThe urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a race against time, \\nas globalization and environmental degradation threaten to erase unique cultures and the wisdom they hold. Our dataset \\nserves as a digital ark, a means to preserve and perpetuate the knowledge that has sustained the Amazon's communities \\nfor millennia. \\nThe environmental signiﬁcance of the Amazon Rainforest cannot be overstated—it is a keystone of global \\nbiodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling \\nthe ecological interdependencies that are vital for the rainforest's survival. This dataset, therefore, is not just an aca-\\ndemic or technological asset; it is a critical record for environmental conservationists and policymakers. \\nThrough our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under-\\nstanding of their habitat is unmatched. By integrating their knowledge into the RAG framework, we hope to create a \\nmodel that not only responds with information but also with wisdom that respects the interconnectedness of life and \\nculture. \\n \\nRetrieval-Augmented Generation Framework \\n \\nThe heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophisticated algorithm \\nthat enables the deconstruction of the language model into discrete, interchangeable components. This framework \\nintegrates a retriever model that sources relevant context and a generator model that synthesizes the retrieved infor-\\nmation into coherent responses. \\nIn mathematical terms, given an input query 𝑞𝑞, the retriever model searches a knowledge base 𝒦𝒦 and retrieves \\na set of relevant documents 𝐷𝐷= {𝑑𝑑1, 𝑑𝑑2, … , 𝑑𝑑𝑘𝑘}. Each document 𝑑𝑑 is represented as a vector 𝐯𝐯𝑑𝑑 in a high-dimensional \\nspace, obtained from an embedding layer. This process transforms the raw text data into a structured form amenable \\nto computational manipulation. \\nTo examine the eﬀects of component interchangeability, we adopt various base language models and similar-\\nity scoring mechanisms. For instance, if 𝐸𝐸denotes the embedding function, and 𝑠𝑠 and 𝑡𝑡 represent the input and target \\ntext sequences, respectively, their vector representations would be 𝐯𝐯𝑠𝑠= 𝐸𝐸(𝑠𝑠) and 𝐯𝐯𝑡𝑡= 𝐸𝐸(𝑡𝑡). We employ cosine simi-\\nlarity as the basis for our similarity score, deﬁned by the formula: \\nsimilarity(𝐯𝐯𝑠𝑠, 𝐯𝐯𝑡𝑡) =\\n𝐯𝐯𝑠𝑠 ⋅𝐯𝐯𝑡𝑡\\nห|v𝑠𝑠|ห ห|v𝑡𝑡|ห \\nHere, ⋅ denotes the dot product between the two vectors, and ห|⋅|ห denotes the Euclidean norm. This score quantiﬁes \\nthe closeness of the semantic meaning represented by the vectors, with a value of 1 indicating identical directionality \\nand thus, maximal similarity.  \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n3\"),\n",
       " Document(metadata={'page': 3, 'source': './pdf-docs/rag_llm.pdf', 'type': 'text'}, page_content=\"The experiment tests diﬀerent conﬁgurations by substituting 𝐸𝐸 with embedding functions from various mod-\\nels (e.g., GPT, Palm), allowing us to discern the impact of the embedding layer on the ﬁnal similarity score. By com-\\nparing the performance of diﬀerent 𝐸𝐸 choices, we can identify which embeddings yield the most semantically rich \\nrepresentations for our unique dataset. \\n \\nExperiment Setup \\n \\nThe experiment commences with the training of the language models using our unique dataset. For the training phase, \\nwe deﬁne the following: \\nℒ: The base language model, which can be either GPT or Palm. \\n𝒟𝒟: The training dataset, consisting of pairs (𝑞𝑞𝑖𝑖, 𝑎𝑎𝑖𝑖) where 𝑞𝑞𝑖𝑖 is a query from the dataset and 𝑎𝑎𝑖𝑖 is the corresponding \\nanswer. \\nThe language model ℒ is ﬁne-tuned on 𝒟𝒟, optimizing the weights to minimize the loss function, typically a \\ncross-entropy loss between the predicted and actual answers.  \\nFollowing training, the question-answering process involves feeding a new query 𝑞𝑞′ to the trained model and retrieving \\nthe answer 𝑎𝑎′. This answer is then compared to a predeﬁned list of correct answers using the similarity score, which is \\nfundamental to evaluating the model's performance. \\n \\nBenchmarking and Evaluation \\n \\nThe evaluation metric for our experiment is based on the similarity scores between the generated responses and a set \\nof reference answers. Let 𝐴𝐴= {𝑎𝑎1\\n′ , 𝑎𝑎2\\n′ , … , 𝑎𝑎𝑛𝑛\\n′ } be the set of answers generated by the model, and 𝐴𝐴ref = {𝑎𝑎1\\n∗, 𝑎𝑎2\\n∗, … , 𝑎𝑎𝑛𝑛\\n∗} \\nbe the set of reference answers. We deﬁne the average similarity score as follows: \\nScoreavg = 1\\nn \\u0dcdsimilarity(𝐯𝐯𝑎𝑎𝑖𝑖\\n′, 𝐯𝐯𝑎𝑎𝑖𝑖\\n∗)\\n𝑛𝑛\\n𝑖𝑖=1\\n \\nThis average score acts as the primary benchmark for comparing diﬀerent model conﬁgurations. We systematically \\nrecord the scores across various combinations of language models and similarity scoring mechanisms to assess which \\nconﬁgurations yield the highest average similarity, indicating the most eﬀective model setup for our dataset. \\nAdditionally, we account for the presence or absence of context in the model's training and response genera-\\ntion. This is critical, as the presence of context has been shown to signiﬁcantly inﬂuence model performance, particu-\\nlarly in the domain of indigenous knowledge and biodiversity, where context provides essential background information \\nthat can drastically aﬀect the meaning and relevance of a response. \\nThrough this meticulous experimental setup, we aim to illuminate the intricate dynamics between diﬀerent \\ncomponents of the RAG framework and their collective impact on the model's ability to accurately replicate and convey \\nthe richness of the Amazon Rainforest's cultural and ecological knowledge. \\n \\nPre-Experiment Performance Expectations and Discussion \\n \\nIn the landscape of varying conﬁgurations, we hypothesize that certain setups will yield higher average similarity \\nscores than others, indicative of more nuanced and accurate language generation. Particularly, we expect that: \\nThe similarity scores for models trained with contextual data 𝐯𝐯context will surpass those trained without, due to the \\nenriched understanding and background the model has of the subject matter. \\nWhen aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em-\\nbed), the semantic vector representations (𝐯𝐯𝑠𝑠, 𝐯𝐯𝑡𝑡) should align more closely, thus producing higher similarity scores. \\nThe modular nature of the RAG setup will reveal that certain combinations of base language models and similarity \\nscoring mechanisms are more eﬀective than others, depending on whether context is included or not. \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n4\"),\n",
       " Document(metadata={'page': 4, 'source': './pdf-docs/rag_llm.pdf', 'type': 'text'}, page_content=\"We denote the expected performance increase due to context as ∆context, and the alignment of native embeddings as \\n∆native. Mathematically, we can represent our hypothesis as: \\nScoreavg,context > Scoreavg,no context + ∆context \\nScoreavg,native > Scoreavg,non−native + ∆native \\nThese hypotheses will be tested through a series of experiments, allowing us to determine the optimal model conﬁgu-\\nration for processing and generating responses reﬂective of the Amazon Rainforest dataset. \\n \\nPotential Implications for LLMs \\n \\nThe results of this experiment are expected to have signiﬁcant implications for the development and ﬁne-tuning of \\nLarge Language Models (LLMs). By identifying the most eﬀective conﬁgurations, we can oﬀer insights into the adapt-\\nability of these models to specialized datasets, which is crucial for applications that require a high degree of cultural \\nand contextual sensitivity. \\nMoreover, the experiment is poised to challenge the prevailing approach to LLM training and ﬁne-tuning, \\nwhich often relies on static, one-size-ﬁts-all models. Our ﬁndings could suggest a shift towards a more dynamic, com-\\nponent-based approach, allowing for greater ﬂexibility and precision in model performance across diverse domains. \\nThe potential success of the RAG framework in this context may also pave the way for more granular im-\\nprovements in LLMs, beyond the standard metrics of accuracy and ﬂuency. It may, for instance, enhance the models' \\nability to engage with and preserve less-represented languages and dialects, fostering greater inclusivity and diversity \\nin the realm of natural language processing. \\n \\nImplications for Indigenous Knowledge Preservation \\n \\nThe signiﬁcance of our experiment extends beyond the technical accomplishments within the ﬁeld of natural language \\nprocessing. It serves as a testament to the power of advanced computational techniques in preserving the rich tapestry \\nof human culture, particularly the imperiled knowledge of the Amazon Rainforest's indigenous peoples. By success-\\nfully training a language model to accurately reﬂect and communicate this knowledge, we not only preserve it for future \\ngenerations but also validate the importance of linguistic and cultural diversity in our global ecosystem. \\nThis experiment, should it succeed, will demonstrate a practical application of LLMs in the service of cultural \\npreservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns \\nwith the broader objectives of UNESCO's Intangible Cultural Heritage initiatives. It serves as a model for how com-\\nmunities around the world can leverage technology to protect and share their unique cultural identities and knowledge \\nsystems. \\n \\nAdvancements in RAG Framework \\n \\nFrom a methodological standpoint, our experiment is poised to contribute to the advancement of the RAG framework \\nwithin the realm of AI language models. By dissecting the RAG components and examining their interplay, we will \\ngain insights into the mechanics of modular design in language models, oﬀering a blueprint for future research and \\ndevelopment. \\nThe outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable \\nframework, one that can be customized for specialized datasets and applications. This adaptability is critical as the \\ndemand for LLMs expands into increasingly varied and complex domains, from legal and medical to historical and \\nanthropological. \\nFurthermore, the experiment's focus on modularity could inspire a new wave of research into component-\\nbased architectures for LLMs. Such architectures may provide a more sustainable and eﬃcient pathway to model im-\\nprovement, as opposed to the computationally intensive process of training large models from scratch. \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n5\"),\n",
       " Document(metadata={'page': 5, 'source': './pdf-docs/rag_llm.pdf', 'type': 'text'}, page_content='In conclusion, the proposed experiment holds the potential to make signiﬁcant contributions to both the ﬁeld of AI and \\nthe preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative \\nfuture for LLMs, where the voices of all communities can be heard and understood. \\n \\n \\n \\n \\nFigure 1. Venn Diagram of Data Sources for RAG. This ﬁgure represents a venn diagram of 3 sources of information \\n(google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the “out-\\nputted answer.” \\n \\n \\nFigure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG \\nalgorithm from the input of the “user question” to the “outputted answer of the user question.” \\n \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n6'),\n",
       " Document(metadata={'page': 6, 'source': './pdf-docs/rag_llm.pdf', 'type': 'text'}, page_content='Experimental Results and Discussion \\n \\nThe purpose of this section is to lay down the diﬀerent steps and customizations used within our experiment in order \\nto demonstrate the conclusive results of this experiment to the reader; our experiment using a RAG methodology \\naccurately shows how each component of a LLM positively or negatively aﬀects the accuracy of the outcome itself. \\nIn the initial world of LLM, in order to incrementally increase its performance engineers of these models would have \\nto ﬁne tune them then retrain which took immense amounts of power and large amounts of null results. However, now \\nas they become more and more complex to tune models like OpenAI’s GPT and Google’s Bard have been plateauing \\nperformance wise. \\n \\nTable 1. Experimental Results. This table represents the various diﬀerent combinations of LLM components with \\nrespect to the average similarity score they each produced. \\n \\n \\nContext \\nLLM \\nEmbed for Similarity Score \\n \\nYes \\nNo \\nGPT \\nPalm \\nOpenAI Embedding \\nPalm Embedding \\nScore \\n1 \\n  \\nx \\nx \\n  \\nx \\n  \\n0.75 \\n2 \\nx \\n \\nx \\n \\n \\nx \\n0.92 \\n3 \\nx \\n \\n \\nx \\n \\nx \\n0.93 \\n4 \\n \\nx \\n \\nx \\n \\nx \\n0.88 \\n5 \\nx \\n \\nx \\n \\nx \\n \\n0.997 \\n6 \\nx \\n \\n \\nx \\nx \\n \\n0.996 \\n7 \\n \\nx \\n \\nx \\nx \\n \\n0.91 \\n8 \\n  \\nx \\nx \\n  \\n  \\nx \\n0.897 \\n \\nThis paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to com-\\nponentize the models and break them down into smaller sections. This allows the user to add certain parts / combina-\\ntions to test the performance of those then to substitute diﬀerent modules in to see which leads to the largest perfor-\\nmance increase over the other. These customizable steps allow you to see minute diﬀerences in performance that slowly \\ntuning a model couldn’t have shown you previously. This is a novel way to approach the tuning of LLM and will only \\nserve to increase their accuracy as time moves forward. \\nAnother major component of our RAG methodology is the ability to switch out which embedding layer you \\nuse. The standard embedding (OpenAI) or Palm’s embed. When choosing between both of those some tradeoﬀs are \\nmade. \\nWhen using no context, Palm’s embedding layer seems to perform much better across the board, allowing for \\na much higher average similarity score, however this drastically shifts when given context as now OpenAI’s embedding \\nlayer performs much more soundly. The evidence for these claims is discussed later in this paper. \\nAdditionally, another beneﬁcial feature of the RAG optimization and breakdown style is the ability to cus-\\ntomize which similarity score the LLM uses to decide which answer to base its response oﬀ from the Q-A list. To go \\ninto further detail, the code when prompted with a user question compares the user question to the Q-A list and reorders \\nthe list based oﬀ highest similarity score to lowest, this allows the LLM to select the top 2-3 answers to the highest \\nranked questions and continue generating its own response from there. \\nThe ﬁrst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In terms \\nof the data when GPT (for the purposes of precision all of the following results include context) and STS OpenAI were \\ncombined, you got an average similarity score of 0.997. If you instead pair this with Palm STS Score instead, the \\naverage score drops to 0.92, a 0.077 decrease in performance. A similar eﬀect when using Palm with Palm STS and \\nPalm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n7'),\n",
       " Document(metadata={'page': 7, 'source': './pdf-docs/rag_llm.pdf', 'type': 'text'}, page_content=\"to reach very high accuracy levels when paired with a similarity score calculated from the same program (this means \\nGPT worked better with OpenAI STS Score and that Palm worked better with Palm STS Score). What is also interest-\\ning to note is that although Palm produced a 0.001 lower performance than GPT it seemed to be more ﬂexible, working \\nbetter with its competitor (OpenAI STS Score + Palm produced 0.93) than the GPT with its competitor (Palm STS \\nScore + GPT produced 0.92). \\nTo recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap or \\nreplace things like the embedding layer used, the similarity score used, and the base language model used, also whether \\nit was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM \\nand the data collected speaks a lot to the importance of each aspect of an LLM. \\nIn terms of expected results, two things were noticed, ﬁrst that Palm had a slightly lower performance than \\nGPT (0.996 versus 0.997 respectively) on their top runs, however, there was also some contradictory data as it seemed \\nthat Palm worked signiﬁcantly better when given no context to work with compared to GPT, Palm produced an average \\nscore of 0.88 and 0.91 when given no context while GPT produced an average score of 0.75 and 0.897. Although GPT \\nmay perform much better when given context, Palm seems to beat it out just given its own proprietary dataset (no \\ncontext). \\nSome unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, pair-\\ning Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually \\nperformed better when paired with OpenAI’s embed. 0.91 (with OpenAI embed) versus 0.88 (Palm embed)—note that \\nthis is without context given. A similar eﬀect was noticed going the other way around as well, without context, GPT \\nperformed much better with Palm embedding layer than with its own OpenAI embedding layer (0.897 versus 0.75 \\nrespectively). This data shows how Palm embedding layer tends to perform much better given no context when com-\\npared to OpenAI’s embed. Similarly, to explored above, it is the opposite when given context, however. OpenAI’s \\nembedding layer performs a bit better when given context across the board than Palm embed. \\nMost notably from this experiment was two realizations. First, that Palm Embedding layer tends to work much \\nbetter when just given its own proprietary dataset (and no context), when compared to OpenAI’s embed. Additionally, \\nwhen given context, the playing ﬁeld switches: Palm tends to perform much worse when given context when compared \\nto OpenAI. Lastly, it is important to note that a combination of Palm/GPT with OpenAI’s embedding layer and context \\nyielded extremely accurate results when its similarity scores were averaged. \\n \\nConclusion \\n \\nBuilding upon the foundation laid by our initial ﬁndings, it is paramount to recognize the exceptional performance of \\nthe Palm model when utilizing its proprietary dataset in conjunction with the Palm embed. This speciﬁcity in data and \\ntechnology synchronization has shown that Palm outshines OpenAI in terms of model accuracy in a context-free envi-\\nronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT \\nwith its native OpenAI embedding layer excels, leveraging the additional context to produce responses of remarkable \\naccuracy that resonate with the cultural and ecological nuances of the Amazon. \\nThis pivot in performance based on context underscores the signiﬁcance of tailored datasets and embedding \\nmechanisms in the optimization of Large Language Models (LLMs). The adaptability of the Retrieval-Augmented \\nGeneration (RAG) framework emerges as a cornerstone for future enhancements in LLMs. By enabling the seamless \\ninterchange of model components, RAG presents an evolutionary leap in the ﬁne-tuning of language models, catering \\nto the intricate demands of culturally rich and contextually complex datasets. \\nIn light of these advancements, our research signiﬁes a pivotal moment for LLMs. The evidence suggests that \\nwhen models are ﬁnely tuned with an awareness of the dataset's inherent context and the corresponding embedding \\nlayers, they reach new heights of linguistic precision. Therefore, the path forward for LLMs lies in embracing the \\nmodular and contextually aware RAG framework, which promises to reﬁne the capabilities of language models to an \\nunprecedented degree, ensuring the preservation and celebration of the world's diverse linguistic and cultural heritage. \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n8\"),\n",
       " Document(metadata={'page': 8, 'source': './pdf-docs/rag_llm.pdf', 'type': 'text'}, page_content='Limitations \\n \\nThe results of the “outputted answer” of this algorithm largely reﬂect the quality of the data. If the LLM is trained oﬀ \\nlow-quality data, then the answer will reﬂect this bias. The results of the experiment will ﬂuctuate with diﬀerent results \\nshould a diﬀerent dataset be used to train the LLM.  \\nIn the future there is a lot of potential to expand on this research by breaking down the RAG algorithm into \\neven more separate components to further see the diﬀerences in average similarity score that adding or removing each \\ncomponent makes. \\n \\nReferences \\n \\nBahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE international \\nconference on acoustics, speech and signal processing (ICASSP), 4945-4949. \\nDevlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint, \\narXiv:1810.04805. \\nLewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural \\nInformation Processing Systems, 9459-9474. \\nRadford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. \\nSiriwardhana, S. W. (2023). Improving the domain adaptation of retrieval augmented generation (RAG) models for \\nopen domain question answering. Transactions of the Association for Computational Linguistics, 1-17. \\nVaswani, A. S. (2017). Attention is all you need. Advances in neural information processing systems. \\nYu, W. (2022). Retrieval-augmented generation across heterogeneous knowledge. Proceedings of the 2022 \\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language \\nTechnologies: Student Research Workshop, 52-58. \\n \\n \\n \\n \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n9')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bc47cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text and OCR for images\n",
    "docs = []\n",
    "for i, page in enumerate(doc):\n",
    "    tp = page.get_textpage_ocr()\n",
    "    text = page.get_text(textpage=tp)\n",
    "    if text.strip():\n",
    "        text_doc = Document(\n",
    "            metadata= {\n",
    "                \"page\": i,\n",
    "                \"source\": pdf_path,\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            page_content= text.strip()\n",
    "        )\n",
    "        docs.append(text_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7318d54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In conclusion, the proposed experiment holds the potential to make significant contributions to both the field of AI and \n",
      "the preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative \n",
      "future for LLMs, where the voices of all communities can be heard and understood. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Figure 1. Venn Diagram of Data Sources for RAG. This figure represents a venn diagram of 3 sources of information \n",
      "(google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the “out-\n",
      "putted answer.” \n",
      " \n",
      " \n",
      "Figure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG \n",
      "algorithm from the input of the “user question” to the “outputted answer of the user question.” \n",
      " \n",
      "Volume 12 Issue 4 (2023) \n",
      "ISSN: 2167-1907\n",
      "www.JSR.org/hs\n",
      "6\n",
      "=\n",
      "Diferent\n",
      "AP! Calls\n",
      "nat@PT\n",
      "SepAP Google\n",
      "‘Search Results)\n",
      "Scrape tough a premade\n",
      "lst\n",
      "nd whch moet\n",
      "close she user question\n",
      "pamembed |\n",
      "open mbed\n",
      "Large Language\n",
      "Model\n",
      "Outpted Answer\n",
      "HIGH SCHOOL EDITION\n",
      "oe Journal of Student Research\n"
     ]
    }
   ],
   "source": [
    "print(docs[5].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273c2abd",
   "metadata": {},
   "source": [
    "### Extract Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf86a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, page in enumerate(doc):\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            # Create unique identifier\n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "            img_path = f\"./extracted_images/{image_id}.png\"\n",
    "            # Save the image\n",
    "            pil_image.save(img_path, format=\"PNG\")\n",
    "        except e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0e9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-use-cases-lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
