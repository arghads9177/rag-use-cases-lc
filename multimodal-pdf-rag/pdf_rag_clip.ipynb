{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0319d99",
   "metadata": {},
   "source": [
    "# Multimodal RAG Using CLIP Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8210516c",
   "metadata": {},
   "source": [
    "### Data Ingetion\n",
    "\n",
    "1. Extract text and images from PDF\n",
    "2. Split the text data into chunks and perform text embedding useing CLIP\n",
    "3. Perform image embedding using CLIP\n",
    "4. Store the embeddings in chroma vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "489ca874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pymupdf\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1944f4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Emport Environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a1cdd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Initialize CLIP model for unified embeddings\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f3844c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate CLIP Model\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea6e85d",
   "metadata": {},
   "source": [
    "#### Embedding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0bd339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image embedding function\n",
    "def embed_image(image_data):\n",
    "    \"\"\"Embed image using CLIP\"\"\"\n",
    "    if isinstance(image_data, str):  # If path\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else:  # If PIL Image\n",
    "        image = image_data\n",
    "    \n",
    "    inputs=clip_processor(images=image,return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        # Normalize embeddings to unit vector\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "# Define text embedding function    \n",
    "def embed_text(text):\n",
    "    \"\"\"Embed text using CLIP.\"\"\"\n",
    "    inputs = clip_processor(\n",
    "        text=text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77  # CLIP's max token length\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        # Normalize embeddings\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcbd347",
   "metadata": {},
   "source": [
    "#### Load and Process PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9663476",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./pdf-docs/rag_llm.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23de57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ea67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for all documents and embeddings\n",
    "all_docs = []\n",
    "all_embeddings = []\n",
    "image_data_store = {}  # Store actual image data for LLM\n",
    "\n",
    "# Text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e236e6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('./pdf-docs/rag_llm.pdf')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "385aa234",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,page in enumerate(doc):\n",
    "    ## process text\n",
    "    text=page.get_text()\n",
    "    if text.strip():\n",
    "        ##create temporary document for splitting\n",
    "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
    "        text_chunks = splitter.split_documents([temp_doc])\n",
    "\n",
    "        #Embed each chunk using CLIP\n",
    "        for chunk in text_chunks:\n",
    "            embedding = embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "\n",
    "\n",
    "\n",
    "    ## process images\n",
    "    ##Three Important Actions:\n",
    "\n",
    "    ##Convert PDF image to PIL format\n",
    "    ##Store as base64 for GPT-4V (which needs base64 images)\n",
    "    ##Create CLIP embedding for retrieval\n",
    "\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            \n",
    "            # Create unique identifier\n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "            \n",
    "            # Store image as base64 for later use with GPT-4V\n",
    "            buffered = io.BytesIO()\n",
    "            pil_image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = img_base64\n",
    "            \n",
    "            # Embed image using CLIP\n",
    "            embedding = embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "            \n",
    "            # Create document for image\n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )\n",
    "            all_docs.append(image_doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d3772fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'type': 'text'}, page_content='A Retrieval-Augmented Generation Based Large \\nLanguage Model Benchmarked on a Novel Dataset \\nKieran Pichai \\nMenlo School \\nABSTRACT \\nThe evolution of natural language processing has seen marked advancements, particularly with the advent of models \\nlike BERT, Transformers, and GPT variants, with recent additions like GPT and Bard. This paper investigates the \\nRetrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='constituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our \\nresearch demonstrates the signiÔ¨Åcance of preserving indigenous cultures and biodiversity. The experiment employs a \\ncustomizable RAG methodology, allowing for the interchangeability of various components, such as the base language \\nmodel and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='exhibits superior performance without context. The results also suggest that models tend to perform optimally when \\npaired with similarity scores from their native platforms. Conclusively, our approach showcases the potential of a \\nmodular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to tra-\\nditional Ô¨Åne-tuning of large language models. \\nIntroduction'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='ditional Ô¨Åne-tuning of large language models. \\nIntroduction \\nThe evolution of natural language processing models has seen signiÔ¨Åcant strides from rule-based approaches in the \\nearly stages of language understanding, eventually leading to the advent of neural networks. However, the full potential \\nof these neural networks awaited the computational infrastructure to catch up. The pivotal moment arrived with the'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='emergence of neural machine translation (NMT), exempliÔ¨Åed by Google Translate, which marked a turning point in \\nmachine language comprehension (Bahdanau, 2016). Subsequently, a plethora of advanced models, including BERT, \\nTransformers, GPT-2, and GPT-3, have emerged, driving the Ô¨Åeld forward. Recent notable additions to this landscape \\nare models like GPT and Bard (Devlin, 2018) (Vaswani, 2017) (Radford, 2018). While Ô¨Åne-tuning such models has'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='proven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG) oÔ¨Äers a prom-\\nising alternative (Lewis, 2020) (Siriwardhana, 2023) (Yu, 2022). \\nCuriously, little attention has been devoted to dissecting the individual components of RAG and their respec-\\ntive impacts on overall performance. In response to this gap, our paper undertakes a comprehensive investigation of'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='the RAG framework and embarks on the design of RAG models from the ground up, with a focus on the modularity \\nand replaceability of its constituent modules. This research seeks to contribute to a deeper understanding of the mech-\\nanisms underlying RAG and its potential for enhancing natural language understanding and generation. These Large \\nLanguage Models (LLMs) exhibit a remarkable proÔ¨Åciency in replicating human language styles, achieving a level of'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='linguistic verisimilitude that borders on the impeccable. In light of these capabilities, it is prudent to delve into the \\nprofound signiÔ¨Åcance of the Amazon rainforest, which equates to the importance of any ethnically or racially diverse \\nnation across the globe. Within the vast expanse of the Amazon, an intricate tapestry of life unfolds, where millions \\nof distinct species intermingle. Each of these species, as rare as the other, holds a unique and intrinsic value to the'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='indigenous populations who have made this ecosystem their home. The Amazon rainforest is not only a cradle of \\nbiological diversity but also a sanctuary for an array of religions and cultures, many of which teeter on the brink of \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n1'),\n",
       " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='oblivion. Preserving the Amazon is not merely an environmental imperative; it is an act of justice to the indigenous \\ncommunities whose ancestral lands are enshrined within its boundaries. It is a call to safeguard the memories of the \\nland, the traditions that have evolved within its embrace, and the very essence of their cultures. However, certain \\nregions of the Amazon remain shrouded in obscurity, their Ô¨Çora and fauna so rare that reliable and readily available'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='information is conspicuously lacking in the vast repository of knowledge available on the internet. In this context, \\nadvanced LLMs play an instrumental role in addressing this deÔ¨Åcit by facilitating the dissemination of indigenous \\nnarratives and thereby amplifying awareness and appreciation of the rich tapestry of beliefs, practices, and traditional \\nknowledge that these communities hold dear. They serve as a bridge connecting the indigenous Amazonian cultures'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='with the global community, emphasizing the paramount importance of preserving the cultural diversity interwoven \\nwithin this vast rainforest. In sum, the overarching mission of this endeavor is twofold: to document and educate the \\nWestern world about hitherto unknown cultures while concurrently ensuring the enduring preservation of these inval-\\nuable facets of human heritage and biodiversity. \\n \\nProposed Experiment \\n \\nBackground and Importance'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='Proposed Experiment \\n \\nBackground and Importance \\n \\nThe intrinsic value of indigenous knowledge, especially from regions as biodiverse and culturally rich as the Amazon \\nRainforest, cannot be overstated. This knowledge, passed down through generations, encompasses not only cultural \\nand religious beliefs but also practical insights into the local Ô¨Çora and fauna. As the modern world encroaches on these'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='lands, this wisdom is in peril of being lost forever. Recognizing this, our proposed experiment aims to employ a state-\\nof-the-art Retrieval-Augmented Generation (RAG) framework to capture and leverage this vast, yet vulnerable, \\nknowledge base. \\nOur dataset, derived from interviews with Amazon Rainforest natives and biologists, is unparalleled in its \\ndepth and breadth. It includes detailed discussions on religious practices, cultural nuances, and the integral role of the'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='surrounding ecosystem in the daily lives of these communities. This data is not just a scientiÔ¨Åc or anthropological \\nresource; it is a repository of living history and an urgent call to action for preservation eÔ¨Äorts. \\nBy integrating this unique dataset into the RAG framework, we anticipate not only the preservation of \\nknowledge but also the generation of responses that reÔ¨Çect the rich tapestry of Amazonian life. The experiment is'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='designed to evaluate how diÔ¨Äerent components within the RAG setup‚Äîsuch as base language models and similarity \\nscoring algorithms‚Äîcan be optimized to reÔ¨Çect the nuances captured within our dataset. In doing so, we aim to bridge \\nthe gap between advanced language models and the profound human insights found within the Amazon. \\nThe central objective of our experiment is twofold: to analyze the performance implications of modular design'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='within the RAG framework and to demonstrate the profound capability of such a system to preserve and communicate \\nthe wealth of indigenous knowledge. We hypothesize that a customizable RAG model will not only facilitate a deeper \\nunderstanding of the data but also allow us to Ô¨Åne-tune the system for optimal performance across diÔ¨Äerent conÔ¨Ågu-\\nrations. To achieve this, we will systematically explore the interchangeability of various RAG components. We will'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='assess diÔ¨Äerent base language models such as GPT and Palm and compare the eÔ¨Écacy of similarity scoring tools from \\ndiverse platforms. The experiment will rigorously test these combinations, identifying which synergies most eÔ¨Äectively \\ncapture the essence of the dataset. \\nThe ultimate goal is to showcase the potential of a modular RAG system in processing culturally signiÔ¨Åcant \\ninformation, paving the way for future applications that can beneÔ¨Åt from such tailored language models. We anticipate'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='that our Ô¨Åndings will contribute signiÔ¨Åcantly to the Ô¨Åelds of computational linguistics and cultural preservation, demon-\\nstrating a novel approach to the application of large language models. \\n \\n \\n \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n2'),\n",
       " Document(metadata={'page': 1, 'type': 'image', 'image_id': 'page_1_img_0'}, page_content='[Image: page_1_img_0]'),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content='Source and Composition \\n \\nOur proprietary dataset stands as the cornerstone of this experiment. It is a rich compendium of verbal histories, inter-\\nviews, and ecological insights gathered from the indigenous peoples of the Amazon Rainforest, as well as from biolo-\\ngists and ecologists dedicated to studying this unique biome. The dataset is characterized by its diversity, comprising'),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content='narratives that elucidate the intricate relationship between the natives and their environment, including the religious \\nand cultural signiÔ¨Åcance of plant and animal life. \\nThe data collection was an extensive process, where linguists and researchers engaged in deep conversations \\nwith the natives, recording their dialects, translating their stories, and documenting their knowledge of the ecological'),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content=\"system. Similarly, biologists contributed their decades of research on the Ô¨Çora and fauna, providing a scientiÔ¨Åc per-\\nspective to the indigenous narratives. The data thus forms a conÔ¨Çuence of traditional wisdom and modern scientiÔ¨Åc \\nunderstanding, oÔ¨Äering a 360-degree view of the Amazon Rainforest's ecosystem. \\n \\nCultural and Environmental SigniÔ¨Åcance \\n \\nThe urgency of preserving indigenous knowledge is akin to conserving an endangered species. It is a race against time,\"),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content=\"as globalization and environmental degradation threaten to erase unique cultures and the wisdom they hold. Our dataset \\nserves as a digital ark, a means to preserve and perpetuate the knowledge that has sustained the Amazon's communities \\nfor millennia. \\nThe environmental signiÔ¨Åcance of the Amazon Rainforest cannot be overstated‚Äîit is a keystone of global \\nbiodiversity. By documenting the intricate knowledge, the natives have of their environment, we are also chronicling\"),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content=\"the ecological interdependencies that are vital for the rainforest's survival. This dataset, therefore, is not just an aca-\\ndemic or technological asset; it is a critical record for environmental conservationists and policymakers. \\nThrough our experiment, we aim to amplify the voices of the Amazon's indigenous peoples, whose under-\\nstanding of their habitat is unmatched. By integrating their knowledge into the RAG framework, we hope to create a\"),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content='model that not only responds with information but also with wisdom that respects the interconnectedness of life and \\nculture. \\n \\nRetrieval-Augmented Generation Framework \\n \\nThe heart of our experiment lies in the Retrieval-Augmented Generation (RAG) framework, a sophisticated algorithm \\nthat enables the deconstruction of the language model into discrete, interchangeable components. This framework'),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content='integrates a retriever model that sources relevant context and a generator model that synthesizes the retrieved infor-\\nmation into coherent responses. \\nIn mathematical terms, given an input query ùëûùëû, the retriever model searches a knowledge base ùí¶ùí¶ and retrieves \\na set of relevant documents ùê∑ùê∑= {ùëëùëë1, ùëëùëë2, ‚Ä¶ , ùëëùëëùëòùëò}. Each document ùëëùëë is represented as a vector ùêØùêØùëëùëë in a high-dimensional'),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content='space, obtained from an embedding layer. This process transforms the raw text data into a structured form amenable \\nto computational manipulation. \\nTo examine the eÔ¨Äects of component interchangeability, we adopt various base language models and similar-\\nity scoring mechanisms. For instance, if ùê∏ùê∏denotes the embedding function, and ùë†ùë† and ùë°ùë° represent the input and target \\ntext sequences, respectively, their vector representations would be ùêØùêØùë†ùë†= ùê∏ùê∏(ùë†ùë†) and ùêØùêØùë°ùë°= ùê∏ùê∏(ùë°ùë°). We employ cosine simi-'),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content='larity as the basis for our similarity score, deÔ¨Åned by the formula: \\nsimilarity(ùêØùêØùë†ùë†, ùêØùêØùë°ùë°) =\\nùêØùêØùë†ùë† ‚ãÖùêØùêØùë°ùë°\\n‡∏´|vùë†ùë†|‡∏´ ‡∏´|vùë°ùë°|‡∏´ \\nHere, ‚ãÖ denotes the dot product between the two vectors, and ‡∏´|‚ãÖ|‡∏´ denotes the Euclidean norm. This score quantiÔ¨Åes \\nthe closeness of the semantic meaning represented by the vectors, with a value of 1 indicating identical directionality \\nand thus, maximal similarity.  \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n3'),\n",
       " Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_0'}, page_content='[Image: page_2_img_0]'),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content='The experiment tests diÔ¨Äerent conÔ¨Ågurations by substituting ùê∏ùê∏ with embedding functions from various mod-\\nels (e.g., GPT, Palm), allowing us to discern the impact of the embedding layer on the Ô¨Ånal similarity score. By com-\\nparing the performance of diÔ¨Äerent ùê∏ùê∏ choices, we can identify which embeddings yield the most semantically rich \\nrepresentations for our unique dataset. \\n \\nExperiment Setup'),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content='representations for our unique dataset. \\n \\nExperiment Setup \\n \\nThe experiment commences with the training of the language models using our unique dataset. For the training phase, \\nwe deÔ¨Åne the following: \\n‚Ñí: The base language model, which can be either GPT or Palm. \\nùíüùíü: The training dataset, consisting of pairs (ùëûùëûùëñùëñ, ùëéùëéùëñùëñ) where ùëûùëûùëñùëñ is a query from the dataset and ùëéùëéùëñùëñ is the corresponding \\nanswer.'),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content=\"answer. \\nThe language model ‚Ñí is Ô¨Åne-tuned on ùíüùíü, optimizing the weights to minimize the loss function, typically a \\ncross-entropy loss between the predicted and actual answers.  \\nFollowing training, the question-answering process involves feeding a new query ùëûùëû‚Ä≤ to the trained model and retrieving \\nthe answer ùëéùëé‚Ä≤. This answer is then compared to a predeÔ¨Åned list of correct answers using the similarity score, which is \\nfundamental to evaluating the model's performance.\"),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content=\"fundamental to evaluating the model's performance. \\n \\nBenchmarking and Evaluation \\n \\nThe evaluation metric for our experiment is based on the similarity scores between the generated responses and a set \\nof reference answers. Let ùê¥ùê¥= {ùëéùëé1\\n‚Ä≤ , ùëéùëé2\\n‚Ä≤ , ‚Ä¶ , ùëéùëéùëõùëõ\\n‚Ä≤ } be the set of answers generated by the model, and ùê¥ùê¥ref = {ùëéùëé1\\n‚àó, ùëéùëé2\\n‚àó, ‚Ä¶ , ùëéùëéùëõùëõ\\n‚àó} \\nbe the set of reference answers. We deÔ¨Åne the average similarity score as follows: \\nScoreavg = 1\\nn \\u0dcdsimilarity(ùêØùêØùëéùëéùëñùëñ\\n‚Ä≤, ùêØùêØùëéùëéùëñùëñ\\n‚àó)\\nùëõùëõ\\nùëñùëñ=1\"),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content='Scoreavg = 1\\nn \\u0dcdsimilarity(ùêØùêØùëéùëéùëñùëñ\\n‚Ä≤, ùêØùêØùëéùëéùëñùëñ\\n‚àó)\\nùëõùëõ\\nùëñùëñ=1\\n \\nThis average score acts as the primary benchmark for comparing diÔ¨Äerent model conÔ¨Ågurations. We systematically \\nrecord the scores across various combinations of language models and similarity scoring mechanisms to assess which \\nconÔ¨Ågurations yield the highest average similarity, indicating the most eÔ¨Äective model setup for our dataset.'),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content=\"Additionally, we account for the presence or absence of context in the model's training and response genera-\\ntion. This is critical, as the presence of context has been shown to signiÔ¨Åcantly inÔ¨Çuence model performance, particu-\\nlarly in the domain of indigenous knowledge and biodiversity, where context provides essential background information \\nthat can drastically aÔ¨Äect the meaning and relevance of a response.\"),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content=\"that can drastically aÔ¨Äect the meaning and relevance of a response. \\nThrough this meticulous experimental setup, we aim to illuminate the intricate dynamics between diÔ¨Äerent \\ncomponents of the RAG framework and their collective impact on the model's ability to accurately replicate and convey \\nthe richness of the Amazon Rainforest's cultural and ecological knowledge. \\n \\nPre-Experiment Performance Expectations and Discussion\"),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content='Pre-Experiment Performance Expectations and Discussion \\n \\nIn the landscape of varying conÔ¨Ågurations, we hypothesize that certain setups will yield higher average similarity \\nscores than others, indicative of more nuanced and accurate language generation. Particularly, we expect that: \\nThe similarity scores for models trained with contextual data ùêØùêØcontext will surpass those trained without, due to the \\nenriched understanding and background the model has of the subject matter.'),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content='enriched understanding and background the model has of the subject matter. \\nWhen aligning models with their native embeddings (e.g., GPT with OpenAI Embed, Palm with Palm Em-\\nbed), the semantic vector representations (ùêØùêØùë†ùë†, ùêØùêØùë°ùë°) should align more closely, thus producing higher similarity scores. \\nThe modular nature of the RAG setup will reveal that certain combinations of base language models and similarity'),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content='scoring mechanisms are more eÔ¨Äective than others, depending on whether context is included or not. \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n4'),\n",
       " Document(metadata={'page': 3, 'type': 'image', 'image_id': 'page_3_img_0'}, page_content='[Image: page_3_img_0]'),\n",
       " Document(metadata={'page': 4, 'type': 'text'}, page_content='We denote the expected performance increase due to context as ‚àÜcontext, and the alignment of native embeddings as \\n‚àÜnative. Mathematically, we can represent our hypothesis as: \\nScoreavg,context > Scoreavg,no context + ‚àÜcontext \\nScoreavg,native > Scoreavg,non‚àínative + ‚àÜnative \\nThese hypotheses will be tested through a series of experiments, allowing us to determine the optimal model conÔ¨Ågu-\\nration for processing and generating responses reÔ¨Çective of the Amazon Rainforest dataset.'),\n",
       " Document(metadata={'page': 4, 'type': 'text'}, page_content='ration for processing and generating responses reÔ¨Çective of the Amazon Rainforest dataset. \\n \\nPotential Implications for LLMs \\n \\nThe results of this experiment are expected to have signiÔ¨Åcant implications for the development and Ô¨Åne-tuning of \\nLarge Language Models (LLMs). By identifying the most eÔ¨Äective conÔ¨Ågurations, we can oÔ¨Äer insights into the adapt-\\nability of these models to specialized datasets, which is crucial for applications that require a high degree of cultural'),\n",
       " Document(metadata={'page': 4, 'type': 'text'}, page_content='and contextual sensitivity. \\nMoreover, the experiment is poised to challenge the prevailing approach to LLM training and Ô¨Åne-tuning, \\nwhich often relies on static, one-size-Ô¨Åts-all models. Our Ô¨Åndings could suggest a shift towards a more dynamic, com-\\nponent-based approach, allowing for greater Ô¨Çexibility and precision in model performance across diverse domains. \\nThe potential success of the RAG framework in this context may also pave the way for more granular im-'),\n",
       " Document(metadata={'page': 4, 'type': 'text'}, page_content=\"provements in LLMs, beyond the standard metrics of accuracy and Ô¨Çuency. It may, for instance, enhance the models' \\nability to engage with and preserve less-represented languages and dialects, fostering greater inclusivity and diversity \\nin the realm of natural language processing. \\n \\nImplications for Indigenous Knowledge Preservation \\n \\nThe signiÔ¨Åcance of our experiment extends beyond the technical accomplishments within the Ô¨Åeld of natural language\"),\n",
       " Document(metadata={'page': 4, 'type': 'text'}, page_content=\"processing. It serves as a testament to the power of advanced computational techniques in preserving the rich tapestry \\nof human culture, particularly the imperiled knowledge of the Amazon Rainforest's indigenous peoples. By success-\\nfully training a language model to accurately reÔ¨Çect and communicate this knowledge, we not only preserve it for future \\ngenerations but also validate the importance of linguistic and cultural diversity in our global ecosystem.\"),\n",
       " Document(metadata={'page': 4, 'type': 'text'}, page_content=\"This experiment, should it succeed, will demonstrate a practical application of LLMs in the service of cultural \\npreservation. It emphasizes the role that technology can play in safeguarding intangible heritage, a mission that aligns \\nwith the broader objectives of UNESCO's Intangible Cultural Heritage initiatives. It serves as a model for how com-\\nmunities around the world can leverage technology to protect and share their unique cultural identities and knowledge \\nsystems.\"),\n",
       " Document(metadata={'page': 4, 'type': 'text'}, page_content='systems. \\n \\nAdvancements in RAG Framework \\n \\nFrom a methodological standpoint, our experiment is poised to contribute to the advancement of the RAG framework \\nwithin the realm of AI language models. By dissecting the RAG components and examining their interplay, we will \\ngain insights into the mechanics of modular design in language models, oÔ¨Äering a blueprint for future research and \\ndevelopment.'),\n",
       " Document(metadata={'page': 4, 'type': 'text'}, page_content=\"development. \\nThe outcomes of this experiment could lead to the evolution of RAG into a more nuanced and adaptable \\nframework, one that can be customized for specialized datasets and applications. This adaptability is critical as the \\ndemand for LLMs expands into increasingly varied and complex domains, from legal and medical to historical and \\nanthropological. \\nFurthermore, the experiment's focus on modularity could inspire a new wave of research into component-\"),\n",
       " Document(metadata={'page': 4, 'type': 'text'}, page_content='based architectures for LLMs. Such architectures may provide a more sustainable and eÔ¨Écient pathway to model im-\\nprovement, as opposed to the computationally intensive process of training large models from scratch. \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n5'),\n",
       " Document(metadata={'page': 4, 'type': 'image', 'image_id': 'page_4_img_0'}, page_content='[Image: page_4_img_0]'),\n",
       " Document(metadata={'page': 5, 'type': 'text'}, page_content='In conclusion, the proposed experiment holds the potential to make signiÔ¨Åcant contributions to both the Ô¨Åeld of AI and \\nthe preservation of human cultural heritage. The insights gained could lead to a more inclusive and representative \\nfuture for LLMs, where the voices of all communities can be heard and understood. \\n \\n \\n \\n \\nFigure 1. Venn Diagram of Data Sources for RAG. This Ô¨Ågure represents a venn diagram of 3 sources of information'),\n",
       " Document(metadata={'page': 5, 'type': 'text'}, page_content='(google search results, OpenAI/Palm, proprietary data collected by the author) combined in order to create the ‚Äúout-\\nputted answer.‚Äù \\n \\n \\nFigure 2. Executive Diagram of Proposed RAG. This diagram outlines the various steps and procedures of the RAG \\nalgorithm from the input of the ‚Äúuser question‚Äù to the ‚Äúoutputted answer of the user question.‚Äù \\n \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n6'),\n",
       " Document(metadata={'page': 5, 'type': 'image', 'image_id': 'page_5_img_0'}, page_content='[Image: page_5_img_0]'),\n",
       " Document(metadata={'page': 5, 'type': 'image', 'image_id': 'page_5_img_1'}, page_content='[Image: page_5_img_1]'),\n",
       " Document(metadata={'page': 5, 'type': 'image', 'image_id': 'page_5_img_2'}, page_content='[Image: page_5_img_2]'),\n",
       " Document(metadata={'page': 6, 'type': 'text'}, page_content='Experimental Results and Discussion \\n \\nThe purpose of this section is to lay down the diÔ¨Äerent steps and customizations used within our experiment in order \\nto demonstrate the conclusive results of this experiment to the reader; our experiment using a RAG methodology \\naccurately shows how each component of a LLM positively or negatively aÔ¨Äects the accuracy of the outcome itself.'),\n",
       " Document(metadata={'page': 6, 'type': 'text'}, page_content='In the initial world of LLM, in order to incrementally increase its performance engineers of these models would have \\nto Ô¨Åne tune them then retrain which took immense amounts of power and large amounts of null results. However, now \\nas they become more and more complex to tune models like OpenAI‚Äôs GPT and Google‚Äôs Bard have been plateauing \\nperformance wise. \\n \\nTable 1. Experimental Results. This table represents the various diÔ¨Äerent combinations of LLM components with'),\n",
       " Document(metadata={'page': 6, 'type': 'text'}, page_content='respect to the average similarity score they each produced. \\n \\n \\nContext \\nLLM \\nEmbed for Similarity Score \\n \\nYes \\nNo \\nGPT \\nPalm \\nOpenAI Embedding \\nPalm Embedding \\nScore \\n1 \\n  \\nx \\nx \\n  \\nx \\n  \\n0.75 \\n2 \\nx \\n \\nx \\n \\n \\nx \\n0.92 \\n3 \\nx \\n \\n \\nx \\n \\nx \\n0.93 \\n4 \\n \\nx \\n \\nx \\n \\nx \\n0.88 \\n5 \\nx \\n \\nx \\n \\nx \\n \\n0.997 \\n6 \\nx \\n \\n \\nx \\nx \\n \\n0.996 \\n7 \\n \\nx \\n \\nx \\nx \\n \\n0.91 \\n8 \\n  \\nx \\nx \\n  \\n  \\nx \\n0.897 \\n \\nThis paper produces a new solution to the slowing improvement of LLM in the form of RAG, a way to com-'),\n",
       " Document(metadata={'page': 6, 'type': 'text'}, page_content='ponentize the models and break them down into smaller sections. This allows the user to add certain parts / combina-\\ntions to test the performance of those then to substitute diÔ¨Äerent modules in to see which leads to the largest perfor-\\nmance increase over the other. These customizable steps allow you to see minute diÔ¨Äerences in performance that slowly \\ntuning a model couldn‚Äôt have shown you previously. This is a novel way to approach the tuning of LLM and will only'),\n",
       " Document(metadata={'page': 6, 'type': 'text'}, page_content='serve to increase their accuracy as time moves forward. \\nAnother major component of our RAG methodology is the ability to switch out which embedding layer you \\nuse. The standard embedding (OpenAI) or Palm‚Äôs embed. When choosing between both of those some tradeoÔ¨Äs are \\nmade. \\nWhen using no context, Palm‚Äôs embedding layer seems to perform much better across the board, allowing for'),\n",
       " Document(metadata={'page': 6, 'type': 'text'}, page_content='a much higher average similarity score, however this drastically shifts when given context as now OpenAI‚Äôs embedding \\nlayer performs much more soundly. The evidence for these claims is discussed later in this paper. \\nAdditionally, another beneÔ¨Åcial feature of the RAG optimization and breakdown style is the ability to cus-\\ntomize which similarity score the LLM uses to decide which answer to base its response oÔ¨Ä from the Q-A list. To go'),\n",
       " Document(metadata={'page': 6, 'type': 'text'}, page_content='into further detail, the code when prompted with a user question compares the user question to the Q-A list and reorders \\nthe list based oÔ¨Ä highest similarity score to lowest, this allows the LLM to select the top 2-3 answers to the highest \\nranked questions and continue generating its own response from there. \\nThe Ô¨Årst choice of similarity score was STS OpenAI Score while the second was STS Palm Score. In terms'),\n",
       " Document(metadata={'page': 6, 'type': 'text'}, page_content='of the data when GPT (for the purposes of precision all of the following results include context) and STS OpenAI were \\ncombined, you got an average similarity score of 0.997. If you instead pair this with Palm STS Score instead, the \\naverage score drops to 0.92, a 0.077 decrease in performance. A similar eÔ¨Äect when using Palm with Palm STS and \\nPalm with OpenAI STS (0.996 versus 0.93, respectively). This data demonstrates that both Palm and OpenAI are able \\nVolume 12 Issue 4 (2023)'),\n",
       " Document(metadata={'page': 6, 'type': 'text'}, page_content='Volume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n7'),\n",
       " Document(metadata={'page': 6, 'type': 'image', 'image_id': 'page_6_img_0'}, page_content='[Image: page_6_img_0]'),\n",
       " Document(metadata={'page': 7, 'type': 'text'}, page_content='to reach very high accuracy levels when paired with a similarity score calculated from the same program (this means \\nGPT worked better with OpenAI STS Score and that Palm worked better with Palm STS Score). What is also interest-\\ning to note is that although Palm produced a 0.001 lower performance than GPT it seemed to be more Ô¨Çexible, working \\nbetter with its competitor (OpenAI STS Score + Palm produced 0.93) than the GPT with its competitor (Palm STS \\nScore + GPT produced 0.92).'),\n",
       " Document(metadata={'page': 7, 'type': 'text'}, page_content='Score + GPT produced 0.92). \\nTo recap on the experimental setup, this code uses an interchangeable piece of a LLM so you can swap or \\nreplace things like the embedding layer used, the similarity score used, and the base language model used, also whether \\nit was given context from the Q-A database or not. This is a novel and important way to be able to break down LLM \\nand the data collected speaks a lot to the importance of each aspect of an LLM.'),\n",
       " Document(metadata={'page': 7, 'type': 'text'}, page_content='and the data collected speaks a lot to the importance of each aspect of an LLM. \\nIn terms of expected results, two things were noticed, Ô¨Årst that Palm had a slightly lower performance than \\nGPT (0.996 versus 0.997 respectively) on their top runs, however, there was also some contradictory data as it seemed \\nthat Palm worked signiÔ¨Åcantly better when given no context to work with compared to GPT, Palm produced an average'),\n",
       " Document(metadata={'page': 7, 'type': 'text'}, page_content='score of 0.88 and 0.91 when given no context while GPT produced an average score of 0.75 and 0.897. Although GPT \\nmay perform much better when given context, Palm seems to beat it out just given its own proprietary dataset (no \\ncontext). \\nSome unexpected results occurred with pairing GPT and Palm with their opposite embeds, for example, pair-\\ning Palm with OpenAI embed. While on paper it makes sense that Palm would work better with Palm embed, it actually'),\n",
       " Document(metadata={'page': 7, 'type': 'text'}, page_content='performed better when paired with OpenAI‚Äôs embed. 0.91 (with OpenAI embed) versus 0.88 (Palm embed)‚Äînote that \\nthis is without context given. A similar eÔ¨Äect was noticed going the other way around as well, without context, GPT \\nperformed much better with Palm embedding layer than with its own OpenAI embedding layer (0.897 versus 0.75 \\nrespectively). This data shows how Palm embedding layer tends to perform much better given no context when com-'),\n",
       " Document(metadata={'page': 7, 'type': 'text'}, page_content='pared to OpenAI‚Äôs embed. Similarly, to explored above, it is the opposite when given context, however. OpenAI‚Äôs \\nembedding layer performs a bit better when given context across the board than Palm embed. \\nMost notably from this experiment was two realizations. First, that Palm Embedding layer tends to work much \\nbetter when just given its own proprietary dataset (and no context), when compared to OpenAI‚Äôs embed. Additionally,'),\n",
       " Document(metadata={'page': 7, 'type': 'text'}, page_content='when given context, the playing Ô¨Åeld switches: Palm tends to perform much worse when given context when compared \\nto OpenAI. Lastly, it is important to note that a combination of Palm/GPT with OpenAI‚Äôs embedding layer and context \\nyielded extremely accurate results when its similarity scores were averaged. \\n \\nConclusion \\n \\nBuilding upon the foundation laid by our initial Ô¨Åndings, it is paramount to recognize the exceptional performance of'),\n",
       " Document(metadata={'page': 7, 'type': 'text'}, page_content='the Palm model when utilizing its proprietary dataset in conjunction with the Palm embed. This speciÔ¨Åcity in data and \\ntechnology synchronization has shown that Palm outshines OpenAI in terms of model accuracy in a context-free envi-\\nronment. However, the landscape shifts when contextual data is integrated. In such scenarios, the combination of GPT \\nwith its native OpenAI embedding layer excels, leveraging the additional context to produce responses of remarkable'),\n",
       " Document(metadata={'page': 7, 'type': 'text'}, page_content='accuracy that resonate with the cultural and ecological nuances of the Amazon. \\nThis pivot in performance based on context underscores the signiÔ¨Åcance of tailored datasets and embedding \\nmechanisms in the optimization of Large Language Models (LLMs). The adaptability of the Retrieval-Augmented \\nGeneration (RAG) framework emerges as a cornerstone for future enhancements in LLMs. By enabling the seamless'),\n",
       " Document(metadata={'page': 7, 'type': 'text'}, page_content=\"interchange of model components, RAG presents an evolutionary leap in the Ô¨Åne-tuning of language models, catering \\nto the intricate demands of culturally rich and contextually complex datasets. \\nIn light of these advancements, our research signiÔ¨Åes a pivotal moment for LLMs. The evidence suggests that \\nwhen models are Ô¨Ånely tuned with an awareness of the dataset's inherent context and the corresponding embedding\"),\n",
       " Document(metadata={'page': 7, 'type': 'text'}, page_content=\"layers, they reach new heights of linguistic precision. Therefore, the path forward for LLMs lies in embracing the \\nmodular and contextually aware RAG framework, which promises to reÔ¨Åne the capabilities of language models to an \\nunprecedented degree, ensuring the preservation and celebration of the world's diverse linguistic and cultural heritage. \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n8\"),\n",
       " Document(metadata={'page': 7, 'type': 'image', 'image_id': 'page_7_img_0'}, page_content='[Image: page_7_img_0]'),\n",
       " Document(metadata={'page': 8, 'type': 'text'}, page_content='Limitations \\n \\nThe results of the ‚Äúoutputted answer‚Äù of this algorithm largely reÔ¨Çect the quality of the data. If the LLM is trained oÔ¨Ä \\nlow-quality data, then the answer will reÔ¨Çect this bias. The results of the experiment will Ô¨Çuctuate with diÔ¨Äerent results \\nshould a diÔ¨Äerent dataset be used to train the LLM.  \\nIn the future there is a lot of potential to expand on this research by breaking down the RAG algorithm into'),\n",
       " Document(metadata={'page': 8, 'type': 'text'}, page_content='even more separate components to further see the diÔ¨Äerences in average similarity score that adding or removing each \\ncomponent makes. \\n \\nReferences \\n \\nBahdanau, D. C. (2016). End-to-end attention-based large vocabulary speech recognition. 2016 IEEE international \\nconference on acoustics, speech and signal processing (ICASSP), 4945-4949. \\nDevlin, J. C. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint, \\narXiv:1810.04805.'),\n",
       " Document(metadata={'page': 8, 'type': 'text'}, page_content='arXiv:1810.04805. \\nLewis, P. P. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural \\nInformation Processing Systems, 9459-9474. \\nRadford, A. N. (2018). Improving language understanding by generative pre-training. OpenAI. \\nSiriwardhana, S. W. (2023). Improving the domain adaptation of retrieval augmented generation (RAG) models for \\nopen domain question answering. Transactions of the Association for Computational Linguistics, 1-17.'),\n",
       " Document(metadata={'page': 8, 'type': 'text'}, page_content='Vaswani, A. S. (2017). Attention is all you need. Advances in neural information processing systems. \\nYu, W. (2022). Retrieval-augmented generation across heterogeneous knowledge. Proceedings of the 2022 \\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language \\nTechnologies: Student Research Workshop, 52-58. \\n \\n \\n \\n \\nVolume 12 Issue 4 (2023)\\nISSN: 2167-1907\\nwww.JSR.org/hs\\n9'),\n",
       " Document(metadata={'page': 8, 'type': 'image', 'image_id': 'page_8_img_0'}, page_content='[Image: page_8_img_0]')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "357bfbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04805334, -0.00206825, -0.00703998, ...,  0.04569346,\n",
       "         0.03057196,  0.00390264],\n",
       "       [ 0.0174334 , -0.00340782,  0.00347105, ...,  0.04686106,\n",
       "         0.00965531, -0.0386954 ],\n",
       "       [ 0.03071707,  0.00183673, -0.03335669, ...,  0.00919674,\n",
       "         0.03857673,  0.00054351],\n",
       "       ...,\n",
       "       [ 0.01572528, -0.01759314, -0.01674261, ..., -0.10801527,\n",
       "        -0.01391945, -0.02823574],\n",
       "       [ 0.03574401, -0.02967369, -0.03134512, ...,  0.01510686,\n",
       "        -0.02042773, -0.04929977],\n",
       "       [-0.02729444,  0.01050514,  0.01541131, ...,  0.05849979,\n",
       "         0.02525214, -0.02767638]], shape=(84, 512), dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create embedding array\n",
    "embeddings_array = np.array(all_embeddings)\n",
    "embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33b1655f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# Create custom FAISS index since we have precomputed embeddings\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
    "    embedding=None,  # We're using precomputed embeddings\n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b95e76",
   "metadata": {},
   "source": [
    "### Data Retrieval\n",
    "1. Define LLM(OpenAI gpt-4.1)\n",
    "2. Create Retriever\n",
    "3. Perform Similarity search to retrieve context\n",
    "4. Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0979f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e66420c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7677dc153620>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7677d62b7500>, root_client=<openai.OpenAI object at 0x7677dc4aef30>, root_async_client=<openai.AsyncOpenAI object at 0x7677dc4bdb50>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4.1\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f794543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Retriever function\n",
    "def retrieve_multimodal(query, k=5):\n",
    "    \"\"\"Unified retrieval using CLIP embeddings for both text and images.\"\"\"\n",
    "    # Embed query using CLIP\n",
    "    query_embedding = embed_text(query)\n",
    "    \n",
    "    # Search in unified vector store\n",
    "    results = vector_store.similarity_search_by_vector(\n",
    "        embedding=query_embedding,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b3c70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_message(query, retrieved_docs):\n",
    "    \"\"\"Create a message with both text and images for GPT-4V.\"\"\"\n",
    "    content = []\n",
    "    \n",
    "    # Add the query\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"\n",
    "    })\n",
    "    \n",
    "    # Separate text and image documents\n",
    "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "    \n",
    "    # Add text context\n",
    "    if text_docs:\n",
    "        text_context = \"\\n\\n\".join([\n",
    "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "            for doc in text_docs\n",
    "        ])\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
    "        })\n",
    "    \n",
    "    # Add images\n",
    "    for doc in image_docs:\n",
    "        image_id = doc.metadata.get(\"image_id\")\n",
    "        if image_id and image_id in image_data_store:\n",
    "            content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
    "            })\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Add instruction\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
    "    })\n",
    "    \n",
    "    return HumanMessage(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd033323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_pdf_rag_pipeline(query):\n",
    "    \"\"\"Main pipeline for multimodal RAG.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    context_docs = retrieve_multimodal(query, k=5)\n",
    "    \n",
    "    # Create multimodal message\n",
    "    message = create_multimodal_message(query, context_docs)\n",
    "    \n",
    "    # Get response from GPT-4V\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    # Print retrieved context info\n",
    "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
    "    for doc in context_docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        if doc_type == \"text\":\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"  - Text from page {page}: {preview}\")\n",
    "        else:\n",
    "            print(f\"  - Image from page {page}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30a56e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What are the Potential Implications for LLMs?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 5 documents:\n",
      "  - Text from page 6: In the initial world of LLM, in order to incrementally increase its performance engineers of these m...\n",
      "  - Text from page 1: information is conspicuously lacking in the vast repository of knowledge available on the internet. ...\n",
      "  - Text from page 4: and contextual sensitivity. \n",
      "Moreover, the experiment is poised to challenge the prevailing approach...\n",
      "  - Text from page 0: proven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG)...\n",
      "  - Text from page 7: and the data collected speaks a lot to the importance of each aspect of an LLM. \n",
      "In terms of expecte...\n",
      "\n",
      "\n",
      "Answer: Based on the provided excerpts, the **potential implications for Large Language Models (LLMs)** are as follows:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Performance Plateau and the Limits of Scaling**\n",
      "- The text notes that models like OpenAI‚Äôs GPT and Google‚Äôs Bard are beginning to **plateau in performance**, even as complexity increases. This suggests that simply making models bigger or fine-tuning them with more data/power is delivering diminishing returns.\n",
      "\n",
      "### 2. **Retrieval-Augmented Generation (RAG) as a Key Alternative**\n",
      "- RAG is highlighted as a **promising alternative** to traditional LLM approaches, especially in contexts requiring nuanced or extended knowledge. This approach dynamically supplements the LLM with external information rather than relying solely on the static memory and parameters of the model.\n",
      "\n",
      "### 3. **Component-Based and Dynamic Approaches**\n",
      "- There is an implied shift from **static, \"one-size-fits-all\" models** to **more dynamic, component-based architectures**. This allows LLMs to be fine-tuned or configured with **greater flexibility and precision** for specific domains, making them more adaptable and specialized.\n",
      "\n",
      "### 4. **Improved Domain Relevance and Cultural Representation**\n",
      "- Advanced LLMs, especially when paired with RAG, facilitate the inclusion and amplification of **indigenous and underrepresented knowledge** (e.g., indigenous Amazonian cultures). This addresses gaps in available internet knowledge and makes LLMs more culturally and contextually aware.\n",
      "\n",
      "### 5. **Granular Measurement, Customization, and Experimentation**\n",
      "- The experiments reveal that the impact of **individual LLM components** is significant and sometimes counterintuitive (e.g., Palm versus GPT in context/no-context scenarios). This underscores the importance of granular experimentation and customization when deploying LLMs for specific tasks.\n",
      "\n",
      "### 6. **Efficiency and Reduction of Resource Wastage**\n",
      "- Historically, improving LLM performance required **retrains with immense computational costs** and a high rate of null results. With competitive alternatives like RAG and component-based strategies, the **resource requirements for improvement may decrease**.\n",
      "\n",
      "---\n",
      "\n",
      "**In summary,** the potential implications for LLMs are:  \n",
      "a shift towards retrieval-augmented, dynamic, and component-based models that are more efficient, adaptable, and able to better serve specialized domains and underrepresented knowledges, marking a move beyond brute-force scale and generic models and toward tailored, resource-efficient solutions with improved cultural/contextual inclusivity.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the Potential Implications for LLMs?\"\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(\"-\" * 50)\n",
    "answer = multimodal_pdf_rag_pipeline(query)\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc61c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_pdf_rag_pipeline_with_chain(query):\n",
    "    \"\"\"Main pipeline for multimodal RAG.\"\"\"\n",
    "    # Create the RAG chain using Runnable components\n",
    "    rag_chain = (\n",
    "        RunnableLambda(lambda query: {\"query\": query, \"docs\": retrieve_multimodal(query, k= 5)})\n",
    "        | RunnableLambda(lambda inputs: {\n",
    "            \"query\": inputs[\"query\"],\n",
    "            \"message\": create_multimodal_message(inputs[\"query\"], inputs[\"docs\"])\n",
    "        })\n",
    "        | RunnableLambda(lambda inputs: llm.invoke([inputs[\"message\"]]))\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Get response from GPT-4V\n",
    "    response = rag_chain.invoke(query)\n",
    "    \n",
    "    # Also print retrieved docs\n",
    "    context_docs = retrieve_multimodal(query)\n",
    "    # Print retrieved context info\n",
    "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
    "    for doc in context_docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        if doc_type == \"text\":\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"  - Text from page {page}: {preview}\")\n",
    "        else:\n",
    "            print(f\"  - Image from page {page}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0002676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What are the Potential Implications for LLMs?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 5 documents:\n",
      "  - Text from page 6: In the initial world of LLM, in order to incrementally increase its performance engineers of these m...\n",
      "  - Text from page 1: information is conspicuously lacking in the vast repository of knowledge available on the internet. ...\n",
      "  - Text from page 4: and contextual sensitivity. \n",
      "Moreover, the experiment is poised to challenge the prevailing approach...\n",
      "  - Text from page 0: proven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG)...\n",
      "  - Text from page 7: and the data collected speaks a lot to the importance of each aspect of an LLM. \n",
      "In terms of expecte...\n",
      "\n",
      "\n",
      "Answer: Certainly! Based on your provided excerpts, here is a synthesis to answer:\n",
      "\n",
      "**Question:** What are the Potential Implications for LLMs?\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "The provided excerpts suggest several key potential implications for the development and application of Large Language Models (LLMs):\n",
      "\n",
      "1. **Plateauing Performance with Traditional Fine-Tuning:**  \n",
      "   As LLMs like OpenAI‚Äôs GPT and Google‚Äôs Bard become more complex, their gains from conventional fine-tuning and retraining are diminishing. This suggests a limitation with current training paradigms, highlighting the need for new approaches to improve model performance effectively ([page 6]).\n",
      "\n",
      "2. **Shift Towards Component-Based and Dynamic Approaches:**  \n",
      "   The research points towards moving away from static, one-size-fits-all models in favor of dynamic, component-based methods. This allows finer control and flexibility‚Äîpotentially tailoring LLMs more precisely for specific domains or applications ([page 4]).\n",
      "\n",
      "3. **Promise of Retrieval-Augmented Generation (RAG):**  \n",
      "   Retrieval-Augmented Generation is highlighted as a promising alternative to traditional LLM training, especially when incorporating external knowledge. However, the full potential may only be realized by dissecting and optimizing individual RAG components ([pages 0, 4]).\n",
      "\n",
      "4. **Domain Adaptation and Contextual Sensitivity:**  \n",
      "   Component-level analysis and dynamic model configuration can improve how LLMs adapt to diverse domains and contexts, possibly addressing knowledge gaps (such as disseminating indigenous cultural knowledge, as on [page 1]).\n",
      "\n",
      "5. **Granular Performance Optimization:**  \n",
      "   Experimental data suggest that model performance can vary not only by architecture (e.g., GPT vs. Palm) but also by context and configuration, pointing towards a need for more nuanced, granular approaches for optimizing LLMs per specific tasks ([pages 7, Table 1]).\n",
      "\n",
      "6. **Expanded Social and Cultural Impact:**  \n",
      "   Advanced LLMs play a role in amplifying underrepresented narratives and knowledge, bridging cultural gaps‚Äîfor example, making indigenous knowledge more accessible and visible ([page 1]). Thus, model improvements can have significant societal consequences.\n",
      "\n",
      "---\n",
      "\n",
      "**In summary:**  \n",
      "The potential implications for LLMs, as inferred from the excerpts, include a move away from monolithic, universally trained models towards more flexible, component-based, and dynamically configurable architectures such as RAG. This shift can address current plateaus in performance, enhance domain sensitivity, enable more fine-tuned optimization, and expand the social and cultural utility of LLMs.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the Potential Implications for LLMs?\"\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(\"-\" * 50)\n",
    "answer = multimodal_pdf_rag_pipeline_with_chain(query)\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the Potential Implications for LLMs?\"\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(\"-\" * 50)\n",
    "answer = multimodal_pdf_rag_pipeline_with_chain(query)\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc27606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Explain the Venn Diagram of Data Sources for RAG\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 5 documents:\n",
      "  - Text from page 0: proven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG)...\n",
      "  - Text from page 2: larity as the basis for our similarity score, deÔ¨Åned by the formula: \n",
      "similarity(ùêØùêØùë†ùë†, ùêØùêØùë°ùë°) =\n",
      "ùêØùêØùë†ùë† ...\n",
      "  - Text from page 2: integrates a retriever model that sources relevant context and a generator model that synthesizes th...\n",
      "  - Text from page 6: In the initial world of LLM, in order to incrementally increase its performance engineers of these m...\n",
      "  - Text from page 1: information is conspicuously lacking in the vast repository of knowledge available on the internet. ...\n",
      "\n",
      "\n",
      "Answer: Certainly! Let‚Äôs break down the Venn Diagram of Data Sources for **Retrieval-Augmented Generation (RAG)** based on the context provided.\n",
      "\n",
      "---\n",
      "\n",
      "### **RAG Overview**\n",
      "From Page 2:  \n",
      "- RAG *integrates a retriever model* (fetches relevant context) and *a generator model* (synthesizes responses).\n",
      "- The retriever model searches a knowledge base ùí¶ and retrieves a set of relevant documents ùê∑ = {ùëë‚ÇÅ, ùëë‚ÇÇ, ..., ùëë‚Çñ}.\n",
      "\n",
      "---\n",
      "\n",
      "### **What are the Data Sources in RAG?**  \n",
      "In RAG, there are typically **three major data sources** that interact or overlap:\n",
      "\n",
      "1. **Knowledge Base (KB) / External Documents**\n",
      "   - This is the database or corpus RAG searches through to retrieve factual supporting information.\n",
      "   - Examples: Wikipedia, proprietary databases, news archives.\n",
      "\n",
      "2. **Pre-trained LLM Knowledge**\n",
      "   - What the large language model already knows from training data (up to its cutoff date).\n",
      "   - This includes general world knowledge, language patterns, grammar, etc.\n",
      "\n",
      "3. **User Query / Prompt Context**\n",
      "   - Information given in the user's input, including the specific context, question, or instructions provided at runtime.\n",
      "\n",
      "---\n",
      "\n",
      "### **The Venn Diagram: How do these sources overlap?**\n",
      "\n",
      "Although the actual image is not provided here, we can *describe* the Venn Diagram as follows:\n",
      "\n",
      "- **Circle 1 (External Knowledge Base):**  \n",
      "  Contains all information retrievable from the document corpus (e.g., scientific papers, web pages).\n",
      "\n",
      "- **Circle 2 (Pre-trained LLM Knowledge):**  \n",
      "  Represents everything the LLM \"remembers\" from pretraining‚Äînot updated after its knowledge cutoff. This may overlap with the knowledge base (if there is shared information), but also includes data not present in the external source.\n",
      "\n",
      "- **Circle 3 (User Query Context):**  \n",
      "  Represents specifics provided in the current prompt/query. Sometimes overlaps with what‚Äôs in the KB or LLM memory, sometimes doesn‚Äôt.\n",
      "\n",
      "#### **Overlapping Regions:**\n",
      "- **A (Intersection of LLM + KB):**  \n",
      "  Information both present in the LLM‚Äôs memory and in the external KB.  \n",
      "  Example: Well-known facts or recently digitized public knowledge.\n",
      "\n",
      "- **B (Intersection of Query + KB):**  \n",
      "  Information the user provides that also exists in external document sources; usually what is being specifically retrieved.\n",
      "\n",
      "- **C (Intersection of LLM + Query):**  \n",
      "  Situations where the user asks about things the LLM ‚Äúremembers‚Äù from training but are not in the current KB.\n",
      "\n",
      "- **D (Intersection of all three):**  \n",
      "  Rare, but represents topics known by the LLM, present in the knowledge base, and directly mentioned in the query.\n",
      "\n",
      "- **Non-overlapping portions:**  \n",
      "  Each source also contains unique information absent from the others.\n",
      "\n",
      "---\n",
      "\n",
      "### **Summary Table: Venn Diagram Relationships**\n",
      "\n",
      "| Region                          | Example                                                        |\n",
      "|----------------------------------|----------------------------------------------------------------|\n",
      "| LLM only                        | Obsolete facts from pre-2021 not in KB                         |\n",
      "| KB only                         | Newly added articles not in LLM training                       |\n",
      "| Query only                      | Very specific context unique to current prompt                 |\n",
      "| LLM ‚à© KB                        | General facts, commonly asked questions                        |\n",
      "| KB ‚à© Query                      | User asks for info that exists in KB, fetched on demand        |\n",
      "| LLM ‚à© Query                     | User asks for facts LLM knows, not in KB                       |\n",
      "| LLM ‚à© KB ‚à© Query                | User asks for popular info, known by LLM and present in KB     |\n",
      "\n",
      "---\n",
      "\n",
      "### **Why is this important in RAG?**\n",
      "- **Retrieval** complements the LLM‚Äôs \"memory,\" bringing in up-to-date or specialized information.\n",
      "- Effective RAG systems maximize the overlap between **user intent (query)** and **external knowledge**, while also leveraging what the LLM already knows, for optimal performance.\n",
      "\n",
      "---\n",
      "\n",
      "## **In Summary:**\n",
      "**The Venn Diagram of Data Sources for RAG** illustrates the relationship and overlaps among:\n",
      "- **External Knowledge Base**\n",
      "- **LLM‚Äôs Pretrained Knowledge**\n",
      "- **User‚Äôs Current Query**\n",
      "\n",
      "RAG leverages all three, retrieving and synthesizing information from the space where they overlap (and beyond), providing more accurate and relevant responses than any single source alone.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain the Venn Diagram of Data Sources for RAG\"\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(\"-\" * 50)\n",
    "answer = multimodal_pdf_rag_pipeline_with_chain(query)\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "161cc337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How question of a user is answered using API calls?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 5 documents:\n",
      "  - Text from page 6: In the initial world of LLM, in order to incrementally increase its performance engineers of these m...\n",
      "  - Text from page 3: answer. \n",
      "The language model ‚Ñí is Ô¨Åne-tuned on ùíüùíü, optimizing the weights to minimize the loss functi...\n",
      "  - Text from page 2: integrates a retriever model that sources relevant context and a generator model that synthesizes th...\n",
      "  - Text from page 6: into further detail, the code when prompted with a user question compares the user question to the Q...\n",
      "  - Text from page 0: proven to be a challenging endeavor, it has become evident that Retrieval-Augmented Generation (RAG)...\n",
      "\n",
      "\n",
      "Answer: Certainly! Here‚Äôs how a user‚Äôs question is answered using API calls, based strictly on your provided text:\n",
      "\n",
      "---\n",
      "\n",
      "**Step-by-Step Process:**\n",
      "\n",
      "1. **User submits a question:**  \n",
      "   The user's question (query ùëû) is sent, typically via an API call, to a question-answering system using a large language model (LLM).\n",
      "\n",
      "2. **Retriever model finds context:**  \n",
      "   - A retriever model searches a knowledge base ùí¶ for documents relevant to the question.  \n",
      "   - It returns a set of documents ùê∑ = {ùëë‚ÇÅ, ùëë‚ÇÇ, ..., ùëë‚Çñ} that are most similar or relevant (Page 2).\n",
      "\n",
      "3. **Similarity scoring and ranking:**  \n",
      "   - The code compares the user's question to an existing Q-A list (predefined list of question-answer pairs).\n",
      "   - It calculates a similarity score for each entry (using metrics like STS OpenAI Score or STS Palm Score, Page 6).\n",
      "   - The Q-A list is reordered from highest to lowest similarity to the user question.\n",
      "\n",
      "4. **Shortlisting candidates:**  \n",
      "   - The system selects the top 2‚Äì3 most relevant answers (those with the highest similarity scores).\n",
      "\n",
      "5. **Response generation:**  \n",
      "   - The generator model takes the context from the top-ranked candidate answers and synthesizes a coherent, complete response, possibly generating its own output based on the retrieved data (Page 2, Page 6).\n",
      "\n",
      "6. **Returning the answer through API:**  \n",
      "   - The generated answer is sent back to the user as the response to their query through the API.\n",
      "\n",
      "---\n",
      "\n",
      "**Summary Table (from the text):**\n",
      "\n",
      "| Step           | Function                                                                 |\n",
      "|----------------|--------------------------------------------------------------------------|\n",
      "| 1. API input   | User question sent via API                                               |\n",
      "| 2. Retrieval   | Retriever model fetches relevant documents/Q-A pairs by similarity score |\n",
      "| 3. Ranking     | Highest-similarity answers shortlisted                                   |\n",
      "| 4. Generation  | Generator synthesizes final answer                                       |\n",
      "| 5. API output  | Response returned to user via API                                        |\n",
      "\n",
      "---\n",
      "\n",
      "**References to Text:**\n",
      "- Retrievers and generators combine to fetch and create answers (Page 2).\n",
      "- Similarity scoring to rank Q-A pairs (Page 6).\n",
      "- Fine-tuned model retrieves and compares answers (Page 3).\n",
      "\n",
      "---\n",
      "\n",
      "**To summarize:**  \n",
      "When a user asks a question via API, the system retrieves the most relevant information using similarity scores, generates a response from the top-ranked answers, and returns this answer through the API.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"How question of a user is answered using API calls?\"\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(\"-\" * 50)\n",
    "answer = multimodal_pdf_rag_pipeline_with_chain(query)\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ee628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-use-cases-lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
